---
title: "Forecasting models"
order: 8
---

```{r echo = FALSE}
set.seed(123)
```

# Objectives

The aim of this session is to introduce some common forecasting models and to evaluate them.

# Libraries used

In this session we will use the `nfidd` package to load the data set of infection times, the `dplyr` and `tidyr` packages for data wrangling, `ggplot2` library for plotting, the `here` library to find the stan model, and the `cmdstanr` library for using stan.
We will also use the `tidybayes` package for extracting results of the inference.

```{r libraries, message = FALSE}
library("nfidd")
library("dplyr")
library("tidyr")
library("ggplot2")
library("here")
library("cmdstanr")
library("tidybayes")
```

::: {.callout-tip}
The best way to interact with the material is via the [Visual Editor](https://docs.posit.co/ide/user/ide/guide/documents/visual-editor.html) of RStudio.
If not using the Visual Editor please remember that the code in the session needs to be run inside the course repository so that the `here()` commands below find the stan model files.
:::

# An overview of forecasting models

See the slides for an overview of forecasting models.

# What did we learn about the random walk model from the [forecasting concepts session](forecasting-concepts)?

- It was unbiased when the reproduction number was constant but when the reproduction number was reducing due to susceptible depletion, the random walk model overestimated the number of cases systematically.
- It did relatively well at short-term forecasting indicating that it was capturing some of the underlying dynamics of the data generating process.

::: {.callout-note, collapse = TRUE}
## What did a geometric random walk model look like again?

```{r}
set.seed(123)
n <- 100
R <- rep(NA, n)
R[1] <- rnorm(1, 1, 0.1)
for (i in 2:n) {
  R[i] <- R[i-1] + rnorm(1, 0, 0.1)
}

data <- tibble(t = 1:n, R = exp(R))

ggplot(data, aes(x = t, y = R)) +
  geom_line() +
  labs(title = "Simulated data from a random walk model",
       x = "Time",
       y = "R")
```

:::

# Forecasting models as a spectrum. 

A large part of this course has been about showing the importance of understanding and modelling the underlying mechanisms of the data generating process.
However, in many forecasting scenarios, we may not have a good understanding of the underlying mechanisms, or the data may be too noisy to make accurate predictions.
The worst case is that we have mistaken beliefs about the underlying mechanisms and use these to make predictions which are systematically wrong and misleading.
In these cases, forecasters of often use _statistical models_ to make predictions which have little or no mechanistic basis.
In our work, we have found that a combination of mechanistic and statistical models can be very powerful but that identifying the best model for a given forecasting task can be challenging.

## Adding more mechanistic structure to the renewal model

One way to improve the renewal model is to add more mechanistic structure. In the [forecasting concepts session](forecasting-concepts), we saw that the renewal model was unbiased when the reproduction number was constant but that it overestimated the number of cases when the reproduction number was reducing due to susceptible depletion.

This suggests that we should add a term to the renewal model which captures the depletion of susceptibles. One way to do this is to add a term which is proportional to the number of susceptibles in the population. This is the idea behind the _SIR model_ which is a simple compartmental model of infectious disease transmission. If we assume that susceptible depletion is the only mechanism which is causing the reproduction number to change, we can write the reproduction model as:

$$
R_t = \frac{S_{t-1}}{N} R_0
$$

This approximates susceptible depletion as a linear function of the number of susceptibles in the population. This is a simplification but it is a good starting point.

::: {.callout-note, collapse = TRUE}
## What behaviour would we expect from this model?

```{r}
set.seed(123)
n <- 100
N <- 1000
R0 <- 1.5
S <- rep(NA, n)
S[1] <- N
R <- rep(NA, n)
R[1] <- R0
I <- rep(NA, n)
I[1] <- 1
for (i in 2:n) {
  R[i] <- (S[i-1]) / N * R0
  I[i] <- I[i-1] * R[i]
  S[i] <- S[i-1] - I[i]
}

data <- tibble(t = 1:n, R = R)

ggplot(data, aes(x = t, y = R)) +
  geom_line() +
  labs(title = "Simulated data from an SIR model",
       x = "Time",
       y = "R")
```

The key assumptions we are making here are: 

- The population is constant and we rouchgly know the size of the population.
- The reproduction number is constant (aside from the effect of susceptible depletion)
- The number of cases is proportional to the number of susceptibles in the population.

We've coded this up as a stan model in `stan/mechanistic-r.stan`. See `stan/functions/pop_bounded_renewal.stan` for the function which calculates the reproduction number. Lets load the model:

```{r}
mech_mod <- cmdstan_model(here("stan/mechanistic-r.stan"))
mech_mod$print(line_numbers = TRUE)
```

## Adding more statistical structure to the renewal model

Rather than adding more mechanistic structure to the renewal model, we can add more statistical structure.
Before we do this, we need to think about what we want from a forecasting model.
As we identified above, we want a model which is unbiased and which has good short-term forecasting properties.
We know that we want it to be able to adapt to trends in the reproduction number and that we want it to be able to capture the noise in the data.
A statistical term that can be used to describe capturing a trend is saying that the time series is _non-stationary_.

The random walk model we used in the [forecasting concept](forecasting-concepts) session is a special case of a more general class of models called _autoregressive (AR) models_. 
AR models are a class of models which predict the next value in a time series as a linear combination of the previous values in the time series.
The random walk model is specfically a special case of an AR(1) model where the next value in the time series is predicted as the previous value, multipled by a value between 1 and -1 , plus some noise. For the log-transformed reproduction number ($ log(R_t) $), the model is:

$$
log(R_t) = \phi log(R_{t-1}) + \epsilon_t
$$

where $\epsilon_t$ is a normally distributed error term with mean 0 and standard deviation $\sigma$ and $\phi$ is a parameter between -1 and 1. If we restict $\phi$ to be between 0 and 1, we get a model which is biased towards a reproduction number of 1 but which can still capture trends in the data that decay over time.

::: {.callout-note, collapse = TRUE
## What behaviour would we expect from this model?

```{r}
set.seed(123)
n <- 100
phi <- 0.4
sigma <- 0.1
log_R <- rep(NA, n)
log_R[1] <- rnorm(1, 0, sigma)
for (i in 2:n) {
  log_R[i] <- phi * log_R[i-1] + rnorm(1, 0, sigma)
}
data <- tibble(t = 1:n, R = exp(log_R))

ggplot(data, aes(x = t, y = R)) +
  geom_line() +
  labs(title = "Simulated data from an exponentiated AR(1) model",
       x = "Time",
       y = "R")
```
:::

However, we probably don't want a model which is biased towards a reproduction number of 1 (though as we know the future in this case it actually could be a reasonable choice!). So what should we do?

Returning to the idea that the reproduction number is a _non-stationary_ time series, we can use a method from the field of time series analysis called _differencing_ to make the time series stationary. This involves taking the difference between consecutive values in the time series. For the log-transformed reproduction number, this would be:

$$
log(R_t) - log(R_{t-1}) = \phi (log(R_{t-1}) - log(R_{t-2})) + \epsilon_t
$$


::: {.callout-note, collapse = TRUE}
## What behaviour would we expect from this model?

```{r}
set.seed(123)
n <- 100
phi <- 0.1
sigma <- 0.1
log_R <- rep(NA, n)
log_R[1] <- rnorm(1, 0, sigma)
log_R[2] <- log_R[1] + rnorm(1, 0, sigma)
for (i in 3:n) {
  log_R[i] <- log_R[i-1] + phi * (log_R[i-1] - log_R[i-2]) + rnorm(1, 0, sigma)
}

data <- tibble(t = 1:n, R = exp(log_R))

ggplot(data, aes(x = t, y = R)) +
  geom_line() +
  labs(title = "Simulated data from an exponentiated AR(1) model with differencing",
       x = "Time",
       y = "R")
```

:::

We've coded this up as a stan model in `stan/statistical-r.stan`. See `stan/functions/geometic_diff_ar1.stan` for the function which calculates the reproduction number. Lets load the model:

```{r}
stat_mod <- cmdstan_model(here("stan/statistical-r.stan"))
stat_mod$print(line_numbers = TRUE)
```


# Forecasting with the mechanistic and statistical models

We will now use the mechanistic and statistical models to forecast the number of cases in the future using the same data set we used in the [forecasting concepts session](forecasting-concepts). We will first load in the data and filter for a target forecast date.

```{r, load-simulated-onset}
source(here::here("snippets", "simulate-onsets.r"))
onset_df

cutoff <- 71

filtered_onset_df <- onset_df |>
  filter(day <= cutoff)
```

We will now fit the mechanistic model to the data.
```{r fit_mech_model}
horizon <- 28

data <- list(
  n =nrow(filtered_onset_df),
  I0 = 1,
  obs = filtered_onset_df$onsets,
  gen_time_max = length(gen_time_pmf),
  gen_time_pmf = gen_time_pmf,
  ip_max = length(ip_pmf) - 1,
  ip_pmf = ip_pmf,
  h = horizon # Here we set the number of days to forecast into the future
  N_prior = (10000, 2000) # the prior for the population size
)
mech_forecast_fit <- mech_mod$sample(
  data = data, parallel_chains = 4, refresh = 0, show_exceptions = FALSE, show_messages = FALSE, adapt_delta = 0.95)
mech_forecast_fit
```

We will now fit the statistical model to the data.

```{r fit_stat_model}
data <- list(
  n =nrow(filtered_onset_df),
  I0 = 1,
  obs = filtered_onset_df$onsets,
  gen_time_max = length(gen_time_pmf),
  gen_time_pmf = gen_time_pmf,
  ip_max = length(ip_pmf) - 1,
  ip_pmf = ip_pmf,
  h = horizon # Here we set the number of days to forecast into the future
)
stat_forecast_fit <- stat_mod$sample(
  data = data, parallel_chains = 4, refresh = 0, show_exceptions = FALSE, show_messages = FALSE, adapt_delta = 0.95)
```

Finally we can extract the forecasts from the models and plot them.

```{r extract-forecast}
mech_forecast <- mech_forecast_fit |>
  gather_draws(forecast[day]) |>
  mutate(day = day + cutoff)

stat_forecast <- stat_forecast_fit |>
  gather_draws(forecast[day]) |>
  mutate(day = day + cutoff)

forecast <- bind_rows(
  mutate(mech_forecast, model = "mechanistic"),
  mutate(stat_forecast, model = "statistical")
)

target_onsets <- onset_df |>
  filter(day > cutoff) |>
  filter(day <= cutoff + horizon)
```

```{r plot_forecast}
forecast |>
  filter(.draw <= 100) |>
  ggplot(aes(x = day)) +
  geom_line(alpha = 0.1, aes(y = .value, group = intersection(.draw, model), col = model) +
  geom_point(data = target_onsets, aes(x = day, y = onsets), color = "black")
```

::: {.callout-tip}
## Take 2 minutes
What do you notice about the forecasts from the mechanistic and statistical models?
:::

::: {.callout-note, collapse = TRUE}
## Solution
- The mechanistic model captures the downturn in the data very well.
- The statistical model is not as good at capturing the downturn in the data but is substantially better than the random walk model was in the [forecasting concepts session](forecasting-concepts).
:::

::: {.callout-tip}
## What happens we don't know the population size? (optional)

In the above we assumed that we knew the population size roughly. In practice, we may not. Refit the mechanistic model with different priors for the population size and see how the forecasts change.

::: {.callout-note, collapse = TRUE}
## The solution

```{r}
data <- list(
  n =nrow(filtered_onset_df),
  I0 = 1,
  obs = filtered_onset_df$onsets,
  gen_time_max = length(gen_time_pmf),
  gen_time_pmf = gen_time_pmf,
  ip_max = length(ip_pmf) - 1,
  ip_pmf = ip_pmf,
  h = horizon # Here we set the number of days to forecast into the future
  N_prior = (1000, 200) # the prior for the population size
)

mech_forecast_fit_diff <- mech_mod$sample(
  data = data, parallel_chains = 4, refresh = 0, show_exceptions = FALSE, show_messages = FALSE, adapt_delta = 0.95)

mech_forecast_diff <- mech_forecast_fit_diff |>
  gather_draws(forecast[day]) |>
  mutate(day = day + cutoff)


mech_forecast_diff |>
  filter(.draw <= 100) |>
  ggplot(aes(x = day)) +
  geom_line(alpha = 0.1, aes(y = .value, group = .draw) +
  geom_point(data = target_onsets, aes(x = day, y = onsets), color = "black")
)
```
:::
:::

# Evaluating forecasts from our models

As in the [forecasting concepts session](forecasting-concepts), we have fit these models to a range of forecast dates so you don't have to wait for the models to fit. We will now evaluate the forecasts from the mechanistic and statistical models.

```{r load_forecasts}
forecasts <- bind_rows(
  rw_forecasts |>
    mutate(model = "random_walk"),
  mech_forecasts |>
    mutate(model = "mechanistic"),
  stat_forecasts |>
    mutate(model = "statistical")
)
```

::: {.callout-tip}
We generated these forecasts using the code in `data-raw/generate-example-forecasts.r` which uses the same approach we just took for a single forecast date but generalises it to many forecasts dates.

Some important things to note about these forecasts:

  - We used a 14 day forecast horizon.
  - Each forecast used all the data up to the forecast date.
  - We generated 1000 posterior samples for each forecast.
  - We started forecasting 3 weeks into the outbreak and then forecast every 7 days until the end of  the data (excluding the last 14 days to allow a full forecast).
  - We used pregenerated simulated data:

```{r}
example_onset_df
```

:::

## Visualising your forecast


```{r plot-all-forecasts}
forecasts |>
  filter(.draw <= 50) |>
  ggplot(aes(x = day)) +
  geom_line(aes(y = .value, group = interaction(.draw, target_day), col = target_day), alpha = 0.1) +
  geom_point(data = example_onset_df |>
    filter(day >= 21),
    aes(x = day, y = onsets), color = "black") +
  facet_wrap(~model)
```


As for the single forecast it is helpful to also plot the forecast on the log scale.

```{r plot-all-forecasts-log}
rw_forecasts |>
  filter(.draw <= 50) |>
  ggplot(aes(x = day)) +
  geom_line(aes(y = .value, group = interaction(.draw, target_day), col = target_day), alpha = 0.1) +
  geom_point(data = example_onset_df, aes(x = day, y = onsets), color = "black") +
  scale_y_continuous(trans = "log") +
  facet_wrap(~model)
```

::: {.callout-tip}
## Take 5 minutes
How do these forecasts compare?
Which do you prefer?
How well do they capture changes in trend?
Does the uncertainty seem reasonable?
Do they seem to under or over predict consistently?

:::

::: {.callout-note collapse="true"}
## Solution
:::

## Scoring your forecast


```{r convert-forecasts}
sc_forecasts <- forecasts |>
  left_join(example_onset_df, by = "day") |>
  filter(!is.na(.value)) |>
  as_forecast(
    forecast_unit = c("target_day", "horizon", "model"),
    forecast_type = "sample",
    observed = "onsets",
    predicted = ".value",
    model = "model",
    sample_id = ".draw"
  )
sc_forecasts
```


Everything seems to be in order. We can now use the `scoringutils` package to calculate some metrics as we did in the [forecasting concepts session](forecasting-concepts).

```{r score-forecasts}
sc_scores <- sc_forecasts |>
  score()

sc_scores
```

### At a glance

Let's summarise the scores by model first.

```{r}
sc_scores |>
  summarise_scores(by = "model")
```

::: {.callout-tip}
## Take 2 minutes
Before we look in detail at the scores, what do you think the scores are telling you? Which model do you think is best?
:::

### Continuous ranked probability score

As in the [forecasting concepts session](forecasting-concepts), we will start by looking at the CRPS by horizon and forecast date.

::: {.callout-tip}
## Reminder: Key things to note about the CRPS
  - Small values are better
  - As it is an absolute scoring rule it can be difficult to use to compare forecasts across scales.
:::

First by forecast horizon.

```{r}
sc_scores |>
  summarise_scores(by = c("model", "horizon")) |>
  ggplot(aes(x = horizon, y = crps, col = model)) +
  geom_point()
```

and across different forecast dates

```{r}
sc_scores |>
  summarise_scores(by = c("target_day", "model")) |>
  ggplot(aes(x = target_day, y = crps, col = model)) +
  geom_point()
```

::: {.callout-tip}
## Take 5 minutes 
How do the CRPS scores change based on forecast date?
How do the CRPS scores change with forecast horizon?
What are the key differences between the models?
:::

::: {.callout-note collapse="true"}
## Solution
:::

### PIT histograms

::: {.callout-tip}
## ReminderL: Interpreting the PIT histogram
- Ideally PIT histograms should be uniform. 
- If is a U shape then the model is overconfident and if it is an inverted U shape then the model is underconfident. 
- If it is skewed then the model is biased towards the direction of the skew.
:::


Let's first look at the overall PIT histogram.

```{r pit-histogram}
 sc_forecasts |>
  get_pit(by = "model") |>
  plot_pit() +
  facet_wrap(~model)
```

As before lets look at the PIT histogram by forecast horizon (to save space we will group horizons)

```{r pit-histogram-horizon}
sc_forecasts |> 
  mutate(group_horizon = case_when(
    horizon <= 3 ~ "1-3",
    horizon <= 7 ~ "4-7",
    horizon <= 14 ~ "8-14"
  )) |>
  get_pit(by = c("model", "group_horizon")) |>
  plot_pit() +
  facet_grid(vars(model), vars(group_horizon))
```

and then for different forecast dates.

```{r pit-histogram-date}
 sc_forecasts |>
  get_pit(by = "target_day") |>
  plot_pit() +
  facet_grid(vars(model), vars(group_horizon))
```

::: {.callout-tip}
## Take 5 minutes
What do you think of the PIT histograms?
Do they look well calibrated?
Do they look biased?
Which model do you think is best?
:::

::: {.callout-note collapse="true"}
## Solution
:::

## Scoring on the log scale

Again as in the [forecasting concepts session](forecasting-concepts), we will also score the forecasts on the log scale.

```{r log-convert-forecasts}
log_sc_forecasts <- sc_forecasts |>
  transform_forecasts(
    fun = log_shift,
    offset = 1,
    append = FALSE
  )

log_scores <- log_sc_forecasts |>
  score()
```


::: {.callout-tip}
Reminder: For more on scoring on the log scale see [Scoring forecasts on transformed scales](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1011393).
:::


### At a glance

```{r}
log_scores |>
  summarise_scores(by = "model")
```

::: {.callout-tip}
## Take 2 minutes
Before we look in detail at the scores, what do you think the scores are telling you? Which model do you think is best?
:::

### CRPS

```{r}
log_sc_scores |>
  summarise_scores(by = c("model", "horizon")) |>
  ggplot(aes(x = horizon, y = crps, col = model)) +
  geom_point()
```

```{r}
log_sc_scores |>
  summarise_scores(by = c("target_day", "model")) |>
  ggplot(aes(x = target_day, y = crps, col = model)) +
  geom_point()
```

::: {.callout-tip}
## Take 5 minutes 
How do the CRPS scores change based on forecast date?
How do the CRPS scores change with forecast horizon?
What does this tell you about the model?
How does it compare to the scores on the original scale?
:::

::: {.callout-note collapse="true"}
## Solution
:::

### PIT histograms

```{r pit-histogram}
 log_sc_forecasts |>
  get_pit(by = "model") |>
  plot_pit() +
  facet_wrap(~model)
```


```{r pit-histogram-horizon}
log_sc_forecasts |> 
  mutate(group_horizon = case_when(
    horizon <= 3 ~ "1-3",
    horizon <= 7 ~ "4-7",
    horizon <= 14 ~ "8-14"
  )) |>
  get_pit(by = c("model", "group_horizon")) |>
  plot_pit() +
  facet_grid(vars(model), vars(group_horizon))
```


```{r pit-histogram-date}
 log_sc_forecasts |>
  get_pit(by = "target_day") |>
  plot_pit() +
  facet_grid(vars(model), vars(group_horizon))
```

::: {.callout-tip}
## Take 5 minutes
What do you think of the PIT histograms?
Do they look well calibrated?
Do they look biased?
Which model do you think is best?
How does it compare to the scores on the original scale?
:::

::: {.callout-note collapse="true"}
## Solution
:::

# Going further

- We have only looked at two forecasting models here. There are many more models that could be used. For example, we could use a more complex mechanistic model which captures more of the underlying dynamics of the data generating process. We could also use a more complex statistical model which captures more of the underlying structure of the data.
- We could also combine the mechanistic and statistical models to create a hybrid model which captures the best of both worlds (maybe).
- We could also use a more complex scoring rule to evaluate the forecasts. For example, we could use a multivariate scoring rule which captures more of the structure of the data.

# Wrap up