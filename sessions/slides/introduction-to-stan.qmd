---
title: "Introduction to Bayesian inference with stan"
author: "Nowcasting and forecasting of infectious disease dynamics"
format:
  revealjs:
    output: slides/introduction-to-stan.html
    footer: "Introduction to Bayesian inference with stan"
    slide-level: 3
---

```{r echo = FALSE, message = FALSE}
library("ggplot2")
set.seed(12345)
```

### Why statistical concepts?

- We'll need to estimate things (delays, reproduction numbers, case numbers now and in the future)

- We'll want to correctly specify uncertainty

- We'll want to incorporate our domain expertise

- We'll do this using *Bayesian inference*

## Bayesian inference in 10 minutes

![](figures/bayesian_model_without_distributions.png)

### Probability distributions (discrete) {.smaller}

```{r create_poisson_plot}
kicks <- seq(0, 6)
prob <- dpois(kicks, lambda = 0.61)
df <- data.frame(kicks = kicks, prob = prob)
pp <- ggplot(df, aes(x = kicks, y = prob)) +
  theme_bw(20) +
  geom_point(size = 3) +
  ylab("Probability") +
  xlab("Number of kicks")
```

- E.g., how many people die of horse kicks if there are 0.61 kicks per year
- Described by the *Poisson* distribution

:::: {.columns}

::: {.column width="50%"}
```{r plot_poisson1}
pp
```
:::

::: {.column width="50%"}
:::

#### Two directions
1. Calculate the probability
2. Randomly sample


::::

### Calculate discrete probability {.smaller}

- E.g., how many people die of horse kicks if there are 0.61 kicks per year
- Described by the *Poisson* distribution

:::: {.columns}

::: {.column width="50%"}
```{r plot_poisson2}
pp
```
:::

::: {.column width="50%"}
What is the probability of 2 deaths in a year?
```{r dpois, echo = TRUE}
  dpois(x = 2, lambda = 0.61)
```
:::

::::

#### Two directions

1. **Calculate the probability**
2. Randomly sample

### Generate a random (Poisson) sample {.smaller}
- E.g., how many people die of horse kicks if there are 0.61 kicks per year
- Described by the *Poisson* distribution

:::: {.columns}

::: {.column width="50%"}
```{r plot_poisson3}
pp
```
:::

::: {.column width="50%"}
Generate one random sample from the probability distribution
```{r rpois, echo = TRUE}
  rpois(n = 1, lambda = 0.61)
```
:::

::::

#### Two directions

1. Calculate the probability
2. **Randomly sample**

### Probability distributions (continuous) {.smaller}
- Extension of probabilities to *continuous* variables
- E.g., the temperature in Stockholm tomorrow

Normalisation:
$$ \int p(a) da = 1 $$

Marginal probabilities:
$$ p(a) = \int_{} p(a, b) db$$

#### Two directions
1. Calculate the probability (density)
2. Randomly sample

### Calculate probability density {.smaller}
- Extension of probabilities to *continuous* variables
- E.g., the temperature in Stockholm tomorrow

```{r create_normal_plot}
temp_mean <- 23
temp_sd <- 2
temp <- seq(15, 31, by = 0.01)
prob <- dnorm(temp,mean = temp_mean, sd = temp_sd)
df <- data.frame(temp = temp, prob = prob)
pn <- ggplot(df, aes(x = temp, y = prob)) +
  theme_bw(20) +
  geom_line(lwd = 2) +
  xlab("Temperature") +
  ylab("Probability density")
```

:::: {.columns}

::: {.column width="50%"}
```{r plot_normal1}
pn
```
:::

::: {.column width="50%"}
What is the probability density of $30^\circ C$ tomorrow, if the mean temperature on the day is $23^\circ C$ (standard deviation $2^\circ C$) ? A naÃ¯ve model could be:
```{r dnorm, echo = TRUE}
  dnorm(x = 30,
        mean = 23,
        sd = 2)
```
:::

::::

#### Two directions

1. **Calculate the probability**
2. Randomly sample

### Generate a random (normal) sample {.smaller}

:::: {.columns}

::: {.column width="50%"}
```{r plot_normal2}
pn
```
:::

::: {.column width="50%"}
Generate one random sample from the normal probability distribution with mean 23 and standard deviation 2:
```{r rnorm, echo = TRUE}
  rnorm(n = 1,
        mean = 23,
        sd = 2)
```
:::

::::

#### Two directions

1. Calculate the probability
2. **Randomly sample**

## Bayesian inference {.smaller}

![](figures/bayesian_model_without_distributions.png)

The generative model can produce output which looks like data given a set of parameters $\theta$.

Idea of Bayesian inference: treat $\theta$ as **random variables** (with a probability distribution) and **condition on data**: posterior probability $p(\theta | \mathrm{data})$ as target of inference.

### Bayes' rule {.smaller}

- We treat the parameters of the a $\theta$ as random with *prior probabilities* given by a distribution $p(\theta)$. Confronting the model with data we obtain *posterior probabilities*  $p(\theta | \mathrm{data})$, our target of inference. Applying the rule of conditional probabilities, we can write this as

$$ p(\theta | \textrm{data}) = \frac{p(\textrm{data} | \theta) p(\theta)}{p(\textrm{data})}$$

- $p(\textrm{data} | \theta)$ is the *likelihood*
- $p(\textrm{data})$ is a *normalisation constant*

- In words,
  $$\textrm{(posterior)} \propto \textrm{(normalised likelihood)} \times \textrm{(prior)}$$

### Bayesian inference

![](figures/bayesian_model.png)

### MCMC

- Markov-chain Monte Carlo (MCMC) is a method to generate *samples* of $\theta$ that come from the *posterior distribution* given $\textrm{data}$, i.e the **target** of inference.
- Many flavours of MCMC exist: Metropolis-Hastings, Hamiltonian Monte Carlo, etc.

## What is stan and why do we use it?

- a *Probabilistic Programming Language* for Bayesian inference
  (i.e., a way to write down models)
  
- models are written in a text file (often ending `.stan`) and then loaded into an R/python/etc interface

- once a model is written down, stan can be used to **generate samples** from the **posterior distribution** (using a variety of methods)

## How to write a model in stan {.smaller}

:::: {.columns}

::: {.column width="40%"}
In a stan model file we specify:

- Data<br>(types and names)

- Parameters<br>(types and names)

- Model<br>(prior and likelihood)
:::

::: {.column width="60%"}
<br>

```{stan empty_model, output.var = "empty", eval = FALSE, echo = TRUE}
data {

}

parameters {

}

model {

}
```
:::
::::

## Example: fairness of a coin {.smaller}

:::: {.columns}

::: {.column width="40%"}

Data:

- $N$ coin flips

- $x$ times heads

Parameters

- $\theta$, probability of getting heads; uniform prior in $[0, 1]$

:::

::: {.column width="60%"}
<br>

```{r coin_model, output.var = "coin", eval = TRUE, echo = FALSE}
nfidd::nfidd_cmdstan_model("coin")
```

:::
::::

## Using stan from R {.smaller}

There are two packages for using stan from R. We will use the `cmdstanr` package:

```{r load_coin_model, echo = TRUE}
library("cmdstanr")
mod <- nfidd::nfidd_cmdstan_model("coin")
mod
```

## Sampling from the posterior {.xmaller}

```{r sample_from_coin_model, echo = TRUE}
data <- list(
  N = 10, ## 10 coin flips
  x = 6 ## 6 times heads
)
nfidd::nfidd_sample(mod, data = data)
```

#

[Return to the session](../R-Stan-and-statistical-concepts)
