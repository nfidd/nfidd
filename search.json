[
  {
    "objectID": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html",
    "href": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html",
    "title": "Using delay distributions to model the data generating process",
    "section": "",
    "text": "We’ve now developed a good idea of how epidemiological time delays affect our understanding of an evolving outbreak. So far, we’ve been working with individual line-list data. However, we usually want to model how an outbreak evolves through a whole population. The aim of this session is to introduce how delay distributions can be used to model population-level data generating process during an epidemic.\nThis means working with aggregated count data. This creates some new issues in correctly accounting for uncertainty. We handle population-level data using convolutions as a way of combining count data with a distribution of individual probabilities. We will have to adjust our continuous probability distributions to work with this using discretisation. We’ll then need to re-introduce additional uncertainty to account for the observation process at a population level.\n\n\n\nDelay distributions at the population level\n\n\n\n\nIn this session, we’ll focus on the delay from infection to symptom onset at a population level. First, we will introduce the techniques of convolution and discretisation. We’ll apply these to an aggregated time series of infections in order to simulate observed symptom onsets. Then, we’ll use those simulated symptom onsets to try and reconstruct a time series of infections.\n\n\n\n\n\n\nNoteSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and tidyr packages for data wrangling, the ggplot2 library for plotting and the tidybayes package for extracting results of the inference.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Using delay distributions to model the data generating process"
    ]
  },
  {
    "objectID": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#slides",
    "href": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#slides",
    "title": "Using delay distributions to model the data generating process",
    "section": "",
    "text": "Delay distributions at the population level",
    "crumbs": [
      "Using delay distributions to model the data generating process"
    ]
  },
  {
    "objectID": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#objectives",
    "href": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#objectives",
    "title": "Using delay distributions to model the data generating process",
    "section": "",
    "text": "In this session, we’ll focus on the delay from infection to symptom onset at a population level. First, we will introduce the techniques of convolution and discretisation. We’ll apply these to an aggregated time series of infections in order to simulate observed symptom onsets. Then, we’ll use those simulated symptom onsets to try and reconstruct a time series of infections.\n\n\n\n\n\n\nNoteSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and tidyr packages for data wrangling, the ggplot2 library for plotting and the tidybayes package for extracting results of the inference.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Using delay distributions to model the data generating process"
    ]
  },
  {
    "objectID": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#source-file",
    "href": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#source-file",
    "title": "Using delay distributions to model the data generating process",
    "section": "",
    "text": "The source file of this session is located at sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.qmd.",
    "crumbs": [
      "Using delay distributions to model the data generating process"
    ]
  },
  {
    "objectID": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#libraries-used",
    "href": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#libraries-used",
    "title": "Using delay distributions to model the data generating process",
    "section": "",
    "text": "In this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and tidyr packages for data wrangling, the ggplot2 library for plotting and the tidybayes package for extracting results of the inference.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.",
    "crumbs": [
      "Using delay distributions to model the data generating process"
    ]
  },
  {
    "objectID": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#initialisation",
    "href": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#initialisation",
    "title": "Using delay distributions to model the data generating process",
    "section": "",
    "text": "We set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Using delay distributions to model the data generating process"
    ]
  },
  {
    "objectID": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#delay-distributions-and-convolutions",
    "href": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#delay-distributions-and-convolutions",
    "title": "Using delay distributions to model the data generating process",
    "section": "Delay distributions and convolutions",
    "text": "Delay distributions and convolutions\nIn the last session we simulated individual outcomes from a delay distribution, and then re-estimated the corresponding parameters. However, sometimes we do not have data on these individual-level outcomes, either because they are not recorded or because they cannot be shared, for example due to privacy concerns. At the population level, individual-level delays translate into convolutions.\nIf we have a time series of infections \\(I_t\\) (\\(t=1, 2, 3, \\ldots, t_\\mathrm{max}\\)), where \\(t\\) denotes the day on which the infections occur, and observable outcomes occur with a delay given by a delay distribution \\(p_i\\) (\\(i=0, 1, 2, \\dots, p_\\mathrm{max}\\)), where \\(i\\) is the number of days after infection that the observation happens, then the number of observable outcomes \\(C_t\\) on day \\(t\\) is given by\n\\[\nC_t = \\sum_{i=0}^{i=p_\\mathrm{max}} I_{t-i} p_i\n\\]\nIn other words, the number of observable outcomes on day \\(t\\) is given by the sum of infections on all previous days multiplied by the probability that those infections are observed on day \\(t\\). For example, the observable outcomes \\(C_t\\) could be the number of symptom onsets on day \\(t\\) and \\(p_i\\) is the incubation period.\nWe can use the same data as in the session on biases in delay distributions, but this time we first aggregate this into a daily time series of infections. You can do this using:\n\ninf_ts &lt;- make_daily_infections(infection_times)\n\n\n\n\n\n\n\nNote\n\n\n\nAs before you can look at the code of the function by typing make_daily_infections. The second part of this function is used to add days without infections with a zero count. This will make our calculations easier later (as otherwise we would have to try and detect these in any models that used this data which could be complicated).\n\n\nAnd look at the first few rows of the daily aggregated data:\n\nhead(inf_ts)\n\n# A tibble: 6 × 2\n  infection_day infections\n          &lt;dbl&gt;      &lt;int&gt;\n1             0          1\n2             1          0\n3             2          1\n4             3          0\n5             4          2\n6             5          1\n\n\nNow we can convolve the time series with a delay distribution to get a time series of outcomes as suggested above.\n\nDiscretising a delay distribution\nIn our first session, we decided to assume the delay from infection to symptom onset had a gamma distribution. However, if we want to use the gamma distribution with shape 5 and rate 1 as before, we face a familiar issue. The gamma distribution is a continuous distribution, but now our delay data are in days which are discrete entities. We will assume that both events that make up the delay (here infection and symptom onset) are observed as daily counts (e.g. as number of infections/symptom onsets by calendar date). Therefore, both observations are censored (as events are rounded to the nearest date). This means that our distribution is double interval censored which we encountered in the the biases in delay distribution session, so we need to use the same ideas introduced in that session.\n\n\n\n\n\n\nNoteMathematical Definition (optional): Discretising a delay distribution subject to double interval censoring\n\n\n\n\n\nThe cumulative distribution function (CDF) (\\(F(t)\\)) of a distribution that has a daily censored primary event can be expressed as,\n\\[\nF^*(t) = \\int_0^1 F(t - u) du\n\\]\nIn effect, this is saying that the daily censored CDF is the average of the continuous distributions CDF over all possible event times (here between 0 and 1).\nThe probability mass function (PMF) of this distribution when observed as a daily process (i.e. the secondary event is also daily censored) is then\n\\[\nf_t \\propto F^*(t + 1) - F^*(t - 1)\n\\]\nThe important point is that the ultimately observed PMF is a combination of a primary event daily censoring process and a secondary event daily censoring process.\n\n\n\nWe can think about this via simulation. We do so by generating many replicates of the corresponding random delay, taking into account that we have already rounded down our infection times to infection days. This means that discretising a delay in this context is double censoring as we discussed in the the biases in delay distribution session. In the absence of any other information or model, we assume for our simulation that infection occurred at some random time during the day, with each time equally likely. We can then apply the incubation period using a continuous probability distribution, before once again rounding down to get the day of symptom onset (mimicking daily reporting). We repeat this many times to get the probability mass function that allows us to go from infection days to symptom onset days.\nYou can view the function we have written do this using:\n\ncensored_delay_pmf\n\nfunction (rgen, max, n = 1e+06, ...) \n{\n    first &lt;- runif(n, min = 0, max = 1)\n    second &lt;- first + rgen(n, ...)\n    delay &lt;- floor(second)\n    counts &lt;- table(factor(delay, levels = seq(0, max)))\n    pmf &lt;- counts/sum(counts)\n    return(pmf)\n}\n&lt;bytecode: 0x56101fa92750&gt;\n&lt;environment: namespace:nfidd&gt;\n\n\n\n\n\n\n\n\nNoteTake 3 minutes\n\n\n\nTry to understand the censored_delay_pmf() function above. Try it with a few different probability distributions and parameters, e.g. for the parameters given above and a maximum delay of 2 weeks (14 days) it would be:\n\ngamma_pmf &lt;- censored_delay_pmf(rgamma, max = 14, shape = 5, rate = 1)\ngamma_pmf\n\n\n           0            1            2            3            4            5 \n0.0006738544 0.0213000084 0.0904146458 0.1637886841 0.1914657786 0.1741648410 \n           6            7            8            9           10           11 \n0.1339778840 0.0919285654 0.0582628773 0.0344727114 0.0194606761 0.0103971836 \n          12           13           14 \n0.0055410260 0.0027965460 0.0013547178 \n\nplot(gamma_pmf)\n\n\n\n\n\n\n\n\n\n\n\n\nApplying a convolution\nNext we apply a convolution with the discretised incubation period distribution to the time series of infections, to generate a time series of symptom onsets. The function we have written to do this is called convolve_with_delay\n\nconvolve_with_delay\n\nfunction (ts, delay_pmf) \n{\n    max_delay &lt;- length(delay_pmf) - 1\n    convolved &lt;- numeric(length(ts))\n    for (i in seq_along(ts)) {\n        first_index &lt;- max(1, i - max_delay)\n        ts_segment &lt;- ts[seq(first_index, i)]\n        pmf &lt;- rev(delay_pmf[seq_len(i - first_index + 1)])\n        ret &lt;- sum(ts_segment * pmf)\n        convolved[i] &lt;- ret\n    }\n    convolved\n}\n&lt;bytecode: 0x561020d9bcf8&gt;\n&lt;environment: namespace:nfidd&gt;\n\n\n\n\n\n\n\n\nTipTake 5 minutes\n\n\n\nTry to understand the convolve_with_delay() function above. Try it with a few different time series and delay distributions. How would you create the time series of symptom onsets from infections, using the discretised gamma distribution created above (saved in gamma_pmf)?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nonsets &lt;- convolve_with_delay(inf_ts$infections, gamma_pmf)\n\nLet’s examine the convolution result to understand how it works. First, we look at the infections and delay distribution:\n\nhead(inf_ts$infections)\n\n[1] 1 0 1 0 2 1\n\nhead(gamma_pmf)\n\n\n           0            1            2            3            4            5 \n0.0006738544 0.0213000084 0.0904146458 0.1637886841 0.1914657786 0.1741648410 \n\n\nNow let’s look at the resulting onsets:\n\nhead(onsets)\n\n[1] 0.0006738544 0.0213000084 0.0910885002 0.1850886925 0.2832281333\n[6] 0.3812273963\n\n\nTo understand what’s happening, let’s work through the first few rows.\nFor day 1, only infections from day 1 can contribute:\n\ninf_ts$infections[1] * gamma_pmf[1]\n\n           0 \n0.0006738544 \n\n\nFor day 2, infections from both day 1 and day 2 can contribute:\n\ninf_ts$infections[1] * gamma_pmf[2] + inf_ts$infections[2] * gamma_pmf[1]\n\n         1 \n0.02130001 \n\n\nFor day 3, infections from days 1, 2, and 3 can contribute:\n\ninf_ts$infections[1] * gamma_pmf[3] + inf_ts$infections[2] * gamma_pmf[2] + inf_ts$infections[3] * gamma_pmf[1]\n\n        2 \n0.0910885 \n\n\nEach day’s onsets are the sum of all previous infections weighted by the probability of observing them on that day.\n\n\n\nWe can plot these symptom onsets:\n\ncombined &lt;- inf_ts |&gt;\n  rename(time = infection_day) |&gt;\n  mutate(onsets = onsets)\nggplot(combined, aes(x = time, y = onsets)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\nDo they look similar to the plot of symptom onsets in the session on delay distributions?",
    "crumbs": [
      "Using delay distributions to model the data generating process"
    ]
  },
  {
    "objectID": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#observation-uncertainty",
    "href": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#observation-uncertainty",
    "title": "Using delay distributions to model the data generating process",
    "section": "Observation uncertainty",
    "text": "Observation uncertainty\nUsually not all data are perfectly observed. Also, the convolution we applied is a deterministic operation that brushes over the fact that individual delays are random. We should therefore find another way to model the variation these processes introduce.\nGiven that we are now dealing with count data a natural choice is the Poisson distribution. We can use this to generate uncertainty around our convolved data.\n\ncombined &lt;- combined |&gt;\n  mutate(reported_onsets = rpois(n(), onsets))\n\n\n\n\n\n\n\nTipTake 5 minutes\n\n\n\nDoes a plot of these observations look more like the plots from the session on delay distributions than the convolution plotted above?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nggplot(combined, aes(x = time, y = reported_onsets)) +\n  geom_bar(stat = \"identity\")",
    "crumbs": [
      "Using delay distributions to model the data generating process"
    ]
  },
  {
    "objectID": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#challenge",
    "href": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#challenge",
    "title": "Using delay distributions to model the data generating process",
    "section": "Challenge",
    "text": "Challenge\n\nAbove, we used a Poisson distribution to characterise uncertainty. In the Poisson distribution, the variance is the same as the mean. Another common choice is the negative binomial distribution, which has a more flexible relationship between variance and mean. If you re-did the analysis above with the negative binomial distribution, what would be the difference?\nWe could have used the individual-level model of the previous section to try to estimate the number of infections with a known delay distribution by estimating each individual infection time. How would this look in stan code? Would you expect it to yield a different result?",
    "crumbs": [
      "Using delay distributions to model the data generating process"
    ]
  },
  {
    "objectID": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#methods-in-practice",
    "href": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#methods-in-practice",
    "title": "Using delay distributions to model the data generating process",
    "section": "Methods in practice",
    "text": "Methods in practice\n\nThe primarycensored package provides tools for efficiently working with censored and truncated delay distributions. It offers both theoretical background on primary and secondary interval censoring and right truncation, as well as practical functions for estimating and using these distributions. The package implements deterministic methods that are significantly faster than the Monte Carlo approach used in censored_delay_pmf() while providing exact results.",
    "crumbs": [
      "Using delay distributions to model the data generating process"
    ]
  },
  {
    "objectID": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#references",
    "href": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#references",
    "title": "Using delay distributions to model the data generating process",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Using delay distributions to model the data generating process"
    ]
  },
  {
    "objectID": "sessions/slides/introduction-to-the-spectrum-of-forecasting-models.html#different-types-of-models",
    "href": "sessions/slides/introduction-to-the-spectrum-of-forecasting-models.html#different-types-of-models",
    "title": "Introduction to the spectrum of forecasting models",
    "section": "Different types of models",
    "text": "Different types of models\n\n\n\n\nWe can classify models by the level of mechanism they include\nAll of the model types we will introduce in the next few slides have been used for COVID-19 forecasting (the US and/or European COVID-19 forecast hub)\n\nNOTE: level of mechanism \\(\\neq\\) model complexity\n\n\nCramer et al., Scientific Data, 2022"
  },
  {
    "objectID": "sessions/slides/introduction-to-the-spectrum-of-forecasting-models.html#your-turn",
    "href": "sessions/slides/introduction-to-the-spectrum-of-forecasting-models.html#your-turn",
    "title": "Introduction to the spectrum of forecasting models",
    "section": " Your Turn",
    "text": "Your Turn\n\nReview the performance of the random walk model\nMotivate a mechanism to order to address some of the issues\nMotivate a statistical approach aiming to address the same issues\nCompare the performance of these models for a single forecast\nEvaluate many forecast from these models and compare their performance"
  },
  {
    "objectID": "sessions/slides/introduction-to-stan.html#bayesian-inference-in-10-minutes",
    "href": "sessions/slides/introduction-to-stan.html#bayesian-inference-in-10-minutes",
    "title": "Introduction to Bayesian inference with stan",
    "section": "Bayesian inference in 10 minutes",
    "text": "Bayesian inference in 10 minutes"
  },
  {
    "objectID": "sessions/slides/introduction-to-stan.html#bayesian-inference",
    "href": "sessions/slides/introduction-to-stan.html#bayesian-inference",
    "title": "Introduction to Bayesian inference with stan",
    "section": "Bayesian inference",
    "text": "Bayesian inference\n\nThe generative model can produce output which looks like data given a set of parameters \\(\\theta\\).\nIdea of Bayesian inference: treat \\(\\theta\\) as random variables (with a probability distribution) and condition on data: posterior probability \\(p(\\theta | \\mathrm{data})\\) as target of inference."
  },
  {
    "objectID": "sessions/slides/introduction-to-stan.html#what-is-stan-and-why-do-we-use-it",
    "href": "sessions/slides/introduction-to-stan.html#what-is-stan-and-why-do-we-use-it",
    "title": "Introduction to Bayesian inference with stan",
    "section": "What is stan and why do we use it?",
    "text": "What is stan and why do we use it?\n\na Probabilistic Programming Language for Bayesian inference (i.e., a way to write down models)\nmodels are written in a text file (often ending .stan) and then loaded into an R/python/etc interface\nonce a model is written down, stan can be used to generate samples from the posterior distribution (using a variety of methods)"
  },
  {
    "objectID": "sessions/slides/introduction-to-stan.html#how-to-write-a-model-in-stan",
    "href": "sessions/slides/introduction-to-stan.html#how-to-write-a-model-in-stan",
    "title": "Introduction to Bayesian inference with stan",
    "section": "How to write a model in stan",
    "text": "How to write a model in stan\n\n\nIn a stan model file we specify:\n\nData(types and names)\nParameters(types and names)\nModel(prior and likelihood)\n\n\n\n\ndata {\n\n}\n\nparameters {\n\n}\n\nmodel {\n\n}"
  },
  {
    "objectID": "sessions/slides/introduction-to-stan.html#example-fairness-of-a-coin",
    "href": "sessions/slides/introduction-to-stan.html#example-fairness-of-a-coin",
    "title": "Introduction to Bayesian inference with stan",
    "section": "Example: fairness of a coin",
    "text": "Example: fairness of a coin\n\n\nData:\n\n\\(N\\) coin flips\n\\(x\\) times heads\n\nParameters\n\n\\(\\theta\\), probability of getting heads; uniform prior in \\([0, 1]\\)\n\n\n\n\n\ndata {\n  int&lt;lower = 1&gt; N;  // integer, minimum 1\n  int&lt;lower = 0&gt; x; // integer, minimum 0\n}\n\nparameters {\n  real&lt;lower = 0, upper = 1&gt; theta; // real, between 0 and 1\n}\n\nmodel {\n  // Uniform prior\n  theta ~ uniform(0, 1);\n  // Binomial likelihood\n  x ~ binomial(N, theta);\n}"
  },
  {
    "objectID": "sessions/slides/introduction-to-stan.html#using-stan-from-r",
    "href": "sessions/slides/introduction-to-stan.html#using-stan-from-r",
    "title": "Introduction to Bayesian inference with stan",
    "section": "Using stan from R",
    "text": "Using stan from R\nThere are two packages for using stan from R. We will use the cmdstanr package:\n\nlibrary(\"cmdstanr\")\nmod &lt;- nfidd::nfidd_cmdstan_model(\"coin\")\nmod\n\ndata {\n  int&lt;lower = 1&gt; N;  // integer, minimum 1\n  int&lt;lower = 0&gt; x; // integer, minimum 0\n}\n\nparameters {\n  real&lt;lower = 0, upper = 1&gt; theta; // real, between 0 and 1\n}\n\nmodel {\n  // Uniform prior\n  theta ~ uniform(0, 1);\n  // Binomial likelihood\n  x ~ binomial(N, theta);\n}"
  },
  {
    "objectID": "sessions/slides/introduction-to-stan.html#sampling-from-the-posterior",
    "href": "sessions/slides/introduction-to-stan.html#sampling-from-the-posterior",
    "title": "Introduction to Bayesian inference with stan",
    "section": "Sampling from the posterior",
    "text": "Sampling from the posterior\n\ndata &lt;- list(\n  N = 10, ## 10 coin flips\n  x = 6 ## 6 times heads\n)\nnfidd::nfidd_sample(mod, data = data)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 1 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 1 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 1 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 1 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 1 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 1 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 1 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 1 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 1 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 1 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 1 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 2 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 2 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 2 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 2 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 2 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 2 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 2 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 2 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 2 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 2 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 2 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 2 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 3 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 3 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 3 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 3 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 3 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 3 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 3 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 3 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 3 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 3 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 3 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 3 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 4 Iteration:   1 / 1000 [  0%]  (Warmup) \nChain 4 Iteration: 100 / 1000 [ 10%]  (Warmup) \nChain 4 Iteration: 200 / 1000 [ 20%]  (Warmup) \nChain 4 Iteration: 300 / 1000 [ 30%]  (Warmup) \nChain 4 Iteration: 400 / 1000 [ 40%]  (Warmup) \nChain 4 Iteration: 500 / 1000 [ 50%]  (Warmup) \nChain 4 Iteration: 501 / 1000 [ 50%]  (Sampling) \nChain 4 Iteration: 600 / 1000 [ 60%]  (Sampling) \nChain 4 Iteration: 700 / 1000 [ 70%]  (Sampling) \nChain 4 Iteration: 800 / 1000 [ 80%]  (Sampling) \nChain 4 Iteration: 900 / 1000 [ 90%]  (Sampling) \nChain 4 Iteration: 1000 / 1000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.3 seconds.\n\n\n variable  mean median   sd  mad     q5   q95 rhat ess_bulk ess_tail\n    lp__  -8.66  -8.38 0.73 0.31 -10.06 -8.15 1.01      726      869\n    theta  0.59   0.60 0.13 0.14   0.36  0.80 1.01      715      704"
  },
  {
    "objectID": "sessions/slides/introduction-to-nowcasting.html#your-turn",
    "href": "sessions/slides/introduction-to-nowcasting.html#your-turn",
    "title": "Introduction to nowcasting",
    "section": " Your Turn",
    "text": "Your Turn\n\nPerform nowcast with a known reporting delay distribution\nPerform a nowcast using a more realistic data generating process\nExplore the impact of getting the delay distribution wrong"
  },
  {
    "objectID": "sessions/slides/introduction-to-forecasting.html#your-turn",
    "href": "sessions/slides/introduction-to-forecasting.html#your-turn",
    "title": "Introduction to forecasting",
    "section": " Your Turn",
    "text": "Your Turn\n\nLoad some forecasts we have generated and visualise them alongside the data.\nExplore different ways of visualising forecasts to assess how good the model performs at forecasting."
  },
  {
    "objectID": "sessions/slides/introduction-to-ensembles.html#ensembles-many-forecasts-into-one",
    "href": "sessions/slides/introduction-to-ensembles.html#ensembles-many-forecasts-into-one",
    "title": "Multi-model ensembles",
    "section": "Ensembles: many forecasts into one",
    "text": "Ensembles: many forecasts into one\n\nFigure credit: Evan Ray and Nick Reich"
  },
  {
    "objectID": "sessions/slides/introduction-to-ensembles.html#why-ensemble",
    "href": "sessions/slides/introduction-to-ensembles.html#why-ensemble",
    "title": "Multi-model ensembles",
    "section": "Why ensemble?",
    "text": "Why ensemble?\n\n\nModels are specialists and you want all the perspectives\n\ndifferent data sources\ndifferent philosophies, e.g. more mechanistic or more statistical approaches\ndifferent methodologies and parameterizations\n\n\n\n\n\nA single “consensus forecast” is easier for decision-makers to digest"
  },
  {
    "objectID": "sessions/slides/introduction-to-ensembles.html#whole-is-greater-than-sum-of-parts",
    "href": "sessions/slides/introduction-to-ensembles.html#whole-is-greater-than-sum-of-parts",
    "title": "Multi-model ensembles",
    "section": "“Whole is greater than sum of parts”",
    "text": "“Whole is greater than sum of parts”\nAverage of multiple predictions is often (not always) more performant than any individual model\n\nStrong evidence in weather & economic forecasting\nRecent evidence in infectious disease forecasting\n\nEbola(Funk et al. 2019)\ndengue(colon-gonzalez_probabilistic_2021?)\nflu(reich_accuracy_2019?)\nCOVID-19(cramer_evaluation_2022?)"
  },
  {
    "objectID": "sessions/slides/introduction-to-ensembles.html#ensemble-methods-how-to-average",
    "href": "sessions/slides/introduction-to-ensembles.html#ensemble-methods-how-to-average",
    "title": "Multi-model ensembles",
    "section": "Ensemble methods: how to average?",
    "text": "Ensemble methods: how to average?\n\nFigure credit: Howerton et al. (2023)"
  },
  {
    "objectID": "sessions/slides/introduction-to-ensembles.html#ensemble-methods-to-weight-or-not",
    "href": "sessions/slides/introduction-to-ensembles.html#ensemble-methods-to-weight-or-not",
    "title": "Multi-model ensembles",
    "section": "Ensemble methods: to weight or not?",
    "text": "Ensemble methods: to weight or not?\n\nWeight models by past forecast performance\n\ne.g. using forecast scores\n\nRarely better than equal average\n\nlots of uncertainty in weight estimation!\nput a “strong prior” on equal weights, both in your mental and statistical models"
  },
  {
    "objectID": "sessions/slides/introduction-to-ensembles.html#collaborative-modelling-hubs",
    "href": "sessions/slides/introduction-to-ensembles.html#collaborative-modelling-hubs",
    "title": "Multi-model ensembles",
    "section": "Collaborative modelling “hubs”",
    "text": "Collaborative modelling “hubs”\n\n\n\nProjects run by research groups, public health agencies\nParticipation generally open\nStandard format enables\n\ndata validation\nensemble-building\nmodel evaluation\nvisualization\n\n\n\n\n\n\nHubverse"
  },
  {
    "objectID": "sessions/slides/introduction-to-ensembles.html#hubs-increasingly-used-in-infectious-disease-modelling",
    "href": "sessions/slides/introduction-to-ensembles.html#hubs-increasingly-used-in-infectious-disease-modelling",
    "title": "Multi-model ensembles",
    "section": "Hubs increasingly used in infectious disease modelling",
    "text": "Hubs increasingly used in infectious disease modelling\n\nReich et al. (2022)"
  },
  {
    "objectID": "sessions/slides/introduction-to-ensembles.html#e.g.-the-european-respicast-hub",
    "href": "sessions/slides/introduction-to-ensembles.html#e.g.-the-european-respicast-hub",
    "title": "Multi-model ensembles",
    "section": "… e.g., the European Respicast Hub",
    "text": "… e.g., the European Respicast Hub"
  },
  {
    "objectID": "sessions/slides/introduction-to-ensembles.html#single-model",
    "href": "sessions/slides/introduction-to-ensembles.html#single-model",
    "title": "Multi-model ensembles",
    "section": "Single model",
    "text": "Single model"
  },
  {
    "objectID": "sessions/slides/introduction-to-ensembles.html#multiple-models",
    "href": "sessions/slides/introduction-to-ensembles.html#multiple-models",
    "title": "Multi-model ensembles",
    "section": "… Multiple models",
    "text": "… Multiple models"
  },
  {
    "objectID": "sessions/slides/introduction-to-ensembles.html#multi-model-ensemble",
    "href": "sessions/slides/introduction-to-ensembles.html#multi-model-ensemble",
    "title": "Multi-model ensembles",
    "section": "… … Multi-model ensemble",
    "text": "… … Multi-model ensemble"
  },
  {
    "objectID": "sessions/slides/introduction-to-ensembles.html#your-turn",
    "href": "sessions/slides/introduction-to-ensembles.html#your-turn",
    "title": "Multi-model ensembles",
    "section": " Your Turn",
    "text": "Your Turn\n\nCreate unweighted and weighted ensembles using forecasts from multiple models.\nEvaluate the forecasts from ensembles compared to their constituent models."
  },
  {
    "objectID": "sessions/slides/introduction-to-ensembles.html#references",
    "href": "sessions/slides/introduction-to-ensembles.html#references",
    "title": "Multi-model ensembles",
    "section": "References",
    "text": "References\n\n\nFunk, Sebastian, Anton Camacho, Adam J. Kucharski, Rachel Lowe, Rosalind M. Eggo, and W. John Edmunds. 2019. “Assessing the Performance of Real-Time Epidemic Forecasts: A Case Study of Ebola in the Western Area Region of Sierra Leone, 2014-15.” PLOS Computational Biology 15 (2): e1006785. https://doi.org/10.1371/journal.pcbi.1006785.\n\n\nHowerton, Emily, Michael C. Runge, Tiffany L. Bogich, Rebecca K. Borchering, Hidetoshi Inamine, Justin Lessler, Luke C. Mullany, et al. 2023. “Context-Dependent Representation of Within- and Between-Model Uncertainty: Aggregating Probabilistic Predictions in Infectious Disease Epidemiology.” Journal of The Royal Society Interface 20 (198): 20220659. https://doi.org/10.1098/rsif.2022.0659.\n\n\nReich, Nicholas G, Justin Lessler, Sebastian Funk, Cecile Viboud, Alessandro Vespignani, Ryan J Tibshirani, Katriona Shea, et al. 2022. “Collaborative Hubs: Making the Most of Predictive Epidemic Modeling.” Am. J. Public Health, April, e1–4. https://doi.org/10.2105/ajph.2022.306831."
  },
  {
    "objectID": "sessions/slides/convolutions.html#individual-delay-distributions",
    "href": "sessions/slides/convolutions.html#individual-delay-distributions",
    "title": "Delay distributions at the population level",
    "section": "Individual delay distributions",
    "text": "Individual delay distributions\nLet us consider an epidemiological process that is characterised by a discrete distribution \\(f_d\\).\n\nExample:\n\\(f_d\\): Probability of having incubation period of \\(d\\) days."
  },
  {
    "objectID": "sessions/slides/convolutions.html#individual-to-population-a-simple-example",
    "href": "sessions/slides/convolutions.html#individual-to-population-a-simple-example",
    "title": "Delay distributions at the population level",
    "section": "Individual to population: A simple example",
    "text": "Individual to population: A simple example\nIf we know individual events, we can add a draw from the delay distribution and sum up the resulting delays."
  },
  {
    "objectID": "sessions/slides/convolutions.html#why-not-use-individual-delays",
    "href": "sessions/slides/convolutions.html#why-not-use-individual-delays",
    "title": "Delay distributions at the population level",
    "section": "Why not use individual delays?",
    "text": "Why not use individual delays?\n\nwe don’t always have individual data available\nwe often model other processes at the population level (such as transmission) and so being able to model delays on the same scale is useful\ndoing the computation at the population level requires fewer calculations (i.e. is faster)\n\n\n\nhowever, a downside is that we won’t have realistic uncertainty, especially if the number of individuals is small"
  },
  {
    "objectID": "sessions/slides/convolutions.html#convolution-from-individual-to-population",
    "href": "sessions/slides/convolutions.html#convolution-from-individual-to-population",
    "title": "Delay distributions at the population level",
    "section": "Convolution: From individual to population",
    "text": "Convolution: From individual to population"
  },
  {
    "objectID": "sessions/slides/convolutions.html#population-level-counts-the-mathematics",
    "href": "sessions/slides/convolutions.html#population-level-counts-the-mathematics",
    "title": "Delay distributions at the population level",
    "section": "Population level counts: The mathematics",
    "text": "Population level counts: The mathematics\nIf the number of individuals \\(P_{t'}\\) that have their primary event at time \\(t'\\) then we can rewrite this as\n\\[\nS_t = \\sum_{t'} P_{t'} f_{t - t'}\n\\]\nThis operation is called a (discrete) convolution of \\(P\\) with \\(f_d\\).\nWe can use convolutions with the delay distribution that applies at the individual level to determine population-level counts."
  },
  {
    "objectID": "sessions/slides/convolutions.html#what-if-f-is-continuous",
    "href": "sessions/slides/convolutions.html#what-if-f-is-continuous",
    "title": "Delay distributions at the population level",
    "section": "What if \\(f\\) is continuous?",
    "text": "What if \\(f\\) is continuous?\nHaving moved to the population level, we can’t estimate individual-level event times any more.\nInstead, we discretise the distribution (remembering that it is double censored - as both events are censored).\nThis can be solved mathematically but in the session we will use simulation."
  },
  {
    "objectID": "sessions/slides/convolutions.html#your-turn",
    "href": "sessions/slides/convolutions.html#your-turn",
    "title": "Delay distributions at the population level",
    "section": " Your Turn",
    "text": "Your Turn\n\nSimulate convolutions with infection counts\nDiscretise continuous distributions\nEstimate parameters numbers of infections from number of symptom onsets, using a convolution model"
  },
  {
    "objectID": "sessions/probability-distributions-and-parameter-estimation.html",
    "href": "sessions/probability-distributions-and-parameter-estimation.html",
    "title": "Probability distributions and parameter estimation",
    "section": "",
    "text": "Many important characteristics, or parameters, of epidemiological processes are not fully observed - and are therefore uncertain. For example, in this course this might include time delays, reproduction numbers, or case numbers now and in the future. We can specify the shape of uncertainty around a specific parameter using a probability distribution.\nWe’ll want to correctly specify this uncertainty. We can do this best by combining our own prior understanding with the data that we have available. In this course, we’ll use this Bayesian approach to modelling. A useful tool for creating these models is the stan probabilistic programming language.\n\n\n\nIntroduction to Bayesian inference with stan\n\n\n\n\nThe aim of this session is to introduce the concept of probability distributions and how to estimate their parameters using Bayesian inference with stan.\n\n\n\n\n\n\nNoteSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/probability-distributions-and-parameter-estimation.qmd.\n\n\n\nIn this session we will use the ggplot2 library for plotting and the nfidd package to load the stan models. We will also use the bayesplot and posterior packages to investigate the results of the inference conducted with stan.\n\nlibrary(\"nfidd\")\nlibrary(\"ggplot2\")\nlibrary(\"bayesplot\")\nlibrary(\"posterior\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Probability distributions and parameter estimation"
    ]
  },
  {
    "objectID": "sessions/probability-distributions-and-parameter-estimation.html#slides",
    "href": "sessions/probability-distributions-and-parameter-estimation.html#slides",
    "title": "Probability distributions and parameter estimation",
    "section": "",
    "text": "Introduction to Bayesian inference with stan",
    "crumbs": [
      "Probability distributions and parameter estimation"
    ]
  },
  {
    "objectID": "sessions/probability-distributions-and-parameter-estimation.html#objectives",
    "href": "sessions/probability-distributions-and-parameter-estimation.html#objectives",
    "title": "Probability distributions and parameter estimation",
    "section": "",
    "text": "The aim of this session is to introduce the concept of probability distributions and how to estimate their parameters using Bayesian inference with stan.\n\n\n\n\n\n\nNoteSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/probability-distributions-and-parameter-estimation.qmd.\n\n\n\nIn this session we will use the ggplot2 library for plotting and the nfidd package to load the stan models. We will also use the bayesplot and posterior packages to investigate the results of the inference conducted with stan.\n\nlibrary(\"nfidd\")\nlibrary(\"ggplot2\")\nlibrary(\"bayesplot\")\nlibrary(\"posterior\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Probability distributions and parameter estimation"
    ]
  },
  {
    "objectID": "sessions/probability-distributions-and-parameter-estimation.html#source-file",
    "href": "sessions/probability-distributions-and-parameter-estimation.html#source-file",
    "title": "Probability distributions and parameter estimation",
    "section": "",
    "text": "The source file of this session is located at sessions/probability-distributions-and-parameter-estimation.qmd.",
    "crumbs": [
      "Probability distributions and parameter estimation"
    ]
  },
  {
    "objectID": "sessions/probability-distributions-and-parameter-estimation.html#libraries-used",
    "href": "sessions/probability-distributions-and-parameter-estimation.html#libraries-used",
    "title": "Probability distributions and parameter estimation",
    "section": "",
    "text": "In this session we will use the ggplot2 library for plotting and the nfidd package to load the stan models. We will also use the bayesplot and posterior packages to investigate the results of the inference conducted with stan.\n\nlibrary(\"nfidd\")\nlibrary(\"ggplot2\")\nlibrary(\"bayesplot\")\nlibrary(\"posterior\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.",
    "crumbs": [
      "Probability distributions and parameter estimation"
    ]
  },
  {
    "objectID": "sessions/probability-distributions-and-parameter-estimation.html#initialisation",
    "href": "sessions/probability-distributions-and-parameter-estimation.html#initialisation",
    "title": "Probability distributions and parameter estimation",
    "section": "",
    "text": "We set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Probability distributions and parameter estimation"
    ]
  },
  {
    "objectID": "sessions/probability-distributions-and-parameter-estimation.html#estimating-the-parameters-of-probability-distributions",
    "href": "sessions/probability-distributions-and-parameter-estimation.html#estimating-the-parameters-of-probability-distributions",
    "title": "Probability distributions and parameter estimation",
    "section": "Estimating the parameters of probability distributions",
    "text": "Estimating the parameters of probability distributions\nWe will now use stan to estimate the parameters of the probability distribution. To do so, we first load in the stan model. Normally, you would do this by saving the model code in a text file called gamma_model.stan and then loading this using cmdstanr::cmdstan_model(). For this course, we included all the models in the nfidd package, and they can be accessed using nfidd_cmdstan_model() which uses cmdstanr::cmdstan_model() to load the specified model included with the package. Afterwards, we can use this model exactly how we would if we had loaded it using cmdstanr directly.\n\n### load gamma model from the nfidd package\nmod &lt;- nfidd_cmdstan_model(\"gamma\")\n### show model code\nmod\n\n 1: // gamma_model.stan\n 2: data {\n 3:   int&lt;lower=0&gt; N;\n 4:   array[N] real y;\n 5: }\n 6: \n 7: parameters {\n 8:   real&lt;lower=0&gt; alpha;\n 9:   real&lt;lower=0&gt; beta;\n10: }\n11: \n12: model {\n13:   alpha ~ normal(0, 10) T[0,];\n14:   beta ~ normal(0, 10) T[0,];\n15:   y ~ gamma(alpha, beta);\n16: }\n\n\n\n\n\n\n\n\nTipArrays in stan\n\n\n\nOn line 4 there is a data declaration starting with array[n]. This declares an array of size n of the type given afterwards (here: real). Arrays work in a similar way as arrays or vectors in R and its elements be accessed with the bracket operator [. For example, to get the third element of the array y you would write y[3].\n\n\n\n\n\n\n\n\nTipTake 5 minutes\n\n\n\nFamilarise yourself with the model above. Do you understand all the lines? Which line(s) define the parameter prior distribution(s), which one(s) the likelihood, and which one(s) the data that has to be supplied to the model?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nLines 13 and 14 define the parametric prior distributions (for parameters alpha and beta). The additional qualifier T[0,] specifies that the distribution should be truncated with a lower bound of zero (because the alpha and beta parameters of the gamma distribution have to be nonnegative). Line 15 defines the likelihood. Lines 3 and 4 define the data that has to be supplied to the model.\n\n\n\nWe use the model we have defined in conjunction with the gamma distributed random numbers generated earlier to see if we can recover the parameters of the gamma distribution used. Once you have familiarised yourself with the model, we will use the nfidd_sample() function to fit the model.\n\n\n\n\n\n\nTipnfidd_sample()\n\n\n\nThe nfidd_sample() function is a wrapper around the cmdstanr sample() method. It provides sensible defaults for the course and simplifies the syntax. In particular, we are generating 500 samples instead of the default of 1000 to make the models run faster. When running your models, you can use the diagnostic capabilities of stan to test for convergence and ensure that you have generated enough samples: mod$diagnose(). For example, nfidd_sample(mod, data = stan_data) is equivalent to mod$sample(data = stan_data, parallel_chains = 4, iter_wamup = 500, iter_sampling = 500). You can still pass additional arguments like init, adapt_delta, or max_treedepth when needed.\n\n\n\nstan_data &lt;- list(\n  N = length(gammas),\n  y = gammas\n)\ngamma_fit &lt;- nfidd_sample(mod, data = stan_data)\n\n\n\n\n\n\n\nNotePassing data to the stan model\n\n\n\nThe stan_data object is a list with elements that will is passed to the stan model as the data argument to sample. The names and types of the elements need to correspond to the data block in the model (see lines 3 and 4 in the model). Here, we pass the length of gammas as N and the vector gammas itself as y.\n\n\n\n\n\n\n\n\nNoteStan messages\n\n\n\nThe nfidd_sample command will produce a lot of messages which we have suppressed above. This is fine and intended to keep the user informed about any issues as well as general progress with the inference. This will come in handy later in the course when we fit more complicated models that can take a little while to run.\n\n\nIn order to view a summary of the posterior samples generated, use\n\ngamma_fit\n\n variable    mean  median   sd  mad      q5     q95 rhat ess_bulk ess_tail\n    lp__  -151.11 -150.80 1.01 0.69 -153.07 -150.18 1.01      431      391\n    alpha   17.97   17.80 2.42 2.38   14.31   22.22 1.01      236      258\n    beta     3.61    3.57 0.49 0.48    2.87    4.50 1.01      236      265\n\n\nYou can see that the estimates are broadly consistent with the parameters we specified. To investigate this further, we will conduct a so-called posterior predictive check by comparing random numbers simulated using the estimated parameters to the ones we simulated earlier.\n\n## Extract posterior draws\ngamma_posterior &lt;- as_draws_df(gamma_fit$draws())\nhead(gamma_posterior)\n\n# A draws_df: 6 iterations, 1 chains, and 3 variables\n  lp__ alpha beta\n1 -151    15  3.1\n2 -150    17  3.4\n3 -150    18  3.7\n4 -152    18  3.8\n5 -150    17  3.4\n6 -150    16  3.3\n# ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n\n## Generate posterior predictive samples\ngamma_ppc &lt;- sapply(seq_along(gammas), function(i) {\n  rgamma(n = length(gammas),\n         shape = gamma_posterior$alpha[i],\n         rate = gamma_posterior$beta[i])\n})\n\n## Plot posterior predictive check\nppc_dens_overlay(y = gammas, yrep = gamma_ppc)\n\n\n\n\n\n\n\n\nWe can see that the random numbers generated from the posterior samples are distributed relatively evenly around the data (in black), i.e., the samples generated earlier that we fitted to.",
    "crumbs": [
      "Probability distributions and parameter estimation"
    ]
  },
  {
    "objectID": "sessions/probability-distributions-and-parameter-estimation.html#challenge",
    "href": "sessions/probability-distributions-and-parameter-estimation.html#challenge",
    "title": "Probability distributions and parameter estimation",
    "section": "Challenge",
    "text": "Challenge\n\nFor the model above we chose truncated normal priors with a mode at 0 and standard deviation 10. If you change the parameters of the prior distributions, does it affect the results?\nYou could try the model included in lognormal.stan to estimate parameters of the lognormal distribution.",
    "crumbs": [
      "Probability distributions and parameter estimation"
    ]
  },
  {
    "objectID": "sessions/probability-distributions-and-parameter-estimation.html#methods-in-practice",
    "href": "sessions/probability-distributions-and-parameter-estimation.html#methods-in-practice",
    "title": "Probability distributions and parameter estimation",
    "section": "Methods in practice",
    "text": "Methods in practice\n\nIn this course we are building an intuition for the epidemiological processes and uncertainties that underlie typical surveillance data. We’ll continue to work in stan to build up from simple to more complex models based on our understanding of these data-generating processes. In practice, you might also want to consider other available tools and/or methods. This can be a trade-off: on one hand, adopting an off-the-shelf tool loses the flexibility of a custom approach that is customised to a specific epidemiological context. On the other hand, using available tools avoids duplicated effort, might improve the chances of finding errors, and enables discussing and ultimately enforcing best practice in a rapidly developing field.\nThroughout the course, we’ll suggest some examples of available tools, methods, and best practices for you to consider. We encourage you to choose a strategy for model building that’s most appropriate for your context.",
    "crumbs": [
      "Probability distributions and parameter estimation"
    ]
  },
  {
    "objectID": "sessions/probability-distributions-and-parameter-estimation.html#references",
    "href": "sessions/probability-distributions-and-parameter-estimation.html#references",
    "title": "Probability distributions and parameter estimation",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Probability distributions and parameter estimation"
    ]
  },
  {
    "objectID": "sessions/methods-in-the-real-world.html",
    "href": "sessions/methods-in-the-real-world.html",
    "title": "Methods in the real world",
    "section": "",
    "text": "Objectives\nThe aim of this session is to provide an overview of the different methods that are used in the real world from the perspective of the particpants and course instructors.",
    "crumbs": [
      "Methods in the real world"
    ]
  },
  {
    "objectID": "sessions/introduction-and-course-overview.html",
    "href": "sessions/introduction-and-course-overview.html",
    "title": "Introduction and course overview",
    "section": "",
    "text": "From an epidemiological line list to informing decisions in real-time",
    "crumbs": [
      "Introduction and course overview"
    ]
  },
  {
    "objectID": "sessions/introduction-and-course-overview.html#footnotes",
    "href": "sessions/introduction-and-course-overview.html#footnotes",
    "title": "Introduction and course overview",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTime travel is messy stuff↩︎",
    "crumbs": [
      "Introduction and course overview"
    ]
  },
  {
    "objectID": "sessions/forecasting-concepts.html",
    "href": "sessions/forecasting-concepts.html",
    "title": "Forecasting concepts",
    "section": "",
    "text": "Epidemiological forecasts are probabilistic statements about what might happen to population disease burden in the future. In this session we will make some simple forecasts using a commonly used infectious disease model, based on the renewal equation. We will see how we can visualise such forecasts, and visually compare them to what really happened.\n\n\n\nIntroduction to forecasting\n\n\n\n\nThe aim of this session is to introduce the concept of forecasting and forecast visualisation using a simple model.\n\n\n\n\n\n\nNoteSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/forecast-visualisation.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and corresponding forecasts, the dplyr package for data wrangling, the ggplot2 library for plotting and the tidybayes package for extracting results of the inference.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Forecasting concepts"
    ]
  },
  {
    "objectID": "sessions/forecasting-concepts.html#slides",
    "href": "sessions/forecasting-concepts.html#slides",
    "title": "Forecasting concepts",
    "section": "",
    "text": "Introduction to forecasting",
    "crumbs": [
      "Forecasting concepts"
    ]
  },
  {
    "objectID": "sessions/forecasting-concepts.html#objectives",
    "href": "sessions/forecasting-concepts.html#objectives",
    "title": "Forecasting concepts",
    "section": "",
    "text": "The aim of this session is to introduce the concept of forecasting and forecast visualisation using a simple model.\n\n\n\n\n\n\nNoteSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/forecast-visualisation.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and corresponding forecasts, the dplyr package for data wrangling, the ggplot2 library for plotting and the tidybayes package for extracting results of the inference.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Forecasting concepts"
    ]
  },
  {
    "objectID": "sessions/forecasting-concepts.html#source-file",
    "href": "sessions/forecasting-concepts.html#source-file",
    "title": "Forecasting concepts",
    "section": "",
    "text": "The source file of this session is located at sessions/forecast-visualisation.qmd.",
    "crumbs": [
      "Forecasting concepts"
    ]
  },
  {
    "objectID": "sessions/forecasting-concepts.html#libraries-used",
    "href": "sessions/forecasting-concepts.html#libraries-used",
    "title": "Forecasting concepts",
    "section": "",
    "text": "In this session we will use the nfidd package to load a data set of infection times and corresponding forecasts, the dplyr package for data wrangling, the ggplot2 library for plotting and the tidybayes package for extracting results of the inference.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.",
    "crumbs": [
      "Forecasting concepts"
    ]
  },
  {
    "objectID": "sessions/forecasting-concepts.html#initialisation",
    "href": "sessions/forecasting-concepts.html#initialisation",
    "title": "Forecasting concepts",
    "section": "",
    "text": "We set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Forecasting concepts"
    ]
  },
  {
    "objectID": "sessions/forecasting-concepts.html#take-5-minutes",
    "href": "sessions/forecasting-concepts.html#take-5-minutes",
    "title": "Forecasting concepts",
    "section": "Take 5 minutes",
    "text": "Take 5 minutes\nWhat have we changed in the model to make it a forecasting model? Do you see any limitations of this approach?",
    "crumbs": [
      "Forecasting concepts"
    ]
  },
  {
    "objectID": "sessions/forecasting-concepts.html#fitting-the-model-and-forecast-for-28-days-into-the-future",
    "href": "sessions/forecasting-concepts.html#fitting-the-model-and-forecast-for-28-days-into-the-future",
    "title": "Forecasting concepts",
    "section": "Fitting the model and forecast for 28 days into the future",
    "text": "Fitting the model and forecast for 28 days into the future\nWe can now fit the model to the data and then make a forecast. This should look very similar to the code we used in the renewal session but with the addition of a non-zero h in the data list.\n\nhorizon &lt;- 28\n\ndata &lt;- list(\n  n = nrow(filtered_onset_df),\n  I0 = 1,\n  obs = filtered_onset_df$onsets,\n  gen_time_max = length(gen_time_pmf),\n  gen_time_pmf = gen_time_pmf,\n  ip_max = length(ip_pmf) - 1,\n  ip_pmf = ip_pmf,\n  h = horizon # Here we set the number of days to forecast into the future\n)\nrw_forecast &lt;- nfidd_sample(\n  mod, data = data, adapt_delta = 0.95,\n  init = \\() list(init_R = 1, rw_sd = 0.01)\n)\n\n\nrw_forecast\n\n    variable    mean  median   sd  mad      q5     q95 rhat ess_bulk ess_tail\n lp__        3218.22 3218.43 7.16 7.38 3206.36 3229.32 1.01      888     1309\n init_R         1.53    1.52 0.10 0.08    1.39    1.71 1.00     1606     1153\n rw_noise[1]    0.05    0.02 1.00 0.96   -1.67    1.69 1.00     3227     1350\n rw_noise[2]    0.04    0.06 1.00 1.01   -1.63    1.68 1.00     3488     1429\n rw_noise[3]    0.00    0.03 0.98 0.96   -1.64    1.67 1.00     3366     1517\n rw_noise[4]   -0.05   -0.04 1.01 0.97   -1.72    1.61 1.00     3156     1356\n rw_noise[5]   -0.04   -0.04 0.94 0.93   -1.57    1.51 1.01     3094     1559\n rw_noise[6]   -0.13   -0.14 1.02 0.99   -1.78    1.56 1.00     3471     1547\n rw_noise[7]   -0.14   -0.14 1.01 0.99   -1.82    1.57 1.00     2772     1356\n rw_noise[8]   -0.18   -0.17 0.96 0.99   -1.72    1.38 1.00     3536     1417\n\n # showing 10 of 426 rows (change via 'max_rows' argument or 'cmdstanr_max_rows' option)\n\n\n\n\n\n\n\n\nNote\n\n\n\nBecause this model can struggle to fit to the data, we have increased the value of adapt_delta from its default value of 0.8. This is a tuning parameter that affects the step size of the sampler in exploring the posterior (higher adapt_delta leading to smaller step sizes meaning posterior exploration is slower but more careful).",
    "crumbs": [
      "Forecasting concepts"
    ]
  },
  {
    "objectID": "sessions/forecasting-concepts.html#visualising-the-forecast-of-the-reproduction-number",
    "href": "sessions/forecasting-concepts.html#visualising-the-forecast-of-the-reproduction-number",
    "title": "Forecasting concepts",
    "section": "Visualising the forecast of the reproduction number",
    "text": "Visualising the forecast of the reproduction number\nAs our forecasting model is based on the reproduction number, we can also visualise the forecast of the reproduction number. This can be helpful for understanding why our forecasts of symptom onsets look the way they do and for understanding the uncertainty in the forecasts. We can also compare this to the “true” reproduction number, estimated once all relevant data is available. To do this, we will fit the model again but with a later cutoff. Then we can compare the reproduction numbers produced as forecasts at the earlier time, with estimates at the later time that used more of the data.\n\nlong_onset_df &lt;- onset_df |&gt;\n  filter(day &lt;= cutoff + horizon)\n\nlong_data &lt;- list(\n  n = nrow(long_onset_df),\n  I0 = 1,\n  obs = long_onset_df$onsets,\n  gen_time_max = length(gen_time_pmf),\n  gen_time_pmf = gen_time_pmf,\n  ip_max = length(ip_pmf) - 1,\n  ip_pmf = ip_pmf,\n  h = 0\n)\n\n\nrw_long &lt;- nfidd_sample(mod,\n  data = long_data,\n  adapt_delta = 0.95,\n  init = \\() list(init_R = 1, rw_sd = 0.01)\n)\n\nWe first need to extract the forecast and estimated reproduction numbers.\n\nforecast_r &lt;- rw_forecast |&gt;\n  gather_draws(R[day]) |&gt;\n  ungroup() |&gt; \n  mutate(type = \"forecast\")\n\nlong_r &lt;- rw_long |&gt;\n  gather_draws(R[day]) |&gt;\n  ungroup() |&gt;\n  mutate(type = \"estimate\")\n\nWe can now plot the forecast and estimated reproduction numbers.\n\nforecast_r |&gt;\n  bind_rows(long_r) |&gt;\n  filter(.draw %in% sample(.draw, 100)) |&gt;\n  ggplot(aes(x = day)) +\n  geom_vline(xintercept = cutoff, linetype = \"dashed\") +\n  geom_hline(yintercept = 1, linetype = \"dashed\") +\n  geom_line(\n    aes(y = .value, group = interaction(.draw, type), color = type),\n    alpha = 0.1\n  ) +\n  labs(\n    title = \"Estimated R\",\n    subtitle = \"Estimated over whole time series (red), and forecast (blue)\"\n  ) +\n  guides(colour = guide_legend(override.aes = list(alpha = 1)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe horizontal dashed line at 1 represents the threshold for epidemic growth. If the reproduction number is above 1 then the epidemic is growing, if it is below 1 then the epidemic is shrinking. The vertical dashed line represents the point at which we started forecasting.\n\n\n\n\n\n\n\n\nTipTake 5 minutes\n\n\n\nCan you use this plot to explain why the forecast of onsets looks the way it does?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nWhen both models are being fit to data (i.e before the vertical dashed line) the forecast and estimated reproduction numbers are very similar.\nFor short-term forecasts \\(R_t\\) estimates continue to be fairly similar.\nHowever, the estimates have a consistent downwards trend which is not captured by the forecast (which looks like it has a constant mean value with increasing uncertainty).\nThis explains the divergence between the forecast and the data as the horizon increases.\nIt looks like only a relatively small number of forecast \\(R_t\\) trajectories grow to be very large but these are enough to visually dominate the forecast of onsets on the natural scale.\nThe performance we are seeing here makes sense given that random walks are defined to have a constant mean and increasing variance.\n\n\n\n\nWe managed to learn quite a lot about our model’s forecasting limitations just looking at a single forecast using visualisations. However, what if we wanted to quantify how well the model is doing? In order to do that we need to look at multiple forecasts from the model.",
    "crumbs": [
      "Forecasting concepts"
    ]
  },
  {
    "objectID": "sessions/forecasting-concepts.html#visualising-multiple-forecasts-from-a-single-model",
    "href": "sessions/forecasting-concepts.html#visualising-multiple-forecasts-from-a-single-model",
    "title": "Forecasting concepts",
    "section": "Visualising multiple forecasts from a single model",
    "text": "Visualising multiple forecasts from a single model\nAs for a single forecast, our first step is to visualise the forecasts as this can give us a good idea of how well the model is doing without having to calculate any metrics.\n\nrw_forecasts |&gt;\n  filter(.draw %in% sample(.draw, 100)) |&gt;\n  ggplot(aes(x = day)) +\n  geom_line(\n    aes(y = .value, group = interaction(.draw, origin_day), col = origin_day),\n    alpha = 0.1\n  ) +\n  geom_point(\n    data = onset_df |&gt;\n      filter(day &gt;= 21),\n    aes(x = day, y = onsets), color = \"black\") +\n  scale_color_binned(type = \"viridis\") +\n  labs(title = \"Weekly forecasts of symptom onsets over an outbreak\",\n       col = \"Forecast start day\")\n\n\n\n\n\n\n\n\nAs for the single forecast it may be helpful to also plot the forecast on the log scale.\n\nrw_forecasts |&gt;\n  filter(.draw %in% sample(.draw, 100)) |&gt; \n  ggplot(aes(x = day)) +\n  geom_line(\n    aes(y = .value, group = interaction(.draw, origin_day), col = origin_day),\n    alpha = 0.1\n  ) +\n  geom_point(data = onset_df, aes(x = day, y = onsets), color = \"black\") +\n  scale_y_log10() +\n  scale_color_binned(type = \"viridis\") +\n  labs(title = \"Weekly symptom onset forecasts: log scale\",\n       col = \"Forecast start day\")\n\nWarning in scale_y_log10(): log-10 transformation introduced infinite values.\nlog-10 transformation introduced infinite values.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipTake 5 minutes\n\n\n\nWhat do you think of these forecasts? Are they any good? How well do they capture changes in trend? Does the uncertainty seem reasonable? Do they seem to under or over predict consistently? Would you visualise the forecast in a different way?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWhat do you think of these forecasts?\n\nWe think these forecasts are a reasonable place to start but there is definitely room for improvement.\n\nAre they any good?\n\nThey seem to do a reasonable job of capturing the short term dynamics but struggle with the longer term dynamics.\n\nHow well do they capture changes in trend?\n\nThere is little evidence of the model capturing the reduction in onsets before it begins to show in the data.\n\nDoes the uncertainty seem reasonable?\n\nOn the natural scale it looks like the model often over predicts. Things seem more balanced on the log scale but the model still seems to be overly uncertain.\n\nDo they seem to under or over predict consistently?\n\nIt looks like the model is consistently over predicting on the natural scale but this is less clear on the log scale.",
    "crumbs": [
      "Forecasting concepts"
    ]
  },
  {
    "objectID": "sessions/forecasting-concepts.html#challenge",
    "href": "sessions/forecasting-concepts.html#challenge",
    "title": "Forecasting concepts",
    "section": "Challenge",
    "text": "Challenge\n\nWhat other ways could we visualise the forecasts - especially if we had forecasts from multiple models and locations, or both?\nBased on these visualisations how could we improve the model?",
    "crumbs": [
      "Forecasting concepts"
    ]
  },
  {
    "objectID": "sessions/forecasting-concepts.html#methods-in-practice",
    "href": "sessions/forecasting-concepts.html#methods-in-practice",
    "title": "Forecasting concepts",
    "section": "Methods in practice",
    "text": "Methods in practice\n\nHeld, Meyer, and Bracher (2017) makes a compelling argument for the use of probabilistic forecasts and evaluates spatial forecasts based on routine surveillance data.\nHadley et al. (2025) describes the visualisation of modelling results for national COVID-19 policy and decision makers, with recommendations on what aspects of visuals, graphs, and plots policymakers found to be most helpful in COVID-19 response work.",
    "crumbs": [
      "Forecasting concepts"
    ]
  },
  {
    "objectID": "sessions/forecasting-concepts.html#references",
    "href": "sessions/forecasting-concepts.html#references",
    "title": "Forecasting concepts",
    "section": "References",
    "text": "References\n\n\nHadley, Liza, Caylyn Rich, Alex Tasker, Olivier Restif, and Sebastian Funk. 2025. “Visual Preferences for Communicating Modelling: A Global Analysis of COVID-19 Policy and Decision Makers.” medRxiv. https://doi.org/10.1101/2024.11.05.24316774.\n\n\nHeld, Leonhard, Sebastian Meyer, and Johannes Bracher. 2017. “Probabilistic Forecasting in Infectious Disease Epidemiology: The 13th Armitage Lecture.” Statistics in Medicine 36 (22): 3443–60. https://doi.org/10.1002/sim.7363.",
    "crumbs": [
      "Forecasting concepts"
    ]
  },
  {
    "objectID": "sessions/forecast-ensembles.html",
    "href": "sessions/forecast-ensembles.html",
    "title": "Forecast ensembles",
    "section": "",
    "text": "As we saw in the session on different forecasting models, different modelling approaches have different strength and weaknesses, and we do not usually know in advance which one will produce the best forecast in any given situation. We can classify models along a spectrum by how much they include an understanding of underlying processes, or mechanisms; or whether they emphasise drawing from the data using a statistical approach. One way to attempt to draw strength from a diversity of approaches is the creation of so-called forecast ensembles from the forecasts produced by different models.\nIn this session, we’ll start with forecasts from the models we explored in the forecasting models session and build ensembles of these models. We will then compare the performance of these ensembles to the individual models and to each other.\n\n\n\n\n\n\nNoteRepresentations of probabilistic forecasts\n\n\n\n\n\nProbabilistic predictions can be described as coming from a probabilistic probability distributions. In general and when using complex models such as the one we discuss in this course, these distributions can not be expressed in a simple analytical formal as we can do if, e.g. talking about common probability distributions such as the normal or gamma distributions. Instead, we typically use a limited number of samples generated from Monte-Carlo methods to represent the predictive distribution. However, this is not the only way to characterise distributions.\nA quantile is the value that corresponds to a given quantile level of a distribution. For example, the median is the 50th quantile of a distribution, meaning that 50% of the values in the distribution are less than the median and 50% are greater. Similarly, the 90th quantile is the value that corresponds to 90% of the distribution being less than this value. If we characterise a predictive distribution by its quantiles, we specify these values at a range of specific quantile levels, e.g. from 5% to 95% in 5% steps.\nDeciding how to represent forecasts depends on many things, for example the method used (and whether it produces samples by default) but also logistic considerations. Many collaborative forecasting projects and so-called forecasting hubs use quantile-based representations of forecasts in the hope to be able to characterise both the centre and tails of the distributions more reliably and with less demand on storage space than a sample-based representation.\n\n\n\n\n\n\nIntroduction to ensembles\n\n\n\n\nThe aim of this session is to introduce the concept of ensembles of forecasts and to evaluate the performance of ensembles of the models we explored in the forecasting models session.\n\n\n\n\n\n\nNoteSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/forecast-ensembles.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and tidyr packages for data wrangling, ggplot2 library for plotting, the tidybayes package for extracting results of the inference and the scoringutils package for evaluating forecasts. We will also use qrensemble for quantile regression averaging and lopensemble for mixture ensembles (also called linear opinion pool).\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"scoringutils\")\nlibrary(\"qrensemble\")\nlibrary(\"lopensemble\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do.\n\nset.seed(123)",
    "crumbs": [
      "Forecast ensembles"
    ]
  },
  {
    "objectID": "sessions/forecast-ensembles.html#slides",
    "href": "sessions/forecast-ensembles.html#slides",
    "title": "Forecast ensembles",
    "section": "",
    "text": "Introduction to ensembles",
    "crumbs": [
      "Forecast ensembles"
    ]
  },
  {
    "objectID": "sessions/forecast-ensembles.html#objectives",
    "href": "sessions/forecast-ensembles.html#objectives",
    "title": "Forecast ensembles",
    "section": "",
    "text": "The aim of this session is to introduce the concept of ensembles of forecasts and to evaluate the performance of ensembles of the models we explored in the forecasting models session.\n\n\n\n\n\n\nNoteSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/forecast-ensembles.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and tidyr packages for data wrangling, ggplot2 library for plotting, the tidybayes package for extracting results of the inference and the scoringutils package for evaluating forecasts. We will also use qrensemble for quantile regression averaging and lopensemble for mixture ensembles (also called linear opinion pool).\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"scoringutils\")\nlibrary(\"qrensemble\")\nlibrary(\"lopensemble\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do.\n\nset.seed(123)",
    "crumbs": [
      "Forecast ensembles"
    ]
  },
  {
    "objectID": "sessions/forecast-ensembles.html#source-file",
    "href": "sessions/forecast-ensembles.html#source-file",
    "title": "Forecast ensembles",
    "section": "",
    "text": "The source file of this session is located at sessions/forecast-ensembles.qmd.",
    "crumbs": [
      "Forecast ensembles"
    ]
  },
  {
    "objectID": "sessions/forecast-ensembles.html#libraries-used",
    "href": "sessions/forecast-ensembles.html#libraries-used",
    "title": "Forecast ensembles",
    "section": "",
    "text": "In this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and tidyr packages for data wrangling, ggplot2 library for plotting, the tidybayes package for extracting results of the inference and the scoringutils package for evaluating forecasts. We will also use qrensemble for quantile regression averaging and lopensemble for mixture ensembles (also called linear opinion pool).\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"scoringutils\")\nlibrary(\"qrensemble\")\nlibrary(\"lopensemble\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.",
    "crumbs": [
      "Forecast ensembles"
    ]
  },
  {
    "objectID": "sessions/forecast-ensembles.html#initialisation",
    "href": "sessions/forecast-ensembles.html#initialisation",
    "title": "Forecast ensembles",
    "section": "",
    "text": "We set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do.\n\nset.seed(123)",
    "crumbs": [
      "Forecast ensembles"
    ]
  },
  {
    "objectID": "sessions/forecast-ensembles.html#quantile-based-ensembles",
    "href": "sessions/forecast-ensembles.html#quantile-based-ensembles",
    "title": "Forecast ensembles",
    "section": "Quantile-based ensembles",
    "text": "Quantile-based ensembles\nWe will first consider forecasts based on the individual quantiles of each model. This corresponds to a situation where each forecast aims to correctly determine a single target predictive distribution. By taking an average of all models, we aim to get a better estimate of this distribution than from the individual models. If we have reason to believe that some models are better than others at estimating this distribution, we can create a weighted version of this average.\n\nConverting sample-based forecasts to quantile-based forecasts\nAs in this session we will be thinking about forecasts in terms quantiles of the predictive distributions, we will need to convert our sample based forecasts to quantile-based forecasts. We will do this by focusing at the marginal distribution at each predicted time point, that is we treat each time point as independent of all others and calculate quantiles based on the sample predictive trajectories at that time point. An easy way to do this is to use the {scoringutils} package. The steps to do this are to first declare the forecasts as sample forecasts.\n\nsample_forecasts &lt;- forecasts |&gt;\n  left_join(onset_df, by = \"day\") |&gt;\n  filter(!is.na(.value)) |&gt;\n  as_forecast_sample(\n    forecast_unit = c(\"origin_day\", \"horizon\", \"model\", \"day\"),\n    observed = \"onsets\",\n    predicted = \".value\",\n    sample_id = \".draw\"\n  )\nsample_forecasts\n\nForecast type: sample\n\n\nForecast unit:\n\n\norigin_day, horizon, model, and day\n\n\n\n        sample_id predicted observed origin_day horizon            model   day\n            &lt;int&gt;     &lt;num&gt;    &lt;int&gt;      &lt;num&gt;   &lt;int&gt;           &lt;char&gt; &lt;num&gt;\n     1:         1         9        4         22       1      Random walk    23\n     2:         2         5        4         22       1      Random walk    23\n     3:         3         5        4         22       1      Random walk    23\n     4:         4         3        4         22       1      Random walk    23\n     5:         5         5        4         22       1      Random walk    23\n    ---                                                                       \n671996:       996         4        2        127      14 More mechanistic   141\n671997:       997         2        2        127      14 More mechanistic   141\n671998:       998         1        2        127      14 More mechanistic   141\n671999:       999         2        2        127      14 More mechanistic   141\n672000:      1000         1        2        127      14 More mechanistic   141\n\n\nand then convert to quantile forecasts.\n\nquantile_forecasts &lt;- sample_forecasts |&gt;\n  as_forecast_quantile()\nquantile_forecasts\n\nForecast type: quantile\n\n\nForecast unit:\n\n\norigin_day, horizon, model, and day\n\n\n\n      observed origin_day horizon            model   day quantile_level\n         &lt;int&gt;      &lt;num&gt;   &lt;int&gt;           &lt;char&gt; &lt;num&gt;          &lt;num&gt;\n   1:        4         22       1      Random walk    23           0.05\n   2:        4         22       1      Random walk    23           0.25\n   3:        4         22       1      Random walk    23           0.50\n   4:        4         22       1      Random walk    23           0.75\n   5:        4         22       1      Random walk    23           0.95\n  ---                                                                  \n3356:        2        127      14 More mechanistic   141           0.05\n3357:        2        127      14 More mechanistic   141           0.25\n3358:        2        127      14 More mechanistic   141           0.50\n3359:        2        127      14 More mechanistic   141           0.75\n3360:        2        127      14 More mechanistic   141           0.95\n      predicted\n          &lt;num&gt;\n   1:         1\n   2:         2\n   3:         3\n   4:         5\n   5:         8\n  ---          \n3356:         0\n3357:         2\n3358:         3\n3359:         4\n3360:         6\n\n\n\n\n\n\n\n\nTipWhat is happening here?\n\n\n\n\n\n\nInternally scoringutils is calculating the quantiles of the sample-based forecasts.\nIt does this by using a set of default quantiles but different ones can be specified by the user to override the default.\nIt then calls the quantile() function from base R to calculate the quantiles.\nThis is estimating the value that corresponds to each given quantile level by ordering the samples and then taking the value at the appropriate position.\n\n\n\n\n\n\nSimple unweighted ensembles\nA good place to start when building ensembles is to take the mean or median of the unweighted forecast at each quantile level, and treat these as quantiles of the ensemble predictive distribution. Typically, the median is preferred when outlier forecasts are likely to be present as it is less sensitive to these. However, the mean is preferred when forecasters have more faith in models that diverge from the median performance and want to represent this in the ensemble.\n\n\n\n\n\n\nNoteVincent average\n\n\n\n\n\nThe procedure of calculating quantiles of a new distribution as a weighted average of quantiles of constituent distributions (e.g., different measurements) is called a Vincent average, after the biologist Stella Vincent who described this as early as 1912 when studying the function of whiskers in the behaviour of white rats.\n\n\n\n\nConstruction\nWe can calculate the mean quantile ensemble by taking the mean of the forecasts at each quantile level.\n\nmean_ensemble &lt;- quantile_forecasts |&gt;\n  as_tibble() |&gt;\n  summarise(\n    predicted = mean(predicted),\n    observed = unique(observed),\n    model = \"Mean ensemble\",\n    .by = c(origin_day, horizon, quantile_level, day)\n  )\n\nSimilarly, we can calculate the median ensemble by taking the median of the forecasts at each quantile level.\n\nmedian_ensemble &lt;- quantile_forecasts |&gt;\n  as_tibble() |&gt;\n  summarise(\n    predicted = median(predicted),\n    observed = unique(observed),\n    model = \"Median ensemble\",\n    .by = c(origin_day, horizon, quantile_level, day)\n  )\n\nWe combine the ensembles into a single data frame along with the individual forecasts in order to make visualisation easier.\n\nsimple_ensembles &lt;- bind_rows(\n  mean_ensemble,\n  median_ensemble,\n  quantile_forecasts\n)\n\n\n\nVisualisation\nHow do these ensembles visually differ from the individual models? Lets start by plotting a single forecast from each model and comparing them.\n\nplot_ensembles &lt;- function(data, obs_data) {\n  data |&gt;\n    pivot_wider(names_from = quantile_level, values_from = predicted) |&gt;\n    ggplot(aes(x = day)) +\n    geom_ribbon(\n    aes(\n      ymin = .data[[\"0.05\"]], ymax = .data[[\"0.95\"]], fill = model,\n        group = origin_day\n      ),\n      alpha = 0.2\n    ) +\n    geom_ribbon(\n      aes(\n        ymin = .data[[\"0.25\"]], ymax = .data[[\"0.75\"]], fill = model,\n        group = origin_day\n      ),\n      alpha = 0.5\n    ) +\n    geom_point(\n      data = obs_data,\n      aes(x = day, y = onsets), color = \"black\"\n    ) +\n    scale_color_binned(type = \"viridis\") +\n    facet_wrap(~model) +\n    theme(legend.position = \"none\")\n}\n\nplot_single_forecasts &lt;- simple_ensembles |&gt;\n  filter(origin_day == 57) |&gt;\n  plot_ensembles(onset_df |&gt; filter(day &gt;= 57, day &lt;= 57 + 14))\n\nplot_single_forecasts\n\n\n\n\n\n\n\n\nAgain we can get a different perspective by plotting the forecasts on the log scale.\n\nplot_single_forecasts +\n  scale_y_log10()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipTake 2 minutes\n\n\n\nHow do these ensembles compare to the individual models? How do they differ from each other?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nHow do these ensembles compare to the individual models?\n\nBoth of the simple ensembles appear to be less variable than the statistical models but are more variable than the mechanistic model.\nBoth ensembles are more like the statistical models than the mechanistic model.\n\nHow do they differ from each other?\n\nThe mean ensemble has slightly tighter uncertainty bounds than the median ensemble.\n\n\n\n\nNow lets plot a range of forecasts from each model and ensemble.\n\nplot_multiple_forecasts &lt;- simple_ensembles |&gt;\n  plot_ensembles(onset_df |&gt; filter(day &gt;= 21)) +\n  lims(y = c(0, 400))\n\nplot_multiple_forecasts\n\nWarning: Removed 17 rows containing missing values or values outside the scale range\n(`geom_ribbon()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_ribbon()`).\n\n\n\n\n\n\n\n\n\nAgain we can get a different perspective by plotting the forecasts on the log scale.\n\nplot_multiple_forecasts +\n  scale_y_log10()\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n\n\nWarning in scale_y_log10(): log-10 transformation introduced infinite values.\nlog-10 transformation introduced infinite values.\nlog-10 transformation introduced infinite values.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipTake 2 minutes\n\n\n\nHow do these ensembles compare to the individual models?\nHow do they differ from each other?\nAre there any differences across forecast dates?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nHow do these ensembles compare to the individual models?\n\nAs before, the ensembles appear to be less variable than the statistical models but more variable than the mechanistic model.\n\nHow do they differ from each other?\n\nThe mean ensemble has marginally tighter uncertainty bounds than the median ensemble as for the single forecast.\n\nAre there any differences across forecast dates?\n\nThe mean ensemble appears to be more variable across forecast dates than the median ensemble with this being more pronounced after the peak of the outbreak.\n\n\n\n\n\n\nEvaluation\nAs in the forecast evaluation session, we can evaluate the accuracy of the ensembles using the {scoringutils} package and in particular the score() function.\n\nensemble_scores &lt;- simple_ensembles |&gt;\n  as_forecast_quantile(forecast_unit = c(\"origin_day\", \"horizon\", \"model\")) |&gt;\n  score()\n\n\n\n\n\n\n\nNote\n\n\n\nThe weighted interval score (WIS) is a proper scoring rule for quantile forecasts that approximates the Continuous Ranked Probability Score (CRPS) by considering a weighted sum of multiple prediction intervals. As the number of intervals increases, the WIS converges to the CRPS, combining sharpness and penalties for over- and under-prediction.\nWe see it here as we are scoring quantiles and not samples hence we cannot use CRPS as we did before.\n\n\nAgain we start with a high level overview of the scores by model.\n\nensemble_scores |&gt;\n  summarise_scores(by = c(\"model\"))\n\n              model       wis overprediction underprediction dispersion\n             &lt;char&gt;     &lt;num&gt;          &lt;num&gt;           &lt;num&gt;      &lt;num&gt;\n1:    Mean ensemble  8.368094       4.053482       1.1252976   3.189314\n2:  Median ensemble 10.408312       5.707768       1.2781250   3.422420\n3:      Random walk 11.942152       7.041786       0.8459821   4.054384\n4: More statistical 11.036451       5.458304       1.7772321   3.800915\n5: More mechanistic  5.647196       1.664643       2.2699107   1.712643\n          bias interval_coverage_50 interval_coverage_90 ae_median\n         &lt;num&gt;                &lt;num&gt;                &lt;num&gt;     &lt;num&gt;\n1:  0.18348214            0.5089286            0.8750000  12.80208\n2:  0.13883929            0.5178571            0.8660714  15.92857\n3:  0.20133929            0.5133929            0.8616071  18.25893\n4: -0.02857143            0.5000000            0.8526786  16.83482\n5:  0.21741071            0.4598214            0.7633929   8.75000\n\n\n\n\n\n\n\n\nTipTake 5 minutes\n\n\n\nWhat do you think the scores are telling you? Which model do you think is best? What other scoring breakdowns might you want to look at?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWhat do you think the scores are telling you? Which model do you think is best?\n\nThe mean ensemble appears to be the best performing ensemble model overall.\nHowever, the more mechanistic model appears to be the best performing model overall.\n\nWhat other scoring breakdowns might you want to look at?\n\nThere might be variation over forecast dates or horizons between the different ensemble methods\n\n\n\n\n\n\n\nUnweighted ensembles of filtered models\nA simple method that is often used to improve ensemble performance is to prune out models that perform very poorly. Balancing this can be tricky however as it can be hard to know how much to prune. The key tradeoff to consider is how much to optimise for which models have performed well in the past (and what your definition of the past is, for example all time or only the last few weeks) versus how much you want to allow for the possibility that these models may not perform well in the future.\n\nConstruction\nAs we just saw, the random walk model (our original baseline model) is performing poorly in comparison to the other models. We can remove this model from the ensemble and see if this improves the performance of the ensemble.\n\n\n\n\n\n\nWarningWarning\n\n\n\nHere we are technically cheating a little as we are using the test data to help select the models to include in the ensemble. In the real world you would not do this as you would not have access to the test data and so this is an idealised scenario.\n\n\n\nfiltered_models &lt;- quantile_forecasts |&gt;\n  filter(model != \"Random walk\")\n\nWe then need to recalculate the ensembles. First the mean ensemble,\n\nfiltered_mean_ensembles &lt;- filtered_models |&gt;\n  as_tibble() |&gt;\n  summarise(\n    predicted = mean(predicted),\n    observed = unique(observed),\n    model = \"Mean filtered ensemble\",\n    .by = c(origin_day, horizon, quantile_level, day)\n  )\n\nand then the median ensemble.\n\nfiltered_median_ensembles &lt;- filtered_models |&gt;\n  as_tibble() |&gt;\n  summarise(\n    predicted = median(predicted),\n    observed = unique(observed),\n    model = \"Median filtered ensemble\",\n    .by = c(origin_day, horizon, quantile_level, day)\n  )\n\nWe combine these new ensembles with our previous ensembles in order to make visualisation easier.\n\nfiltered_ensembles &lt;- bind_rows(\n  filtered_mean_ensembles,\n  filtered_median_ensembles,\n  simple_ensembles\n)\n\n\n\nVisualisation\nAs for the simple ensembles, we can plot a single forecast from each model and ensemble.\n\nfiltered_ensembles |&gt;\n  filter(origin_day == 57) |&gt;\n  plot_ensembles(onset_df |&gt; filter(day &gt;= 57, day &lt;= 57 + 14))\n\n\n\n\n\n\n\n\nand on the log scale.\n\nfiltered_ensembles |&gt;\n  filter(origin_day == 57) |&gt;\n  plot_ensembles(onset_df |&gt; filter(day &gt;= 57, day &lt;= 57 + 14)) +\n  scale_y_log10()\n\n\n\n\n\n\n\n\nTo get an overview we also plot a range of forecasts from each model and ensemble.\n\nplot_multiple_filtered_forecasts &lt;- filtered_ensembles |&gt;\n  plot_ensembles(onset_df |&gt; filter(day &gt;= 21)) +\n  lims(y = c(0, 400))\nplot_multiple_filtered_forecasts\n\nWarning: Removed 17 rows containing missing values or values outside the scale range\n(`geom_ribbon()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_ribbon()`).\n\n\n\n\n\n\n\n\n\nand on the log scale.\n\nplot_multiple_filtered_forecasts +\n  scale_y_log10()\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n\n\nWarning in scale_y_log10(): log-10 transformation introduced infinite values.\nlog-10 transformation introduced infinite values.\nlog-10 transformation introduced infinite values.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipTake 2 minutes\n\n\n\nHow do the filtered ensembles compare to the simple ensembles? Which do you think is best?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nHow do the filtered ensembles compare to the simple ensembles?\n\nThe filtered ensembles appear to be less variable than the simple ensembles.\nThe filtered ensembles appear to be more like the mechanistic model than the simple ensembles.\n\nWhich do you think is best?\n\nVisually, the filtered ensembles appear very similar. This makes sense given we know there are only two models left in the ensemble.\n\n\n\n\n\n\nEvaluation\nLet us score the filtered ensembles.\n\nfiltered_ensemble_scores &lt;- filtered_ensembles |&gt;\n  as_forecast_quantile(\n    forecast_unit = c(\n      \"origin_day\", \"horizon\", \"model\"\n    )\n  ) |&gt;\n  score()\n\nAgain we can get a high level overview of the scores by model.\n\nfiltered_ensemble_scores |&gt;\n  summarise_scores(by = c(\"model\"))\n\n                      model       wis overprediction underprediction dispersion\n                     &lt;char&gt;     &lt;num&gt;          &lt;num&gt;           &lt;num&gt;      &lt;num&gt;\n1:   Mean filtered ensemble  7.001690       2.785045       1.4598661   2.756779\n2: Median filtered ensemble  7.001690       2.785045       1.4598661   2.756779\n3:            Mean ensemble  8.368094       4.053482       1.1252976   3.189314\n4:          Median ensemble 10.408312       5.707768       1.2781250   3.422420\n5:              Random walk 11.942152       7.041786       0.8459821   4.054384\n6:         More statistical 11.036451       5.458304       1.7772321   3.800915\n7:         More mechanistic  5.647196       1.664643       2.2699107   1.712643\n          bias interval_coverage_50 interval_coverage_90 ae_median\n         &lt;num&gt;                &lt;num&gt;                &lt;num&gt;     &lt;num&gt;\n1:  0.13482143            0.5089286            0.8705357  10.76562\n2:  0.13482143            0.5089286            0.8705357  10.76562\n3:  0.18348214            0.5089286            0.8750000  12.80208\n4:  0.13883929            0.5178571            0.8660714  15.92857\n5:  0.20133929            0.5133929            0.8616071  18.25893\n6: -0.02857143            0.5000000            0.8526786  16.83482\n7:  0.21741071            0.4598214            0.7633929   8.75000\n\n\n\n\n\n\n\n\nTipTake 2 minutes\n\n\n\nHow do the filtered ensembles compare to the simple ensembles?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nHow do the filtered ensembles compare to the simple ensembles?\n\nThe filtered ensembles appear to be more accurate than the simple ensembles.\nAs you would expect they are an average of the more mechanistic model and the more statistical model.\nAs there are only two models in the ensemble, the median and mean ensembles are identical.\nFor the first time there are features of the ensemble that outperform the more mechanistic model though it remains the best performing model overall.\n\n\n\n\n\n\n\nWeighted ensembles\nThe simple mean and median we used to average quantiles earlier treats every model as the same. We could try to improve performance by replacing this with a weighted mean (or weighted median), for example given greater weight to models that have proven to make better forecasts. Here we will explore two common weighting methods based on quantile averaging:\n\nInverse WIS weighting\nQuantile regression averaging\n\nInverse WIS weighting is a simple method that weights the forecasts by the inverse of their WIS over some period (note that identifying what this period should be in order to produce the best forecasts is not straightforward as predictive performance may vary over time if models are good at different things). The main benefit of WIS weighting over other methods is that it is simple to understand and implement. However, it does not optimise the weights directly to produce the best forecasts. It relies on the hope that giving more weight to better performing models yields a better ensemble\nQuantile regression averaging (QRA), on the other hand, optimises the weights directly in order to yield the best scores on past data.\n\nConstruction\n\nInverse WIS weighting\nIn order to perform inverse WIS weighting we first need to calculate the WIS for each model. We already have this from the previous evaluation so we can reuse this.\n\nmodel_scores &lt;- quantile_forecasts |&gt;\n  score()\ncumulative_scores &lt;- list()\n## filter for scores up to the origin day of the forecast\nfor (day_by in unique(model_scores$origin_day)) {\n  cumulative_scores[[as.character(day_by)]] &lt;- model_scores |&gt;\n    filter(origin_day &lt; day_by) |&gt;\n    summarise_scores(by = \"model\") |&gt;\n    mutate(day_by = day_by)\n}\nweights_per_model &lt;- bind_rows(cumulative_scores) |&gt;\n  select(model, day_by, wis) |&gt;\n  mutate(inv_wis = 1 / wis) |&gt;\n  mutate(\n    inv_wis_total_by_date = sum(inv_wis), .by = day_by\n  ) |&gt;\n  mutate(weight = inv_wis / inv_wis_total_by_date) |&gt; ## normalise\n  select(model, origin_day = day_by, weight)\n\nweights_per_model |&gt;\n  pivot_wider(names_from = model, values_from = weight)\n\n# A tibble: 15 × 4\n   origin_day `Random walk` `More statistical` `More mechanistic`\n        &lt;dbl&gt;         &lt;dbl&gt;              &lt;dbl&gt;              &lt;dbl&gt;\n 1         29         0.325              0.412              0.262\n 2         36         0.363              0.381              0.256\n 3         43         0.370              0.353              0.277\n 4         50         0.362              0.324              0.314\n 5         57         0.345              0.302              0.353\n 6         64         0.329              0.293              0.379\n 7         71         0.308              0.285              0.406\n 8         78         0.354              0.292              0.354\n 9         85         0.316              0.306              0.378\n10         92         0.239              0.250              0.511\n11         99         0.239              0.251              0.510\n12        106         0.237              0.254              0.509\n13        113         0.228              0.246              0.526\n14        120         0.233              0.254              0.513\n15        127         0.237              0.258              0.505\n\n\nNow lets apply the weights to the forecast models. As we can only use information that was available at the time of the forecast to perform the weighting, we use weights from two weeks prior to the forecast date to inform each ensemble.\n\ninverse_wis_ensemble &lt;- quantile_forecasts |&gt;\n  as_tibble() |&gt;\n  left_join(\n    weights_per_model |&gt;\n      mutate(origin_day = origin_day + 14),\n    by = c(\"model\", \"origin_day\")\n  ) |&gt;\n  # assign equal weights if no weights are available\n  mutate(weight = ifelse(is.na(weight), 1/3, weight)) |&gt;\n  summarise(\n    predicted = sum(predicted * weight),\n    observed = unique(observed),\n    model = \"Inverse WIS ensemble\",\n    .by = c(origin_day, horizon, quantile_level, day)\n  )\n\n\n\nQuantile regression averaging\nWe futher to perform quantile regression averaging (QRA) for each forecast date. Again we need to consider how many previous forecasts we wish to use to inform each ensemble forecast. Here we decide to use up to 3 weeks of previous forecasts to inform each QRA ensemble. We use the qrensemble package to perform this task.\n\nforecast_dates &lt;- quantile_forecasts |&gt;\n  as_tibble() |&gt;\n  pull(origin_day) |&gt;\n  unique()\n\nqra_by_forecast &lt;- function(\n  quantile_forecasts,\n  forecast_dates,\n  group = c(\"target_end_date\"), \n  ...\n) {\n  lapply(forecast_dates, \\(x) {\n    quantile_forecasts |&gt;\n      mutate(target_end_date = x) |&gt;\n      dplyr::filter(origin_day &lt;= x) |&gt;\n      dplyr::filter(origin_day &gt;= x - (3 * 7 + 1)) |&gt;\n      dplyr::filter(origin_day == x | day &lt;= x) |&gt;\n      qra(\n        group = group,\n        target = c(origin_day = x),\n        ...\n      )\n  })\n}\n\nqra_ensembles_obj &lt;- qra_by_forecast(\n  quantile_forecasts,\n  forecast_dates[-1],\n  group = c(\"target_end_date\")\n)\n\nqra_weights &lt;- seq_along(qra_ensembles_obj) |&gt;\n  lapply(\\(i) attr(qra_ensembles_obj[[i]], \"weights\") |&gt;\n    mutate(origin_day = forecast_dates[i + 1])\n  ) |&gt;\n  bind_rows() |&gt;\n  dplyr::filter(quantile == 0.5) |&gt;\n  select(-quantile)\n\nqra_ensembles &lt;- qra_ensembles_obj |&gt;\n  bind_rows()\n\nInstead of creating a single optimised ensemble and using this for all forecast horizons we might also want to consider a separate optimised QRA ensemble for each forecast horizon, reflecting that models might perform differently depending on how far ahead a forecast is produced. We can do this using qra() with the group argument.\n\nqra_ensembles_by_horizon &lt;- qra_by_forecast(\n  quantile_forecasts,\n  forecast_dates[-c(1:2)],\n  group = c(\"horizon\", \"target_end_date\"),\n  model = \"QRA by horizon\"\n)\n\nqra_weights_by_horizon &lt;- seq_along(qra_ensembles_by_horizon) |&gt;\n  lapply(\\(i) attr(qra_ensembles_by_horizon[[i]], \"weights\") |&gt;\n    mutate(origin_day = forecast_dates[i + 2])\n  ) |&gt;\n  bind_rows()",
    "crumbs": [
      "Forecast ensembles"
    ]
  },
  {
    "objectID": "sessions/forecast-ensembles.html#sample-based-weighted-ensembles",
    "href": "sessions/forecast-ensembles.html#sample-based-weighted-ensembles",
    "title": "Forecast ensembles",
    "section": "Sample-based weighted ensembles",
    "text": "Sample-based weighted ensembles\nQuantile averaging can be interpreted as a combination of different uncertain estimates of a true distribution of a given shape. Instead, we might want to interpret multiple models as multiple possible versions of this truth, with weights assigned to each of them representing the probability of each one being the true one. In that case, we want to create a (weighted) mixture distribution of the constituent models. This can be done from samples, with weights tuned to optimise the CRPS. The procedure is also called a linear opinion pool. Once again one can create unweighted, filtered unweighted and weighted ensembles. For now we will just consider weighted ensembles. We use the lopensemble package to perform this task.\n\n## lopensemble expects a \"date\" column indicating the timing of forecasts\nlop_forecasts &lt;- sample_forecasts |&gt;\n  rename(date = day)\n\nlop_by_forecast &lt;- function(\n  sample_forecasts,\n  forecast_dates,\n  group = c(\"target_end_date\"),\n  ...\n) {\n  lapply(forecast_dates, \\(x) {\n    y &lt;- sample_forecasts |&gt;\n      mutate(target_end_date = x) |&gt;\n      dplyr::filter(origin_day &lt;= x) |&gt;\n      dplyr::filter(origin_day &gt;= x - (3 * 7 + 1)) |&gt;\n      dplyr::filter(origin_day == x | date &lt;= x)\n    lop &lt;- mixture_from_samples(y) |&gt;\n      filter(date &gt; x) |&gt;\n      mutate(origin_day = x)\n    return(lop)\n  })\n}\n\nlop_ensembles_obj &lt;- lop_by_forecast(\n  lop_forecasts,\n  forecast_dates[-1]\n)\n\nlop_weights &lt;- seq_along(lop_ensembles_obj) |&gt;\n  lapply(\\(i) attr(lop_ensembles_obj[[i]], \"weights\") |&gt;\n    mutate(origin_day = forecast_dates[i + 1])\n  ) |&gt;\n  bind_rows()\n\n## combine and generate quantiles from the resulting samples\nlop_ensembles &lt;- lop_ensembles_obj |&gt;\n  bind_rows() |&gt;\n  mutate(model = \"Linear Opinion Pool\") |&gt;\n  rename(day = date) |&gt; ## rename column back for later plotting\n  as_forecast_sample() |&gt;\n  as_forecast_quantile()",
    "crumbs": [
      "Forecast ensembles"
    ]
  },
  {
    "objectID": "sessions/forecast-ensembles.html#compare-the-different-ensembles",
    "href": "sessions/forecast-ensembles.html#compare-the-different-ensembles",
    "title": "Forecast ensembles",
    "section": "Compare the different ensembles",
    "text": "Compare the different ensembles\nWe have created a number of ensembles which we can now compare.\n\nweighted_ensembles &lt;- bind_rows(\n  inverse_wis_ensemble,\n  qra_ensembles,\n  qra_ensembles_by_horizon,\n  filtered_ensembles,\n  lop_ensembles\n) |&gt;\n  # remove the repeated filtered ensemble\n  filter(model != \"Mean filtered ensemble\")\n\n\nVisualisation\n\nSingle forecasts\nAgain we start by plotting a single forecast from each model and ensemble.\n\nweighted_ensembles |&gt;\n  filter(origin_day == 57) |&gt;\n  plot_ensembles(onset_df |&gt; filter(day &gt;= 57, day &lt;= 57 + 14))\n\n\n\n\n\n\n\n\nand on the log scale.\n\nweighted_ensembles |&gt;\n  filter(origin_day == 57) |&gt;\n  plot_ensembles(onset_df |&gt; filter(day &gt;= 57, day &lt;= 57 + 14)) +\n  scale_y_log10()\n\n\n\n\n\n\n\n\n\n\n\nMultiple forecasts\nAs before we can plot a range of forecasts from each model and ensemble.\n\nplot_multiple_weighted_forecasts &lt;- weighted_ensembles |&gt;\n  plot_ensembles(onset_df |&gt; filter(day &gt;= 21)) +\n  lims(y = c(0, 400))\nplot_multiple_weighted_forecasts\n\nWarning: Removed 17 rows containing missing values or values outside the scale range\n(`geom_ribbon()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_ribbon()`).\n\n\n\n\n\n\n\n\n\nand on the log scale.\n\nplot_multiple_weighted_forecasts +\n  scale_y_log10()\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n\n\nWarning in scale_y_log10(): log-10 transformation introduced infinite values.\nlog-10 transformation introduced infinite values.\nlog-10 transformation introduced infinite values.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipTake 2 minutes\n\n\n\nHow do the weighted ensembles compare to the simple ensembles? Which do you think is best? Are you surprised by the results? Can you think of any reasons that would explain them?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nHow do the weighted ensembles compare to the simple ensembles?\n\n\n\n\n\nModel weights\nWe can also compare the weights that the different weighted ensembles we have created assign to each model.\n\nweights &lt;- rbind(\n  weights_per_model |&gt; mutate(method = \"Inverse WIS\"),\n  qra_weights |&gt; mutate(method = \"QRA\"),\n  lop_weights |&gt; mutate(method = \"LOP\")\n)\nweights |&gt;\n  ggplot(aes(x = origin_day, y = weight, fill = model)) +\n  geom_col(position = \"stack\") +\n  facet_grid(~ method) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipTake 2 minutes\n\n\n\nAre the weights assigned to models different between the three methods? How do the weights change over time? Are you surprised by the results given what you know about the models performance?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nAre the weights assigned to models different between the three methods?\n\nThere are major differences especially early on, where the LOP ensemble prefers the random walk model and QRA the more statistical model.\nThe inverse WIS model transitions from fairly even weights early on to giving most weights to the mechanistic model, however it does so in a more balanced manner than the optimised ensemble, giving substantial weight to all three models.\n\nHow do the weights change over time?\n\nEarly on the more statistical models have higher weights in the respective ensemble methods.\nGradually the mechanistic model gains weight in both models and by the end of the forecast horizon it represents the entire ensemble.\n\nAre you surprised by the results given what you know about the models performance?\n\nAs the random walk model is performing poorly, you would expect it to have low weights but actually it often doesn’t. This implies that its poor performance is restricted to certain parts of the outbreak.\nThe mechanistic model performs really well overall and dominates the final optimised ensemble.\n\n\n\n\n\n\nEvaluation\nFor a final evaluation we can look at the scores for each model and ensemble again. We remove the two weeks of forecasts as these do not have a quantile regression average forecasts as these require training data to estimate.\n\nweighted_ensemble_scores &lt;- weighted_ensembles |&gt;\n  filter(origin_day &gt;= 29) |&gt;\n  as_forecast_quantile(forecast_unit = c(\"origin_day\", \"horizon\", \"model\")) |&gt;\n  score()\n\nAgain we can get a high level overview of the scores by model.\n\nweighted_ensemble_scores |&gt;\n  summarise_scores(by = c(\"model\"))\n\n                          model       wis overprediction underprediction\n                         &lt;char&gt;     &lt;num&gt;          &lt;num&gt;           &lt;num&gt;\n 1:        Inverse WIS ensemble  8.179605       3.739972       1.1895563\n 2: Quantile Regression Average  7.438817       2.930858       2.1179392\n 3:              QRA by horizon  8.006609       3.222753       2.2214614\n 4:    Median filtered ensemble  7.255293       2.848571       1.5567143\n 5:               Mean ensemble  8.709146       4.197524       1.1996825\n 6:             Median ensemble 10.878952       5.954000       1.3623810\n 7:                 Random walk 12.514186       7.376952       0.9014286\n 8:            More statistical 11.595419       5.756476       1.8947619\n 9:            More mechanistic  5.745548       1.568476       2.4212381\n10:         Linear Opinion Pool  6.370671       1.275143       2.2822857\n    dispersion        bias interval_coverage_50 interval_coverage_90 ae_median\n         &lt;num&gt;       &lt;num&gt;                &lt;num&gt;                &lt;num&gt;     &lt;num&gt;\n 1:   3.250076  0.18476190            0.5190476            0.8619048 12.544135\n 2:   2.390020  0.07571429            0.4809524            0.7857143 11.436517\n 3:   2.562395  0.12193878            0.4642857            0.7908163 12.215520\n 4:   2.850007  0.09380952            0.5285714            0.8666667 11.121429\n 5:   3.311940  0.14571429            0.5285714            0.8714286 13.284127\n 6:   3.562571  0.10047619            0.5380952            0.8619048 16.600000\n 7:   4.235805  0.16714286            0.5333333            0.8571429 19.085714\n 8:   3.944181 -0.06809524            0.4952381            0.8428571 17.690476\n 9:   1.755833  0.17571429            0.4809524            0.7714286  8.876190\n10:   2.813243  0.03619048            0.5666667            0.8666667  9.390476\n\n\nRemembering the session on forecast evaluation, we should also check performance on the log scale.\n\nlog_ensemble_scores &lt;- weighted_ensembles |&gt;\n  filter(origin_day &gt;= 29) |&gt;\n  as_forecast_quantile(forecast_unit = c(\"origin_day\", \"horizon\", \"model\")) |&gt;\n    transform_forecasts(\n    fun = log_shift,\n    offset = 1,\n    append = FALSE\n  ) |&gt;\n  score()\n\nlog_ensemble_scores |&gt;\n  summarise_scores(by = c(\"model\"))\n\n                          model       wis overprediction underprediction\n                         &lt;char&gt;     &lt;num&gt;          &lt;num&gt;           &lt;num&gt;\n 1:        Inverse WIS ensemble 0.1660409     0.07773211      0.02514637\n 2: Quantile Regression Average 0.1807427     0.08359583      0.03726745\n 3:              QRA by horizon 0.1673600     0.08902760      0.02703905\n 4:    Median filtered ensemble 0.1565710     0.06928781      0.02878697\n 5:               Mean ensemble 0.1702717     0.07649681      0.02965817\n 6:             Median ensemble 0.1930613     0.07919152      0.04153467\n 7:                 Random walk 0.2059349     0.09204866      0.03774232\n 8:            More statistical 0.2265978     0.06678145      0.07615982\n 9:            More mechanistic 0.1612289     0.09838393      0.02188482\n10:         Linear Opinion Pool 0.1627772     0.06745170      0.02915914\n    dispersion        bias interval_coverage_50 interval_coverage_90 ae_median\n         &lt;num&gt;       &lt;num&gt;                &lt;num&gt;                &lt;num&gt;     &lt;num&gt;\n 1: 0.06316243  0.18476190            0.5190476            0.8619048 0.2535244\n 2: 0.05987939  0.07809524            0.4809524            0.7857143 0.2719032\n 3: 0.05129337  0.12448980            0.4744898            0.7908163 0.2469915\n 4: 0.05849623  0.09380952            0.5285714            0.8666667 0.2397427\n 5: 0.06411670  0.14571429            0.5285714            0.8714286 0.2602670\n 6: 0.07233514  0.10047619            0.5380952            0.8619048 0.2978788\n 7: 0.07614388  0.16714286            0.5333333            0.8571429 0.3172793\n 8: 0.08365652 -0.06809524            0.4952381            0.8428571 0.3422939\n 9: 0.04096012  0.17571429            0.4809524            0.7714286 0.2445859\n10: 0.06616635  0.03619048            0.5666667            0.8666667 0.2415565\n\n\n\n\n\n\n\n\nTipTake 2 minutes\n\n\n\nHow do the weighted ensembles compare to the simple ensembles on the natural and log scale?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nThe best ensembles slightly outperform some of the simple ensembles but there is no obvious benefit from using weighted ensembles. Why might this be the case?",
    "crumbs": [
      "Forecast ensembles"
    ]
  },
  {
    "objectID": "sessions/forecast-ensembles.html#challenge",
    "href": "sessions/forecast-ensembles.html#challenge",
    "title": "Forecast ensembles",
    "section": "Challenge",
    "text": "Challenge\n\nThroughout the course we’ve been carefully building models from our understanding of the underlying data generating process. What might be the implications of combining models from different modellers, with different assumptions about this process? Think about the reliability and validity of the resulting ensemble forecast.\nWould it change how you approach communicating the forecast?",
    "crumbs": [
      "Forecast ensembles"
    ]
  },
  {
    "objectID": "sessions/forecast-ensembles.html#methods-in-the-real-world",
    "href": "sessions/forecast-ensembles.html#methods-in-the-real-world",
    "title": "Forecast ensembles",
    "section": "Methods in the real world",
    "text": "Methods in the real world\n\nHowerton et al. (2023) suggests that the choice of an ensemble method should be informed by an assumption about how to represent uncertainty between models: whether differences between component models is “noisy” variation around a single underlying distribution, or represents structural uncertainty about the system.\nSherratt et al. (2023) investigates the performance of different ensembles in the European COVID-19 Forecast Hub.\nAmaral et al. (2025) discusses the challenges in improving on the predictive performance of simpler approaches using weighted ensembles.",
    "crumbs": [
      "Forecast ensembles"
    ]
  },
  {
    "objectID": "sessions/forecast-ensembles.html#references",
    "href": "sessions/forecast-ensembles.html#references",
    "title": "Forecast ensembles",
    "section": "References",
    "text": "References\n\n\nAmaral, André Victor Ribeiro, Daniel Wolffram, Paula Moraga, and Johannes Bracher. 2025. “Post-Processing and Weighted Combination of Infectious Disease Nowcasts.” PLoS Computational Biology 21 (3): e1012836. https://doi.org/10.1371/journal.pcbi.1012836.\n\n\nHowerton, Emily, Michael C. Runge, Tiffany L. Bogich, Rebecca K. Borchering, Hidetoshi Inamine, Justin Lessler, Luke C. Mullany, et al. 2023. “Context-Dependent Representation of Within- and Between-Model Uncertainty: Aggregating Probabilistic Predictions in Infectious Disease Epidemiology.” Journal of The Royal Society Interface 20 (198): 20220659. https://doi.org/10.1098/rsif.2022.0659.\n\n\nSherratt, Katharine, Hugo Gruson, Rok Grah, Helen Johnson, Rene Niehus, Bastian Prasse, Frank Sandmann, et al. 2023. “Predictive Performance of Multi-Model Ensemble Forecasts of COVID-19 Across European Nations.” Edited by Amy Wesolowski, Neil M Ferguson, Jeffrey L Shaman, and Sen Pei. eLife 12 (April): e81916. https://doi.org/10.7554/eLife.81916.",
    "crumbs": [
      "Forecast ensembles"
    ]
  },
  {
    "objectID": "sessions/end-of-course-summary-and-discussion.html",
    "href": "sessions/end-of-course-summary-and-discussion.html",
    "title": "End of course summary and discussion",
    "section": "",
    "text": "We hope that you’ve enjoyed taking this course on nowcasting and forecasting infectious disease dynamics. We will be very happy to have your feedback, comments or any questions on the course discussion page.\nIf you are exploring the course asynchronously, we’d be grateful to hear your experience. Please use this survey to let us know how you found the course.\n\n\nEnd of course summary\n\n\n\nWe recommend exploring these topics in Further reading. You can also access these in a living library, which you are welcome to contribute to.\nAll the course material is openly available for you to come back to at any time. If you return to the course after some time, you may want to update your setup to ensure you have the latest content and package versions.\nYou are free to share and reuse this content in any way. Please consider including a citation (and/or let us know!) if you find it useful.",
    "crumbs": [
      "End of course summary and discussion"
    ]
  },
  {
    "objectID": "sessions/end-of-course-summary-and-discussion.html#slides",
    "href": "sessions/end-of-course-summary-and-discussion.html#slides",
    "title": "End of course summary and discussion",
    "section": "",
    "text": "End of course summary",
    "crumbs": [
      "End of course summary and discussion"
    ]
  },
  {
    "objectID": "sessions/end-of-course-summary-and-discussion.html#going-further",
    "href": "sessions/end-of-course-summary-and-discussion.html#going-further",
    "title": "End of course summary and discussion",
    "section": "",
    "text": "We recommend exploring these topics in Further reading. You can also access these in a living library, which you are welcome to contribute to.\nAll the course material is openly available for you to come back to at any time. If you return to the course after some time, you may want to update your setup to ensure you have the latest content and package versions.\nYou are free to share and reuse this content in any way. Please consider including a citation (and/or let us know!) if you find it useful.",
    "crumbs": [
      "End of course summary and discussion"
    ]
  },
  {
    "objectID": "sessions/biases-in-delay-distributions.html",
    "href": "sessions/biases-in-delay-distributions.html",
    "title": "Biases in delay distributions",
    "section": "",
    "text": "So far, we’ve looked at the uncertainty of the time delays between epidemiological events. The next challenge is that our information on these delays is usually biased, especially when we’re analysing data in real time. We’ll consider two types of biases that commonly occur in reported infectious disease data:\n\nCensoring: when we know an event occurred at some time, but not exactly when.\nTruncation: when not enough time has passed for all the relevant epidemiological events to occur or be observed.\n\nWe can again handle these by including them as uncertain parameters in the modelling process.\n\n\nIntroduction to biases in epidemiological delays\n\n\n\nIn this session, we’ll introduce censoring and right truncation as typical properties of the process that generates infectious disease data sets, using the delay from symptom onset to hospitalisation as an example.\n\n\n\n\n\n\nNoteSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/biases-in-delay-distributions.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the ggplot2 package for plotting, the dplyr and tidyr packages to wrangle data, the lubridate package to deal with dates, the purrr package for functional programming, and the tidybayes package for extracting results from model fits.\n\nlibrary(\"nfidd\")\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"purrr\")\nlibrary(\"lubridate\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Biases in delay distributions"
    ]
  },
  {
    "objectID": "sessions/biases-in-delay-distributions.html#slides",
    "href": "sessions/biases-in-delay-distributions.html#slides",
    "title": "Biases in delay distributions",
    "section": "",
    "text": "Introduction to biases in epidemiological delays",
    "crumbs": [
      "Biases in delay distributions"
    ]
  },
  {
    "objectID": "sessions/biases-in-delay-distributions.html#objectives",
    "href": "sessions/biases-in-delay-distributions.html#objectives",
    "title": "Biases in delay distributions",
    "section": "",
    "text": "In this session, we’ll introduce censoring and right truncation as typical properties of the process that generates infectious disease data sets, using the delay from symptom onset to hospitalisation as an example.\n\n\n\n\n\n\nNoteSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/biases-in-delay-distributions.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the ggplot2 package for plotting, the dplyr and tidyr packages to wrangle data, the lubridate package to deal with dates, the purrr package for functional programming, and the tidybayes package for extracting results from model fits.\n\nlibrary(\"nfidd\")\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"purrr\")\nlibrary(\"lubridate\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Biases in delay distributions"
    ]
  },
  {
    "objectID": "sessions/biases-in-delay-distributions.html#source-file",
    "href": "sessions/biases-in-delay-distributions.html#source-file",
    "title": "Biases in delay distributions",
    "section": "",
    "text": "The source file of this session is located at sessions/biases-in-delay-distributions.qmd.",
    "crumbs": [
      "Biases in delay distributions"
    ]
  },
  {
    "objectID": "sessions/biases-in-delay-distributions.html#libraries-used",
    "href": "sessions/biases-in-delay-distributions.html#libraries-used",
    "title": "Biases in delay distributions",
    "section": "",
    "text": "In this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the ggplot2 package for plotting, the dplyr and tidyr packages to wrangle data, the lubridate package to deal with dates, the purrr package for functional programming, and the tidybayes package for extracting results from model fits.\n\nlibrary(\"nfidd\")\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"purrr\")\nlibrary(\"lubridate\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.",
    "crumbs": [
      "Biases in delay distributions"
    ]
  },
  {
    "objectID": "sessions/biases-in-delay-distributions.html#initialisation",
    "href": "sessions/biases-in-delay-distributions.html#initialisation",
    "title": "Biases in delay distributions",
    "section": "",
    "text": "We set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Biases in delay distributions"
    ]
  },
  {
    "objectID": "sessions/biases-in-delay-distributions.html#understanding-double-interval-censoring",
    "href": "sessions/biases-in-delay-distributions.html#understanding-double-interval-censoring",
    "title": "Biases in delay distributions",
    "section": "Understanding double interval censoring",
    "text": "Understanding double interval censoring\n\nPrimary vs Secondary Event Censoring\nIt’s important to understand that double interval censoring can be decomposed into two distinct components:\nPrimary event censoring: Uncertainty about when the first event (symptom onset) occurred within its reported day. Even if we know someone developed symptoms “on 20 June”, we don’t know if this was at 00:01 or 23:59.\nSecondary event censoring: Uncertainty about when the second event (hospitalisation) occurred within its reported day. Similarly, “admitted on 22 June” could mean any time during those 24 hours.\n\n\nWhy this distinction matters\nA common mistake in epidemiological delay estimation is to only account for secondary event censoring (uncertainty in the outcome event) while ignoring primary event censoring (uncertainty in the initiating event).\nThis approach is often incorrect for epidemiological data because:\n\nPrimary events (infections, symptom onsets) are rarely observed at exact times\nIgnoring primary event censoring leads to systematic bias in delay estimates\nThe magnitude of bias depends on the epidemic growth rate and censoring intervals\n\nFor accurate delay estimation, we need to account for both components as we do in our “double interval censored” approach.\n\n\n\n\n\n\nNoteWhy are we so worried?\n\n\n\n\n\nWe’ll be spending a lot of time thinking about a potentially “short” 24 hour time interval. In this session, we’ll start to get a sense for how variation, uncertainty, and bias within this interval might combine to impact daily outbreak estimates. The concepts and techniques that we use here also apply to longer time intervals that we might find in surveillance data, for example, reported case counts aggregated by week.",
    "crumbs": [
      "Biases in delay distributions"
    ]
  },
  {
    "objectID": "sessions/biases-in-delay-distributions.html#implementing-bias-correction-for-double-interval-censoring",
    "href": "sessions/biases-in-delay-distributions.html#implementing-bias-correction-for-double-interval-censoring",
    "title": "Biases in delay distributions",
    "section": "Implementing bias correction for double interval censoring",
    "text": "Implementing bias correction for double interval censoring\nLet’s estimate the time from symptom onset to hospitalisation with the censored data.\nA naïve approach to estimating the delay would be to ignore the fact that the data are censored. To estimate the delay from onset to hospitalisation, we could just use the difference between the censored times, which is an integer (the number of days).\n\ndf_dates &lt;- df_dates |&gt;\n  mutate(\n    incubation_period = onset_time - infection_time,\n    onset_to_hosp = hosp_time - onset_time\n  )\n\nTo help us think about this, let’s summarise the original time-based incubation period\n\nsummary(df$onset_time - df$infection_time)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.3569  3.3499  4.6329  4.9559  6.2027 18.5544 \n\n\nand then the date based incubation period as we observe it.\n\nsummary(df_dates$incubation_period)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    3.00    5.00    4.96    6.00   19.00 \n\n\nYou should see that they have nearly the same means but different medians (by about half a day).\n\n\n\n\n\n\nTipTake 5 minutes\n\n\n\nFit the lognormal model used in the session on delay distributions to the estimates from the rounded data, i.e. using the df_dates data set. Do you still recover the parameters that we put in?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nmod &lt;- nfidd_cmdstan_model(\"lognormal\")\nres &lt;- nfidd_sample(mod,\n  data = list(\n    n = nrow(na.omit(df_dates)),\n    y = na.omit(df_dates)$onset_to_hosp + 0.01\n  )\n)\n\n\n\n\n\n\n\nTip\n\n\n\nNote the + 0.01 in the data argument. We have had to add this to avoid delays of 0 days. This should be a hint that we are not modelling the data correctly.\n\n\n\nres\n\n variable     mean   median   sd  mad       q5      q95 rhat ess_bulk ess_tail\n  lp__    -1633.72 -1633.41 1.00 0.70 -1635.78 -1632.76 1.00     1008     1347\n  meanlog     0.96     0.96 0.01 0.01     0.93     0.98 1.00     1848     1437\n  sdlog       0.60     0.60 0.01 0.01     0.58     0.62 1.00     1811     1220\n\n\nWe can calculate the mean and standard deviation to see the bias more clearly:\n\nres |&gt;\n  summarise_lognormal()\n\n      mean             sd       \n Min.   :2.969   Min.   :1.832  \n 1st Qu.:3.087   1st Qu.:2.017  \n Median :3.120   Median :2.058  \n Mean   :3.120   Mean   :2.059  \n 3rd Qu.:3.152   3rd Qu.:2.101  \n Max.   :3.270   Max.   :2.305  \n\n\nUsually the estimates will be further from the “true” parameters than before when we worked with the unrounded data.\n\n\n\nThere are many ad-hoc solutions to this problem that, for example, introduce a shift in the data by half a day to centre it on mid-day or discretise the distribution and use the difference between two cumulative density functions with a day, or two day interval. Of these all but the two-day interval approach introduce more bias than doing nothing as above. See the How epidemic dynamics affect bias severity callout for a visual comparison of these approaches and Park et al. for a detailed discussion of approximate approaches (Park et al. 2024).\nTo properly account for double interval censoring, we need to modify the model to include the fact that we don’t know when exactly on any given day the event happened. For example, if we know that symptom onset of an individual occurred on 20 June, 2024, and they were admitted to hospital on 22 June, 2024, this could mean an onset-to-hospitalisation delay from 1 day (onset at 23:59 on the 20th, admitted at 0:01 on the 22nd) to 3 days (onset at 0:01 on the 20th, admitted at 23:59 on the 22nd).\n\n\n\n\n\n\nNoteVisualising two types of censoring\n\n\n\n\n\nLet’s visualise the difference between double and secondary censoring using a simple example (you can change the parameters to see how the bias changes).\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Set parameters for our example\nn &lt;- 1e4\nmeanlog &lt;- 1.0  # Mean ~3 days - try changing this\nsdlog &lt;- 0.5\nobs_time &lt;- 15\n\n# Generate true delay distribution\ntrue_delays &lt;- rlnorm(n, meanlog = meanlog, sdlog = sdlog)\n\n# Primary censoring only - add uniform uncertainty to primary event timing\nprimary_censored &lt;- true_delays + runif(n, 0, 1)\n\n# Secondary censoring only (common mistake) - just discretise\nsecondary_only &lt;- floor(true_delays)\n\n# Double censoring - discretise the primary censored delays\ndouble_censored &lt;- floor(primary_censored)\n\n# Filter to a reasonable range and create PMF for discrete data\nkeep_range &lt;- function(x) x[x &lt;= obs_time]\nprimary_filtered &lt;- keep_range(primary_censored)\nsecondary_filtered &lt;- keep_range(secondary_only)\ndouble_filtered &lt;- keep_range(double_censored)\n\n# Create PMF for discrete data\nsecondary_pmf &lt;- table(secondary_filtered) / length(secondary_filtered)\ndouble_pmf &lt;- table(double_filtered) / length(double_filtered)\n\n# Create the comparison plot\nggplot() +\n  # True distribution (black line)\n  geom_function(\n    fun = dlnorm,\n    args = list(meanlog = meanlog, sdlog = sdlog),\n    color = \"#252525\",\n    linewidth = 1.2\n  ) +\n  # Primary censoring (continuous, blue density)\n  geom_density(\n    data = data.frame(x = primary_filtered),\n    aes(x = x),\n    fill = \"#4292C6\",\n    col = \"#252525\",\n    alpha = 0.6\n  ) +\n  # Secondary censoring only (discrete, coral bars)\n  geom_col(\n    data = data.frame(\n      x = as.numeric(names(secondary_pmf)),\n      y = as.numeric(secondary_pmf)\n    ),\n    aes(x = x, y = y),\n    fill = \"#E31A1C\",\n    col = \"#252525\",\n    alpha = 0.6,\n    width = 0.9\n  ) +\n  # Double censoring (discrete, green bars)\n  geom_col(\n    data = data.frame(\n      x = as.numeric(names(double_pmf)),\n      y = as.numeric(double_pmf)\n    ),\n    aes(x = x, y = y),\n    fill = \"#20b986\",\n    col = \"#252525\",\n    alpha = 0.4,\n    width = 0.9\n  ) +\n  labs(\n    title = \"Comparison of Different Censoring Approaches\",\n    x = \"Delay (days)\",\n    y = \"Density / Probability Mass\",\n    caption = paste0(\n      \"Black line: True log-normal distribution\\n\",\n      \"Blue density: Primary censoring (continuous)\\n\",\n      \"Red bars: Secondary censoring only (common mistake)\\n\",\n      \"Green bars: Double censoring (both components)\"\n    )\n  ) +\n  scale_x_continuous(limits = c(0, 15)) +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)\n  )\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_col()`).\nRemoved 2 rows containing missing values or values outside the scale range\n(`geom_col()`).\n\n\n\n\n\n\n\n\n\nThis figure demonstrates why accounting for only secondary censoring (red bars) is problematic - it alters the mean versus the true censoring process and so induces bias when double interval censored data is treated as single interval censored data.\n\n\n\nWe can use the interval information for both events in our delay estimation by making the exact time of the events based on the dates given part of the estimation procedure:\n\ncmod &lt;- nfidd_cmdstan_model(\"censored-delay-model\")\ncmod\n\n 1: data {\n 2:   int&lt;lower = 0&gt; n;\n 3:   array[n] int&lt;lower = 0&gt; onset_to_hosp;\n 4: }\n 5: \n 6: parameters {\n 7:   real meanlog;\n 8:   real&lt;lower = 0&gt; sdlog;\n 9:   array[n] real&lt;lower = 0, upper = 1&gt; onset_day_time;\n10:   array[n] real&lt;lower = 0, upper = 1&gt; hosp_day_time;\n11: }\n12: \n13: transformed parameters {\n14:   array[n] real&lt;lower = 0&gt; true_onset_to_hosp;\n15:   for (i in 1:n) {\n16:     true_onset_to_hosp[i] =\n17:       onset_to_hosp[i] + hosp_day_time[i] - onset_day_time[i];\n18:   }\n19: }\n20: \n21: model {\n22:   meanlog ~ normal(0, 10);\n23:   sdlog ~ normal(0, 10) T[0, ];\n24:   onset_day_time ~ uniform(0, 1);\n25:   hosp_day_time ~ uniform(0, 1);\n26: \n27:   true_onset_to_hosp ~ lognormal(meanlog, sdlog);\n28: }\n\n\n\n\n\n\n\n\nTipTake 5 minutes\n\n\n\nFamiliarise yourself with the model above. Do you understand all the lines? Which line(s) define the parameter prior distribution(s), which one(s) the likelihood, and which one(s) reflect that we have now provided the delay as the difference in integer days?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nLines 21-24 define the parametric prior distributions (for parameters meanlog and sdlog, and the estimates of exact times of events). The key idea here is that the true event can happen at any time within the observed interval for both events. Line 27 defines the likelihood. Lines 15-17 reflect the integer delays, adjusted by the estimated times of day.\n\n\n\nNow we can use this model to re-estimate the parameters of the delay distribution:\n\ncres &lt;- nfidd_sample(cmod,\n  data = list(\n    n = nrow(na.omit(df_dates)),\n    onset_to_hosp = na.omit(df_dates)$onset_to_hosp\n  )\n)\n\n\ncres\n\n          variable      mean    median    sd   mad        q5       q95 rhat\n lp__              -10218.47 -10217.65 51.14 51.93 -10307.52 -10132.66 1.02\n meanlog                0.98      0.98  0.01  0.01      0.96      1.00 1.00\n sdlog                  0.49      0.49  0.01  0.01      0.47      0.51 1.00\n onset_day_time[1]      0.34      0.28  0.24  0.25      0.03      0.80 1.00\n onset_day_time[2]      0.56      0.58  0.28  0.35      0.07      0.96 1.01\n onset_day_time[3]      0.57      0.61  0.29  0.34      0.07      0.96 1.01\n onset_day_time[4]      0.48      0.48  0.29  0.36      0.05      0.94 1.01\n onset_day_time[5]      0.34      0.28  0.25  0.27      0.02      0.82 1.01\n onset_day_time[6]      0.53      0.55  0.29  0.37      0.07      0.96 1.00\n onset_day_time[7]      0.33      0.28  0.24  0.26      0.02      0.80 1.00\n ess_bulk ess_tail\n      449      875\n     4645     1458\n     3442     1696\n     3332     1406\n     4040     1185\n     5004     1180\n     4174     1218\n     3996     1049\n     4118     1397\n     3122      917\n\n # showing 10 of 5394 rows (change via 'max_rows' argument or 'cmdstanr_max_rows' option)\n\n\nWe can also examine the mean and standard deviation of the estimated delay distribution:\n\ncres |&gt;\n  summarise_lognormal()\n\n      mean             sd       \n Min.   :2.876   Min.   :1.426  \n 1st Qu.:2.981   1st Qu.:1.538  \n Median :3.005   Median :1.567  \n Mean   :3.005   Mean   :1.567  \n 3rd Qu.:3.029   3rd Qu.:1.596  \n Max.   :3.137   Max.   :1.767  \n\n\n\n\n\n\n\n\nTipTake 10 minutes\n\n\n\nTry resimulating the delays using different parameters of the delay distribution. Can you establish under which conditions the bias in recovered parameters gets worse? Is it the same as when we just simulated the data? If not, why not?",
    "crumbs": [
      "Biases in delay distributions"
    ]
  },
  {
    "objectID": "sessions/biases-in-delay-distributions.html#implementing-bias-correction-for-truncation",
    "href": "sessions/biases-in-delay-distributions.html#implementing-bias-correction-for-truncation",
    "title": "Biases in delay distributions",
    "section": "Implementing bias correction for truncation",
    "text": "Implementing bias correction for truncation\nIf we take the naïve mean of delays we get an underestimate as expected:\n\n# truncated mean delay\nmean(df_realtime$onset_to_hosp)\n\n[1] 5.952562\n\n# compare with the mean delay over the full outbreak\nmean(df$hosp_time - df$onset_time, na.rm=TRUE)\n\n[1] 6.382549\n\n\n\n\n\n\n\n\nTipTake 5 minutes\n\n\n\nFit the lognormal model used above to the estimates from the truncated data, i.e. using the df_realtime data set. How far away from the “true” parameters do you end up?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nres &lt;- nfidd_sample(mod,\n  data = list(\n    n = nrow(na.omit(df_realtime)),\n    y = na.omit(df_realtime)$onset_to_hosp\n  )\n)\n\n\nres\n\n variable    mean  median   sd  mad      q5     q95 rhat ess_bulk ess_tail\n  lp__    -141.69 -141.40 0.97 0.68 -143.63 -140.78 1.00      933     1230\n  meanlog    1.67    1.67 0.03 0.03    1.62    1.72 1.00     1872     1242\n  sdlog      0.47    0.47 0.02 0.02    0.43    0.51 1.00     1639     1203\n\n\nThe mean and standard deviation show the underestimation due to truncation:\n\nres |&gt;\n  summarise_lognormal()\n\n      mean             sd       \n Min.   :5.312   Min.   :2.334  \n 1st Qu.:5.824   1st Qu.:2.818  \n Median :5.950   Median :2.948  \n Mean   :5.952   Mean   :2.960  \n 3rd Qu.:6.083   3rd Qu.:3.098  \n Max.   :6.708   Max.   :3.852  \n\n\n\n\n\nOnce again, we can write a model that adjusts for truncation by re-creating the simulated truncation effect in the stan model:\n\ntmod &lt;- nfidd_cmdstan_model(\"truncated-delay-model\")\ntmod\n\n 1: data {\n 2:   int&lt;lower = 0&gt; n;\n 3:   array[n] real&lt;lower = 0&gt; onset_to_hosp;\n 4:   array[n] real&lt;lower = 0&gt; time_since_onset;\n 5: }\n 6: \n 7: parameters {\n 8:   real meanlog;\n 9:   real&lt;lower = 0&gt; sdlog;\n10: }\n11: \n12: model {\n13:   meanlog ~ normal(0, 10);\n14:   sdlog ~ normal(0, 10) T[0, ];\n15: \n16:   for (i in 1:n) {\n17:     onset_to_hosp[i] ~ lognormal(meanlog, sdlog) T[0, time_since_onset[i]];\n18:   }\n19: }\n\n\n\n\n\n\n\n\nTipTake 5 minutes\n\n\n\nFamiliarise yourself with the model above. Which line introduces the truncation, i.e. the fact that we have not been able to observe hospitalisation times beyond the cutoff of (here) 70 days?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nLine 17 defines the upper limit of onset_to_hosp as time_since_onset. This line introduces the truncation using the T[0, time_since_onset[i]] syntax, which is equivalent to target += lognormal_lpdf(onset_to_hosp[i] | meanlog, sdlog) - lognormal_lcdf(time_since_onset[i] | meanlog, sdlog). This is normalising the likelihood by the CDF of the relative observation time.\n\n\n\nNow we can use this model to re-estimate the parameters of the delay distribution:\n\ntres &lt;- nfidd_sample(tmod,\n  data = list(\n    n = nrow(df_realtime),\n    onset_to_hosp = df_realtime$onset_to_hosp, \n    time_since_onset = 70 - df_realtime$onset_time\n  )\n)\n\n\ntres\n\n variable    mean  median   sd  mad      q5     q95 rhat ess_bulk ess_tail\n  lp__    -115.57 -115.27 0.98 0.70 -117.56 -114.65 1.01      747     1128\n  meanlog    1.77    1.77 0.04 0.04    1.71    1.84 1.01     1317     1197\n  sdlog      0.50    0.50 0.03 0.03    0.46    0.55 1.01     1050     1023\n\n\nLet’s also check the mean and standard deviation of the truncation-adjusted delay distribution:\n\ntres |&gt;\n  summarise_lognormal()\n\n      mean             sd       \n Min.   :5.719   Min.   :2.684  \n 1st Qu.:6.469   1st Qu.:3.351  \n Median :6.665   Median :3.561  \n Mean   :6.688   Mean   :3.604  \n 3rd Qu.:6.888   3rd Qu.:3.828  \n Max.   :8.162   Max.   :5.408  \n\n\n\n\n\n\n\n\nTipTake 10 minutes\n\n\n\nTry estimating the delays at different times (i.e. varying the observation cut-off) and also try re-simulating the delays using different parameters of the delay distribution. Can you establish under which conditions the bias in estimation gets worse?",
    "crumbs": [
      "Biases in delay distributions"
    ]
  },
  {
    "objectID": "sessions/biases-in-delay-distributions.html#decision-framework-for-bias-correction",
    "href": "sessions/biases-in-delay-distributions.html#decision-framework-for-bias-correction",
    "title": "Biases in delay distributions",
    "section": "Decision framework for bias correction",
    "text": "Decision framework for bias correction\nWhen faced with real epidemiological data, it can be challenging to decide which bias corrections to apply. The flowchart below provides practical guidance for making these decisions (Charniga et al. 2024):\n\n\n\nDecision framework for bias adjustment\n\n\n\n\n\n\n\n\nTipKey decision points\n\n\n\nThe flowchart highlights several critical decision points:\n\nTime variation interest: Whether you need to model changes in delays over time\nAnalysis type: Retrospective vs real-time analysis affects truncation concerns\nObservation cut-off: Whether truncation near primary events affects representativeness\nAscertainment method: Whether cases are identified via primary or secondary events",
    "crumbs": [
      "Biases in delay distributions"
    ]
  },
  {
    "objectID": "sessions/biases-in-delay-distributions.html#challenge",
    "href": "sessions/biases-in-delay-distributions.html#challenge",
    "title": "Biases in delay distributions",
    "section": "Challenge",
    "text": "Challenge\n\nWe have looked at censoring and truncation separately, but in reality often both are present. Can you combine the two in a model?\nThe solutions we introduced for addressing censoring and truncation are only some possible ones for the censoring problem. Other solutions reduce the biases from estimation even further. For an overview, see the review by (Park et al. 2024).",
    "crumbs": [
      "Biases in delay distributions"
    ]
  },
  {
    "objectID": "sessions/biases-in-delay-distributions.html#methods-in-practice",
    "href": "sessions/biases-in-delay-distributions.html#methods-in-practice",
    "title": "Biases in delay distributions",
    "section": "Methods in practice",
    "text": "Methods in practice\n\nCharniga et al. (2024) summarises challenges in estimating delay distributions into a set of best practices, with a practical flowchart and reporting checklist.\nThe primarycensored R package provides a framework for dealing with censored and truncated delay distributions. It implements methods and techniques for handling primary event censoring, secondary event censoring, and right truncation in a unified and efficient manner with analytical solutions.\nThe epidist package extends primarycensored to work with brms for estimating epidemiological delay distributions. It enables flexible modelling including time-varying components, spatial effects, and partially pooled estimates of demographic characteristics.\nThe coarseDataTools package provides methods for double censored data but does not support truncation.",
    "crumbs": [
      "Biases in delay distributions"
    ]
  },
  {
    "objectID": "sessions/biases-in-delay-distributions.html#references",
    "href": "sessions/biases-in-delay-distributions.html#references",
    "title": "Biases in delay distributions",
    "section": "References",
    "text": "References\n\n\nCharniga, Kelly, Sang Woo Park, Andrei R. Akhmetzhanov, Anne Cori, Jonathan Dushoff, Sebastian Funk, Katelyn M. Gostic, et al. 2024. “Best Practices for Estimating and Reporting Epidemiological Delay Distributions of Infectious Diseases.” PLOS Computational Biology 20 (10): e1012520. https://doi.org/10.1371/journal.pcbi.1012520.\n\n\nPark, Sang Woo, Andrei R. Akhmetzhanov, Kelly Charniga, Anne Cori, Nicholas G. Davies, Jonathan Dushoff, Sebastian Funk, et al. 2024. “Estimating Epidemiological Delay Distributions for Infectious Diseases.” medRxiv. https://doi.org/10.1101/2024.01.12.24301247.",
    "crumbs": [
      "Biases in delay distributions"
    ]
  },
  {
    "objectID": "reference/using-our-stan-models.html",
    "href": "reference/using-our-stan-models.html",
    "title": "Using our Stan models",
    "section": "",
    "text": "This guide covers how to use NFIDD’s Stan tools for working with both package models and custom Stan code.",
    "crumbs": [
      "Using our Stan models"
    ]
  },
  {
    "objectID": "reference/using-our-stan-models.html#using-stan-models",
    "href": "reference/using-our-stan-models.html#using-stan-models",
    "title": "Using our Stan models",
    "section": "Using Stan Models",
    "text": "Using Stan Models\n\nBasic Package Models\nUse models included in the NFIDD package:\n\nnfidd_stan_models()\n\n [1] \"censored-delay-model\"           \"coin\"                          \n [3] \"estimate-inf-and-r-rw-forecast\" \"estimate-inf-and-r-rw\"         \n [5] \"estimate-inf-and-r\"             \"estimate-infections\"           \n [7] \"estimate-r\"                     \"gamma\"                         \n [9] \"joint-nowcast-with-r\"           \"joint-nowcast\"                 \n[11] \"lognormal\"                      \"mechanistic-r\"                 \n[13] \"simple-nowcast-rw\"              \"simple-nowcast\"                \n[15] \"statistical-r\"                  \"truncated-delay-model\"         \n\n\nCreate a model using a package model:\n\nmodel &lt;- nfidd_cmdstan_model(\"simple-nowcast\")\n\nSample with course defaults (faster):\n\nfit &lt;- nfidd_sample(model, data = your_data)\n\n\n\nCustom Stan Files\nUse a custom Stan file:\n\nmodel &lt;- nfidd_cmdstan_model(model_file = \"path/to/your/model.stan\")\n\nYou still get access to NFIDD Stan functions:\n\nfit &lt;- nfidd_sample(model, data = your_data)\n\n\n\nCustom Include Paths\nSet globally using R options:\n\noptions(nfidd.stan_path = \"/path/to/your/stan\")\nmodel &lt;- nfidd_cmdstan_model(\"simple-nowcast\")\n\nOr override per-model:\n\nmodel &lt;- nfidd_cmdstan_model(\n  model_file = \"custom.stan\",\n  include_paths = c(\"/custom/path1\", \"/custom/path2\")\n)",
    "crumbs": [
      "Using our Stan models"
    ]
  },
  {
    "objectID": "reference/using-our-stan-models.html#working-with-stan-functions",
    "href": "reference/using-our-stan-models.html#working-with-stan-functions",
    "title": "Using our Stan models",
    "section": "Working with Stan Functions",
    "text": "Working with Stan Functions\n\nDiscover Available Functions\nGet all function names from NFIDD:\n\nfunctions &lt;- nfidd_stan_functions()\nfunctions\n\n[1] \"combine_obs_with_predicted_obs_rng\" \"condition_onsets_by_report\"        \n[3] \"convolve_with_delay\"                \"geometric_diff_ar\"                 \n[5] \"geometric_random_walk\"              \"observe_onsets_with_delay\"         \n[7] \"pop_bounded_renewal\"                \"renewal\"                           \n\n\nFind which files contain specific functions:\n\nfiles &lt;- nfidd_stan_function_files(functions = c(\"renewal\"))\nfiles\n\n[1] \"functions/renewal.stan\"\n\n\n\n\nExtract Functions for Local Use\nLoad specific functions as a string:\n\nrenewal_code &lt;- nfidd_load_stan_functions(\n  functions = c(\"renewal\")\n)\n\nShow first few lines:\n\ncat(substr(renewal_code, 1, 200), \"...\")\n\n// Stan functions from nfidd version 1.1.2.9000\narray[] real renewal(real I0, array[] real R, array[] real gen_time) {\n  // length of time series\n  int n = num_elements(R);\n  int max_gen_time = num_el ...\n\n\nWrite functions to a temporary file for demonstration:\n\ntemp_file &lt;- file.path(tempdir(), \"my_functions.stan\")\nnfidd_load_stan_functions(\n  functions = c(\"renewal\"),\n  write_to_file = TRUE,\n  output_file = temp_file,\n  wrap_in_block = TRUE\n)\n\nStan functions written to: /tmp/RtmpHAn5lh/my_functions.stan\n\n\n[1] \"functions {\\n// Stan functions from nfidd version 1.1.2.9000\\narray[] real renewal(real I0, array[] real R, array[] real gen_time) {\\n  // length of time series\\n  int n = num_elements(R);\\n  int max_gen_time = num_elements(gen_time);\\n  array[n + 1] real I;\\n  I[1] = I0;\\n  for (i in 1:n) {\\n    int first_index = max(1, i - max_gen_time + 1);\\n    int len = i - first_index + 1;\\n    array[len] real I_segment = I[first_index:i];\\n    array[len] real gen_pmf = reverse(gen_time[1:len]);\\n    I[i + 1] = dot_product(I_segment, gen_pmf) * R[i];\\n  }\\n  return(I[2:(n + 1)]);\\n}\\n}\"\n\n\nVerify file was created:\n\ncat(\"File created at:\", temp_file)\n\nFile created at: /tmp/RtmpHAn5lh/my_functions.stan\n\ncat(\"\\nFile exists:\", file.exists(temp_file))\n\n\nFile exists: TRUE\n\n\n\n\nLoad All Functions\nGet all NFIDD functions:\n\nall_functions &lt;- nfidd_load_stan_functions()\n\nWrite all functions to file:\n\nnfidd_load_stan_functions(\n  write_to_file = TRUE,\n  output_file = \"nfidd_functions.stan\",\n  wrap_in_block = TRUE\n)",
    "crumbs": [
      "Using our Stan models"
    ]
  },
  {
    "objectID": "reference/using-our-stan-models.html#writing-package-models-locally",
    "href": "reference/using-our-stan-models.html#writing-package-models-locally",
    "title": "Using our Stan models",
    "section": "Writing Package Models Locally",
    "text": "Writing Package Models Locally\nSometimes you want to copy a package model to modify it locally rather than modifying the package source.\nLoad a package model:\n\nmodel &lt;- nfidd_cmdstan_model(\"simple-nowcast\", compile = FALSE)\n\nGet the Stan code from the model:\n\nstan_code &lt;- model$code()\n\nWrite it to a local file:\n\nwriteLines(stan_code, \"local-simple-nowcast.stan\")\n\nCreate functions directory:\n\ndir.create(\"functions\", showWarnings = FALSE)\n\nCopy all function files individually:\n\nstan_functions_path &lt;- file.path(nfidd_stan_path(), \"functions\")\nfunction_files &lt;- list.files(stan_functions_path, pattern = \"\\\\.stan$\", full.names = TRUE)\n\nfor (file in function_files) {\n  file.copy(file, \"functions/\", overwrite = TRUE)\n}\n\nSet options to use local functions:\n\noptions(nfidd.stan_path = \".\")\n\nNow you can modify the local model and it will use local functions:\n\nmodified_model &lt;- nfidd_cmdstan_model(model_file = \"local-simple-nowcast.stan\")",
    "crumbs": [
      "Using our Stan models"
    ]
  },
  {
    "objectID": "reference/using-our-stan-models.html#practical-workflows",
    "href": "reference/using-our-stan-models.html#practical-workflows",
    "title": "Using our Stan models",
    "section": "Practical Workflows",
    "text": "Practical Workflows\n\nBuilding Custom Models with NFIDD Functions\nExtract the functions you need:\n\nnfidd_load_stan_functions(\n  functions = c(\"renewal\", \"convolve_with_delay\"),\n  write_to_file = TRUE,\n  output_file = \"my_functions.stan\"\n)\n\nCreate your custom model file:\n\n#include my_functions.stan\n\ndata {\n  // Your data block\n}\n\nparameters {\n  // Your parameters\n}\n\nmodel {\n  // Use NFIDD functions like renewal(), delay_pmf(), etc.\n}\n\nCompile and use:\n\nmodel &lt;- nfidd_cmdstan_model(\n  model_file = \"my_custom_model.stan\",\n  include_paths = \".\"\n)\n\n\n\nDevelopment Workflow\nExplore existing models:\nSee what models are available:\n\nnfidd_stan_models()\n\n [1] \"censored-delay-model\"           \"coin\"                          \n [3] \"estimate-inf-and-r-rw-forecast\" \"estimate-inf-and-r-rw\"         \n [5] \"estimate-inf-and-r\"             \"estimate-infections\"           \n [7] \"estimate-r\"                     \"gamma\"                         \n [9] \"joint-nowcast-with-r\"           \"joint-nowcast\"                 \n[11] \"lognormal\"                      \"mechanistic-r\"                 \n[13] \"simple-nowcast-rw\"              \"simple-nowcast\"                \n[15] \"statistical-r\"                  \"truncated-delay-model\"         \n\n\nLook at model locations:\n\nnfidd_stan_path()\n\n[1] \"/home/runner/work/_temp/Library/nfidd/stan\"\n\n\nUnderstand function dependencies:\nSee all available functions:\n\nnfidd_stan_functions()\n\nFind which files contain functions you need:\n\nnfidd_stan_function_files(functions = c(\"function_name\"))\n\nBuild incrementally:\nStart with package model:\n\nbase_model &lt;- nfidd_cmdstan_model(\"simple-nowcast\")\n\nExtend with custom functions:\n\ncustom_model &lt;- nfidd_cmdstan_model(\n  model_file = \"extended_model.stan\"\n)",
    "crumbs": [
      "Using our Stan models"
    ]
  },
  {
    "objectID": "reference/using-our-stan-models.html#file-organisation",
    "href": "reference/using-our-stan-models.html#file-organisation",
    "title": "Using our Stan models",
    "section": "File Organisation",
    "text": "File Organisation\nFor projects using custom Stan code:\nyour_project/\n├── models/\n│   ├── my_model.stan\n│   └── functions/\n│       └── my_functions.stan\nIn your R code:\n\noptions(nfidd.stan_path = c(\"models/functions\", nfidd_stan_path()))\nmodel &lt;- nfidd_cmdstan_model(model_file = \"models/my_model.stan\")\n\nThis setup gives you access to both your custom functions and all NFIDD functions.",
    "crumbs": [
      "Using our Stan models"
    ]
  },
  {
    "objectID": "reference/sessions.html",
    "href": "reference/sessions.html",
    "title": "Session timetable",
    "section": "",
    "text": "Monday June 23\n\n\nMonday June 23: 09.00-09.30\n\nIntroduction: the course and the instructors (10 mins)\nMotivating the course: From an epidemiological line list to informing decisions in real-time (20 mins)\n\n\n\n\nMonday June 23: 09.30-10.30\n\nIntroduction: statistical concepts used in the course and how they can be applied in stan (20 mins)\nPractice: introduction to estimation in stan (40 mins)\n\n\n\n\nMonday June 23: 11.00-11.45\n\nIntroduction: epidemiological delays and how to represent them with probability distributions (10 mins)\nPractice: simulate and estimate epidemiological delays (30 mins)\nWrap up (5 mins)\n\n\n\n\nMonday June 23: 11.45-12.30 and 14.00-14.45\n\nIntroduction: biases in delay distributions (10 mins)\nPractice:\n\nsimulating biases in delay distributions and estimating delays without adjustment on these data (35 mins)\nestimating delay distributions with adjustments for bias (35 mins)\n\nWrap up (10 mins)\n\n\n\n\nMonday June 23: 14.45-15.30\n\nIntroduction: Using delay distributions to model the data generating process of an epidemic (15 mins)\nPractice: implementing a convolution model and identifying potential problems (30 mins)\n\n\n\n\nMonday June 23: 16.00-17.30\n\nIntroduction: the time-varying reproduction number (10 mins)\nPractice:\n\nusing the renewal equation to estimate R (35 mins)\ncombining \\(R_t\\) estimation with delay distribution convolutions (35 mins)\n\nWrap up (10 mins)",
    "crumbs": [
      "Session timetable"
    ]
  },
  {
    "objectID": "reference/sessions.html#session-0-introduction-and-course-overview",
    "href": "reference/sessions.html#session-0-introduction-and-course-overview",
    "title": "Session timetable",
    "section": "",
    "text": "Monday June 23: 09.00-09.30\n\nIntroduction: the course and the instructors (10 mins)\nMotivating the course: From an epidemiological line list to informing decisions in real-time (20 mins)",
    "crumbs": [
      "Session timetable"
    ]
  },
  {
    "objectID": "reference/sessions.html#session-1-r-stan-and-statistical-concept-background",
    "href": "reference/sessions.html#session-1-r-stan-and-statistical-concept-background",
    "title": "Session timetable",
    "section": "",
    "text": "Monday June 23: 09.30-10.30\n\nIntroduction: statistical concepts used in the course and how they can be applied in stan (20 mins)\nPractice: introduction to estimation in stan (40 mins)",
    "crumbs": [
      "Session timetable"
    ]
  },
  {
    "objectID": "reference/sessions.html#session-2-delay-distributions",
    "href": "reference/sessions.html#session-2-delay-distributions",
    "title": "Session timetable",
    "section": "",
    "text": "Monday June 23: 11.00-11.45\n\nIntroduction: epidemiological delays and how to represent them with probability distributions (10 mins)\nPractice: simulate and estimate epidemiological delays (30 mins)\nWrap up (5 mins)",
    "crumbs": [
      "Session timetable"
    ]
  },
  {
    "objectID": "reference/sessions.html#session-3-biases-in-delay-distributions",
    "href": "reference/sessions.html#session-3-biases-in-delay-distributions",
    "title": "Session timetable",
    "section": "",
    "text": "Monday June 23: 11.45-12.30 and 14.00-14.45\n\nIntroduction: biases in delay distributions (10 mins)\nPractice:\n\nsimulating biases in delay distributions and estimating delays without adjustment on these data (35 mins)\nestimating delay distributions with adjustments for bias (35 mins)\n\nWrap up (10 mins)",
    "crumbs": [
      "Session timetable"
    ]
  },
  {
    "objectID": "reference/sessions.html#session-4-using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic",
    "href": "reference/sessions.html#session-4-using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic",
    "title": "Session timetable",
    "section": "",
    "text": "Monday June 23: 14.45-15.30\n\nIntroduction: Using delay distributions to model the data generating process of an epidemic (15 mins)\nPractice: implementing a convolution model and identifying potential problems (30 mins)",
    "crumbs": [
      "Session timetable"
    ]
  },
  {
    "objectID": "reference/sessions.html#session-5-r_t-estimation-and-the-renewal-equation",
    "href": "reference/sessions.html#session-5-r_t-estimation-and-the-renewal-equation",
    "title": "Session timetable",
    "section": "",
    "text": "Monday June 23: 16.00-17.30\n\nIntroduction: the time-varying reproduction number (10 mins)\nPractice:\n\nusing the renewal equation to estimate R (35 mins)\ncombining \\(R_t\\) estimation with delay distribution convolutions (35 mins)\n\nWrap up (10 mins)",
    "crumbs": [
      "Session timetable"
    ]
  },
  {
    "objectID": "reference/sessions.html#session-6-nowcasting-concepts",
    "href": "reference/sessions.html#session-6-nowcasting-concepts",
    "title": "Session timetable",
    "section": "Session 6: Nowcasting concepts",
    "text": "Session 6: Nowcasting concepts\nTuesday June 24: 09.15-10.30\n\nIntroduction: nowcasting as a right-truncation problem (10 mins)\nPractice:\n\nsimulating the delay distribution (25 mins)\nnowcasting using pre-estimated delay distributions (30 mins)\n\nWrap up (10 mins)",
    "crumbs": [
      "Session timetable"
    ]
  },
  {
    "objectID": "reference/sessions.html#session-7-nowcasting-with-an-unknown-reporting-delay",
    "href": "reference/sessions.html#session-7-nowcasting-with-an-unknown-reporting-delay",
    "title": "Session timetable",
    "section": "Session 7: Nowcasting with an unknown reporting delay",
    "text": "Session 7: Nowcasting with an unknown reporting delay\nTuesday June 24: 11.00-12.30\n\nIntroduction: joint estimation of delays and nowcasts (10 mins)\nPractice:\n\njoint estimation of delays and nowcasts (35 mins)\njoint estimation of delays, nowcasts and reproduction numbers (35 mins)\n\nWrap up (10 mins)",
    "crumbs": [
      "Session timetable"
    ]
  },
  {
    "objectID": "reference/sessions.html#session-8-forecasting-concepts",
    "href": "reference/sessions.html#session-8-forecasting-concepts",
    "title": "Session timetable",
    "section": "Session 8: Forecasting concepts",
    "text": "Session 8: Forecasting concepts\nTuesday June 24: 14.00-14.45\n\nIntroduction: forecasting as an epidemiological problem, and its relationship with nowcasting and \\(R_t\\) estimation (10 mins)\nPractice: extending a model into the future and visualising your forecast (30 minutes)\nWrap up (5 minutes)",
    "crumbs": [
      "Session timetable"
    ]
  },
  {
    "objectID": "reference/sessions.html#session-9-forecast-evaluation",
    "href": "reference/sessions.html#session-9-forecast-evaluation",
    "title": "Session timetable",
    "section": "Session 9: Forecast evaluation",
    "text": "Session 9: Forecast evaluation\nTuesday June 24: 14.45-15.30\n\nIntroduction: quantitatively evaluating forecasts (10 mins)\nPractice: evaluating forecasts with a range of metrics (30 mins)\nWrap up (5 mins)",
    "crumbs": [
      "Session timetable"
    ]
  },
  {
    "objectID": "reference/sessions.html#session-10-forecasting-models",
    "href": "reference/sessions.html#session-10-forecasting-models",
    "title": "Session timetable",
    "section": "Session 10: Forecasting models",
    "text": "Session 10: Forecasting models\nTuesday June 24: 16.00-17.30\n\nIntroduction: a spectrum of forecasting models (10 mins)\nPractice: evaluating forecasts from a range of models (60 mins)\nWrap up & discussion (20 mins)",
    "crumbs": [
      "Session timetable"
    ]
  },
  {
    "objectID": "reference/sessions.html#session-11-forecasting-with-ensembles",
    "href": "reference/sessions.html#session-11-forecasting-with-ensembles",
    "title": "Session timetable",
    "section": "Session 11: Forecasting with ensembles",
    "text": "Session 11: Forecasting with ensembles\nWednesday June 25: 09.15-10.30\n\nIntroduction: strategies for collating and combining models (10 mins)\nPractice: evaluating methods for ensemble forecasts (55 mins)\nWrap up (10 mins)",
    "crumbs": [
      "Session timetable"
    ]
  },
  {
    "objectID": "reference/sessions.html#session-12-methods-in-the-real-world",
    "href": "reference/sessions.html#session-12-methods-in-the-real-world",
    "title": "Session timetable",
    "section": "Session 12: Methods in the real world",
    "text": "Session 12: Methods in the real world\nWednesday June 25: 11.00-12.00\n\nPresentations and Q&A on uses of nowcasts & forecasts in the real world (60 mins)",
    "crumbs": [
      "Session timetable"
    ]
  },
  {
    "objectID": "reference/sessions.html#session-13-end-of-course-summary",
    "href": "reference/sessions.html#session-13-end-of-course-summary",
    "title": "Session timetable",
    "section": "Session 13: End of course summary",
    "text": "Session 13: End of course summary\nWednesday June 25: 12.00-12.30\n\nSummary of the course (10 mins)\nFinal discussion and closing (20 mins)\n\nFurther reading",
    "crumbs": [
      "Session timetable"
    ]
  },
  {
    "objectID": "reference/help.html",
    "href": "reference/help.html",
    "title": "Getting help",
    "section": "",
    "text": "For any questions about the course or its content, feel free to use the Discussion board",
    "crumbs": [
      "Getting help"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nowcasting and forecasting infectious disease dynamics",
    "section": "",
    "text": "A course and living resource for learning about nowcasting and forecasting infectious disease dynamics.\nWe invite anyone to use the materials here in their own teaching and learning. For questions and discussion of the course or its content, we welcome all users to the Discussion board. We welcome all forms of contributions, additions and suggestions for improving the materials.\nAll materials here are provided under the permissive MIT License."
  },
  {
    "objectID": "authors.html",
    "href": "authors.html",
    "title": "Authors",
    "section": "",
    "text": "The NFIDD course was developed by the NFIDD course contributors:"
  },
  {
    "objectID": "authors.html#how-to-cite",
    "href": "authors.html#how-to-cite",
    "title": "Authors",
    "section": "How to Cite",
    "text": "How to Cite\nIf you use materials from this course in your work, please cite:\n\nNFIDD course contributors (2025). NFIDD: Nowcasting and Forecasting Infectious Disease Dynamics. Version [VERSION]. DOI: [PLACEHOLDER - Zenodo DOI will be added upon release]\n\n\nBibTeX Entry\n@misc{nfidd2025,\n  author = {{Sam Abbott, Katherine Sherratt, Sebastian Funk}},\n  title = {{NFIDD: Nowcasting and Forecasting Infectious Disease Dynamics}},\n  year = {2025},\n  version = {[VERSION]},\n  doi = {[PLACEHOLDER - Zenodo DOI]},\n  url = {https://nfidd.github.io/nfidd/}\n}"
  },
  {
    "objectID": "authors.html#license",
    "href": "authors.html#license",
    "title": "Authors",
    "section": "License",
    "text": "License\nAll course materials are provided under the MIT License, making them freely available for teaching, learning, and adaptation."
  },
  {
    "objectID": "authors.html#contributing",
    "href": "authors.html#contributing",
    "title": "Authors",
    "section": "Contributing",
    "text": "Contributing\nWe welcome contributions to improve and expand the course materials. You can:\n\nReport issues or suggest improvements\nJoin discussions\nSubmit pull requests"
  },
  {
    "objectID": "authors.html#acknowledgements",
    "href": "authors.html#acknowledgements",
    "title": "Authors",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe thank all participants who have helped improve these materials through feedback, bug reports, and code contributions."
  },
  {
    "objectID": "getting-started.html",
    "href": "getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "NFIDD (Nowcasting and Forecasting Infectious Disease Dynamics) is an MIT-licensed library of sessions for learning about nowcasting and forecasting of infectious disease surveillance data. This course is a living resource designed to help epidemiologists, public health professionals, and researchers understand and apply real-time analysis methods to infectious disease surveillance data."
  },
  {
    "objectID": "getting-started.html#what-is-nfidd",
    "href": "getting-started.html#what-is-nfidd",
    "title": "Getting started",
    "section": "",
    "text": "NFIDD (Nowcasting and Forecasting Infectious Disease Dynamics) is an MIT-licensed library of sessions for learning about nowcasting and forecasting of infectious disease surveillance data. This course is a living resource designed to help epidemiologists, public health professionals, and researchers understand and apply real-time analysis methods to infectious disease surveillance data."
  },
  {
    "objectID": "getting-started.html#set-up",
    "href": "getting-started.html#set-up",
    "title": "Getting started",
    "section": "Set up",
    "text": "Set up\nEach session in this course uses R code for demonstration. All the content is self-contained within a software package designed for the course.\nYou have three options for using this course:\n\nWeb-only: View sessions on the website\nLocal setup: Install R, packages, and download course materials for the full interactive experience\nHybrid: Install just the packages but use the website for viewing content\n\n\n\n\n\n\n\nImportant\n\n\n\nInstallation Issues? If you’re having trouble with any installation steps, ask for help early! Don’t skip ahead - each step builds on the previous ones. On the day of the course, we have a small number of web clients available as backup if installation issues persist.\n\n\n\nSummary of Installation Steps\nIf you choose the local setup option, here’s what you’ll need to do:\n\nInstall R and RStudio (use the Visual Editor for best notebook experience)\nInstall the nfidd R package\nInstall cmdstan\nDownload course materials (if using the hybrid approach you may not need to do this)\n\nDon’t skip any steps - they all work together to provide the full course experience.\n\n\nInstalling R\n\nR is used as the main programming language. You can check which version you have by typing R.version in your R session. We recommend installing the latest R version 4.5.2 (2025-10-31).\nRStudio is a popular graphic user interface (GUI). Its Visual Editor provides the best experience of going through this course. Please make sure you update RStudio to the latest version.\n\n\n\nInstalling additional requirements\nBefore you get started with the course, you will first need to install the following software.\n\nInstallation of the nfidd package\nTo install the packages needed in the course, including the nfidd package that contains data files and helper functions used throughout:\n\noptions(repos = c(\n  \"CRAN\" = \"https://cloud.r-project.org\",\n  \"stan-dev\" = \"https://stan-dev.r-universe.dev\",\n  \"epiforecasts\" = \"https://epiforecasts.r-universe.dev\"\n))\ninstall.packages(\"nfidd\", dependencies = TRUE)\n\nThen you can check that the installation completed successfully by loading the package into your R session:\n\nlibrary(\"nfidd\")\n\n\n\n\nInstalling cmdstan\nThe course relies on running stan through the cmdstanr R package, which itself uses the cmdstan software. This requires a separate installation step:\n\ncmdstanr::install_cmdstan()\n\n\n\n\n\n\n\nNote\n\n\n\nThis may take a few minutes. Also you’re likely to see lots of warnings and other messages printed to your screen - don’t worry, this is normal and doesn’t mean there is a problem.\n\n\nIf there are any problems with this, you can try (on Windows) to fix them using\n\ncmdstanr::check_cmdstan_toolchain(fix = TRUE)\n\nYou can test that you have a working cmdstanr setup using\n\ncmdstanr::cmdstan_version()\n\n[1] \"2.37.0\"\n\n\nFor more details, and for links to resources in case something goes wrong, see the Getting Started with CmdStanr vignette of the package."
  },
  {
    "objectID": "getting-started.html#accessing-the-course",
    "href": "getting-started.html#accessing-the-course",
    "title": "Getting started",
    "section": "Accessing the course",
    "text": "Accessing the course\nIf you want to use the local workflow, you will need a local copy of the course material.\n\nDirectly download the course material:\n\n\n\n\n\n\nTip\n\n\n\nDownload\n\n\nAlternatively, if you are familiar with git you can clone the repo.\nIf you prefer to use the hybrid workflow, you can view each session on the website, and copy-paste the code into your own R script. In that case you don’t need to download the material.\n\nTip: if you hover over each code chunk on the website you can use a “Copy” button at the top right corner.\n\n\n\nInteracting with a local copy of the course material\nA benefit of downloading or cloning all the material is that you can interact with the session files directly.\nIn this course, all content is written using Quarto notebooks (.qmd files). This means that we can combine text with code and see the output directly. The notebooks are then directly reproduced on the course website (for example, this page).\nRecommended approach: Work with the notebooks using RStudio’s visual editor mode. See guidance on this below.\n\n\n\n\n\n\nTipUsing RStudio’s Visual Editor (Recommended for Notebooks)\n\n\n\n\nOpen a session notebook: Each session is saved as a .qmd file in nfidd/sessions/\nSwitch to Visual mode: Look for the “Visual” button in the top-left of the editor pane (next to “Source”)\nExecute code: Use the green “play” button at the top-right corner of each code chunk, or Ctrl/Cmd + Enter for line-by-line execution\nVisual mode benefits: Easier to read formatted text and equations, better experience with code chunks and outputs\n\n\n\nAlternative approaches that also work:\nOther editors: Use VS Code or other editors that support Quarto notebooks. The .qmd files will work in any Quarto-compatible environment.\n\n\n\n\n\n\nTip\n\n\n\nThe Quarto extension for VS Code also supports a visual editor mode. You can find it in the command palette."
  },
  {
    "objectID": "getting-started.html#day-of-course-updates",
    "href": "getting-started.html#day-of-course-updates",
    "title": "Getting started",
    "section": "Day-of-Course Updates",
    "text": "Day-of-Course Updates\nIf you’re returning to the course after some time or joining a live session, you may want to update your setup to ensure you have the latest content and package versions.\n\nQuick Update (Recommended)\n\nUpdate the nfidd package (this is quick and ensures you have the latest functions):\n\ninstall.packages(\"nfidd\", dependencies = TRUE)\n\nDownload fresh course materials if using local files:\n\nDownload the latest version: Download\nOr use git pull if you cloned the repository\n\nYou don’t need to reinstall cmdstan unless you’re having specific issues with it"
  },
  {
    "objectID": "reference/further_reading.html",
    "href": "reference/further_reading.html",
    "title": "Further reading",
    "section": "",
    "text": "The following is a highly subjective list of papers we would recommend to read for those interested in engaging further with the topics discussed here. You can also access this via Zotero in an open living library, which you are welcome to contribute to.",
    "crumbs": [
      "Further reading"
    ]
  },
  {
    "objectID": "reference/further_reading.html#delay-estimation",
    "href": "reference/further_reading.html#delay-estimation",
    "title": "Further reading",
    "section": "Delay estimation",
    "text": "Delay estimation\n\nPark et al. (2024) provide a comprehensive overview of challenges in estimating delay distribution and how to overcome them.\nCharniga et al. (2024) summarises challenges in estimating delay distributions into a set of best practices.",
    "crumbs": [
      "Further reading"
    ]
  },
  {
    "objectID": "reference/further_reading.html#r_t-estimation",
    "href": "reference/further_reading.html#r_t-estimation",
    "title": "Further reading",
    "section": "\\(R_t\\) estimation",
    "text": "\\(R_t\\) estimation\n\nGostic et al. (2020) provides an overview of some of the challenges in estimating reproduction numbers.\nBrockhaus et al. (2023) compares reproduction number estimates from different models and investigates their differences.",
    "crumbs": [
      "Further reading"
    ]
  },
  {
    "objectID": "reference/further_reading.html#nowcasting",
    "href": "reference/further_reading.html#nowcasting",
    "title": "Further reading",
    "section": "Nowcasting",
    "text": "Nowcasting\n\nWolffram et al. (2023) compares the performance of a range of methods that were used in a nowcasting hub and investigates what might explain performance differences.\nLison et al. (2024) develops a generative model for nowcasting and \\(R_t\\) estimation and compares its performance to an approach where the steps for estimating incidence and reproduction number are separated.\nStoner, Economou, and Halliday (2020) contains a nice review of different methods for nowcasting evaluates a range of methods, in addition to introducing a new approach.",
    "crumbs": [
      "Further reading"
    ]
  },
  {
    "objectID": "reference/further_reading.html#forecasting",
    "href": "reference/further_reading.html#forecasting",
    "title": "Further reading",
    "section": "Forecasting",
    "text": "Forecasting\n\nFunk et al. (2019) evaluates the performance of a forecasting method that combines a mechanistic SEIR model with a random walk prior for the reproduction number.\nHeld, Meyer, and Bracher (2017) makes a compelling argument for the use of probabilistic forecasts and evaluates spatial forecasts based on routine surveillance data.\nLopez et al. (2024) describes the difficulty encountered in making accurate forecasts in COVID-19 cases in the US COVID-19 Forecast Hub.\nAsher (2018) describes model that implements an extension to the random walk with a drift term.\n“CDCgov/Wastewater-Informed-Covid-Forecasting” (2025), code repo for Wastewater-informed COVID-19 forecasting.\nHyndman and Athanasopoulos (2021) is a free online text book on forecasting with a range of time series models and a great resource for finding out more about them.",
    "crumbs": [
      "Further reading"
    ]
  },
  {
    "objectID": "reference/further_reading.html#ensembles",
    "href": "reference/further_reading.html#ensembles",
    "title": "Further reading",
    "section": "Ensembles",
    "text": "Ensembles\n\nSherratt et al. (2023) investigates the performance of different ensembles in the European COVID-19 Forecast Hub.\nAmaral et al. (2025) discusses the challenges in improving on the predictive performance of simpler approaches using weighted ensembles.",
    "crumbs": [
      "Further reading"
    ]
  },
  {
    "objectID": "reference/further_reading.html#references",
    "href": "reference/further_reading.html#references",
    "title": "Further reading",
    "section": "References",
    "text": "References\n\n\nAmaral, André Victor Ribeiro, Daniel Wolffram, Paula Moraga, and Johannes Bracher. 2025. “Post-Processing and Weighted Combination of Infectious Disease Nowcasts.” PLoS Computational Biology 21 (3): e1012836. https://doi.org/10.1371/journal.pcbi.1012836.\n\n\nAsher, Jason. 2018. “Forecasting Ebola with a Regression Transmission Model.” Epidemics 22 (March): 50–55. https://doi.org/10.1016/j.epidem.2017.02.009.\n\n\nBrockhaus, Elisabeth K., Daniel Wolffram, Tanja Stadler, Michael Osthege, Tanmay Mitra, Jonas M. Littek, Ekaterina Krymova, et al. 2023. “Why Are Different Estimates of the Effective Reproductive Number so Different? A Case Study on COVID-19 in Germany.” PLOS Computational Biology 19 (11): e1011653. https://doi.org/10.1371/journal.pcbi.1011653.\n\n\n“CDCgov/Wastewater-Informed-Covid-Forecasting.” 2025. Centers for Disease Control and Prevention.\n\n\nCharniga, Kelly, Sang Woo Park, Andrei R. Akhmetzhanov, Anne Cori, Jonathan Dushoff, Sebastian Funk, Katelyn M. Gostic, et al. 2024. “Best Practices for Estimating and Reporting Epidemiological Delay Distributions of Infectious Diseases.” PLOS Computational Biology 20 (10): e1012520. https://doi.org/10.1371/journal.pcbi.1012520.\n\n\nFunk, Sebastian, Anton Camacho, Adam J. Kucharski, Rachel Lowe, Rosalind M. Eggo, and W. John Edmunds. 2019. “Assessing the Performance of Real-Time Epidemic Forecasts: A Case Study of Ebola in the Western Area Region of Sierra Leone, 2014-15.” PLOS Computational Biology 15 (2): e1006785. https://doi.org/10.1371/journal.pcbi.1006785.\n\n\nGostic, Katelyn M., Lauren McGough, Edward B. Baskerville, Sam Abbott, Keya Joshi, Christine Tedijanto, Rebecca Kahn, et al. 2020. “Practical Considerations for Measuring the Effective Reproductive Number, Rt.” PLOS Computational Biology 16 (12): e1008409. https://doi.org/10.1371/journal.pcbi.1008409.\n\n\nHeld, Leonhard, Sebastian Meyer, and Johannes Bracher. 2017. “Probabilistic Forecasting in Infectious Disease Epidemiology: The 13th Armitage Lecture.” Statistics in Medicine 36 (22): 3443–60. https://doi.org/10.1002/sim.7363.\n\n\nHyndman, Rob J, and George Athanasopoulos. 2021. Forecasting: Principles and Practice. 3rd ed. Melbourne, Australia: OTexts.\n\n\nLison, Adrian, Sam Abbott, Jana Huisman, and Tanja Stadler. 2024. “Generative Bayesian Modeling to Nowcast the Effective Reproduction Number from Line List Data with Missing Symptom Onset Dates.” PLOS Computational Biology 20 (4): e1012021. https://doi.org/10.1371/journal.pcbi.1012021.\n\n\nLopez, Velma K., Estee Y. Cramer, Robert Pagano, John M. Drake, Eamon B. O’Dea, Madeline Adee, Turgay Ayer, et al. 2024. “Challenges of COVID-19 Case Forecasting in the US, 2020–2021.” PLOS Computational Biology 20 (5): e1011200. https://doi.org/10.1371/journal.pcbi.1011200.\n\n\nPark, Sang Woo, Andrei R. Akhmetzhanov, Kelly Charniga, Anne Cori, Nicholas G. Davies, Jonathan Dushoff, Sebastian Funk, et al. 2024. “Estimating Epidemiological Delay Distributions for Infectious Diseases.” medRxiv. https://doi.org/10.1101/2024.01.12.24301247.\n\n\nSherratt, Katharine, Hugo Gruson, Rok Grah, Helen Johnson, Rene Niehus, Bastian Prasse, Frank Sandmann, et al. 2023. “Predictive Performance of Multi-Model Ensemble Forecasts of COVID-19 Across European Nations.” Edited by Amy Wesolowski, Neil M Ferguson, Jeffrey L Shaman, and Sen Pei. eLife 12 (April): e81916. https://doi.org/10.7554/eLife.81916.\n\n\nStoner, Oliver, Theo Economou, and Alba Halliday. 2020. “A Powerful Modelling Framework for Nowcasting and Forecasting COVID-19 and Other Diseases.” arXiv. https://doi.org/10.48550/arXiv.1912.05965.\n\n\nWolffram, Daniel, Sam Abbott, Matthias an der Heiden, Sebastian Funk, Felix Günther, Davide Hailer, Stefan Heyder, et al. 2023. “Collaborative Nowcasting of COVID-19 Hospitalization Incidences in Germany.” PLOS Computational Biology 19 (8): e1011394. https://doi.org/10.1371/journal.pcbi.1011394.",
    "crumbs": [
      "Further reading"
    ]
  },
  {
    "objectID": "reference/learning_objectives.html",
    "href": "reference/learning_objectives.html",
    "title": "Learning outcomes",
    "section": "",
    "text": "The skills and methods taught in this course apply broadly across infectious disease epidemiology, from outbreak response to routine surveillance of endemic diseases. While examples often use outbreak scenarios for clarity, participants should consider how these approaches apply to their epidemiological contexts.",
    "crumbs": [
      "Learning outcomes"
    ]
  },
  {
    "objectID": "reference/learning_objectives.html#r-and-statistical-concepts-used",
    "href": "reference/learning_objectives.html#r-and-statistical-concepts-used",
    "title": "Learning outcomes",
    "section": "R and statistical concepts used",
    "text": "R and statistical concepts used\n\nunderstanding of common probability distributions used for epidemiological delays\nfamiliarity with using stan to estimate parameters of a probability distribution",
    "crumbs": [
      "Learning outcomes"
    ]
  },
  {
    "objectID": "reference/learning_objectives.html#delay-distributions",
    "href": "reference/learning_objectives.html#delay-distributions",
    "title": "Learning outcomes",
    "section": "Delay distributions",
    "text": "Delay distributions\n\nability to estimate parameters of epidemiological delay distributions\nunderstanding of the ubiquity of delays in epidemiological data",
    "crumbs": [
      "Learning outcomes"
    ]
  },
  {
    "objectID": "reference/learning_objectives.html#biases-in-delay-distributions",
    "href": "reference/learning_objectives.html#biases-in-delay-distributions",
    "title": "Learning outcomes",
    "section": "Biases in delay distributions",
    "text": "Biases in delay distributions\n\nunderstanding of how censoring affects the estimation and interpretation of epidemiological delay distributions\nability to estimate parameters of probability distributions from observed delays, taking into account censoring, using R\nunderstanding of right truncation in epidemiolgical data\nability to estimate parameters of probability distributions from observed delays, taking into account truncation, in R",
    "crumbs": [
      "Learning outcomes"
    ]
  },
  {
    "objectID": "reference/learning_objectives.html#using-delay-distributions-to-model-the-data-generating-process",
    "href": "reference/learning_objectives.html#using-delay-distributions-to-model-the-data-generating-process",
    "title": "Learning outcomes",
    "section": "Using delay distributions to model the data generating process",
    "text": "Using delay distributions to model the data generating process\n\nunderstanding of using delay distributions to model population-level data generating processes\nability to use convolutions to combine count data with a distribution of individual probabilities, adjusting continuous probability distributions with discretisation\nunderstanding of the need to introduce additional uncertainty to account for the observation process at a population level",
    "crumbs": [
      "Learning outcomes"
    ]
  },
  {
    "objectID": "reference/learning_objectives.html#r_t-estimation-and-the-renewal-equation",
    "href": "reference/learning_objectives.html#r_t-estimation-and-the-renewal-equation",
    "title": "Learning outcomes",
    "section": "\\(R_t\\) estimation and the renewal equation",
    "text": "\\(R_t\\) estimation and the renewal equation\n\nunderstanding of the reproduction number and challenges in its estimation\nawareness of broad categories of methods for estimating the reproduction number, including estimation from population-level data\nunderstanding of the renewal equation as an epidemiological model\nfamiliarity with the generation time as a particular type of delay distributions\nability to estimate static and time-varying reproduction numbers from time-series data in R",
    "crumbs": [
      "Learning outcomes"
    ]
  },
  {
    "objectID": "reference/learning_objectives.html#nowcasting",
    "href": "reference/learning_objectives.html#nowcasting",
    "title": "Learning outcomes",
    "section": "Nowcasting",
    "text": "Nowcasting\n\nunderstanding of nowcasting as a particular right truncation problem\nAbility to perform a simple nowcast in R\nawareness of the breadth of methods to perform nowcasting\n\\(R_t\\) estimation as a nowcasting problem",
    "crumbs": [
      "Learning outcomes"
    ]
  },
  {
    "objectID": "reference/learning_objectives.html#forecasting",
    "href": "reference/learning_objectives.html#forecasting",
    "title": "Learning outcomes",
    "section": "Forecasting",
    "text": "Forecasting\n\nunderstanding of forecasting as an epidemiological problem, and its relationship with nowcasting and \\(R_t\\) estimation\nability to use a forecasting model on an epidemiological time series in R",
    "crumbs": [
      "Learning outcomes"
    ]
  },
  {
    "objectID": "reference/learning_objectives.html#evaluating-forecasts-and-nowcasts",
    "href": "reference/learning_objectives.html#evaluating-forecasts-and-nowcasts",
    "title": "Learning outcomes",
    "section": "Evaluating forecasts (and nowcasts)",
    "text": "Evaluating forecasts (and nowcasts)\n\nability to visually assess forecasts and nowcasts\nfamiliarity with metrics for evaluating probabilistic forecasts and their properties\nability to score probabilistic forecasts in R\nability to compare different models by their forecast scores",
    "crumbs": [
      "Learning outcomes"
    ]
  },
  {
    "objectID": "reference/learning_objectives.html#ensemble-models",
    "href": "reference/learning_objectives.html#ensemble-models",
    "title": "Learning outcomes",
    "section": "Ensemble models",
    "text": "Ensemble models\n\nunderstanding of predictive ensembles and their properties\nability to create a predictive ensemble of forecasts in R",
    "crumbs": [
      "Learning outcomes"
    ]
  },
  {
    "objectID": "reference/stan.html",
    "href": "reference/stan.html",
    "title": "Stan Reference",
    "section": "",
    "text": "Uncertainty is an unavoidable part of real-world data\nNeed to estimate unobserved quantities (true cases, the effective reproduction number, future trends)\nBayesian approach naturally handles missing data and incorporates prior knowledge\nStan is a powerful tool for Bayesian inference\n\n\n\nStan is a probabilistic programming language for Bayesian inference. It allows us to:\n\nWrite down models in a text file (ending .stan)\nGenerate samples from the posterior distribution using various methods (like Hamiltonian Monte Carlo)\nGet proper uncertainty quantification for our estimates\n\nWe use Stan because: - We’ll need to estimate things (delays, reproduction numbers, case numbers now and in the future) - We’ll want to correctly specify uncertainty - We’ll want to incorporate our domain expertise - We’ll do this using Bayesian inference",
    "crumbs": [
      "Stan Reference"
    ]
  },
  {
    "objectID": "reference/stan.html#why-stan-in-epidemiology",
    "href": "reference/stan.html#why-stan-in-epidemiology",
    "title": "Stan Reference",
    "section": "",
    "text": "Uncertainty is an unavoidable part of real-world data\nNeed to estimate unobserved quantities (true cases, the effective reproduction number, future trends)\nBayesian approach naturally handles missing data and incorporates prior knowledge\nStan is a powerful tool for Bayesian inference\n\n\n\nStan is a probabilistic programming language for Bayesian inference. It allows us to:\n\nWrite down models in a text file (ending .stan)\nGenerate samples from the posterior distribution using various methods (like Hamiltonian Monte Carlo)\nGet proper uncertainty quantification for our estimates\n\nWe use Stan because: - We’ll need to estimate things (delays, reproduction numbers, case numbers now and in the future) - We’ll want to correctly specify uncertainty - We’ll want to incorporate our domain expertise - We’ll do this using Bayesian inference",
    "crumbs": [
      "Stan Reference"
    ]
  },
  {
    "objectID": "reference/stan.html#just-enough-probability",
    "href": "reference/stan.html#just-enough-probability",
    "title": "Stan Reference",
    "section": "2. Just Enough Probability",
    "text": "2. Just Enough Probability\n\nWhat is a Probability Distribution?\nA probability distribution describes how likely different outcomes are for a random variable.\nDiscrete distributions (for countable outcomes): - Example: Number of horse kick deaths per year (0, 1, 2, 3, …) - Poisson distribution with \\(\\lambda\\) = 0.61 kicks per year - Probability of exactly 2 deaths: dpois(2, lambda = 0.61) = 0.11\nContinuous distributions (for measurable quantities): - Example: Temperature in Stockholm tomorrow - Normal distribution with mean 23°C, standard deviation 2°C - Probability density at 30°C: dnorm(30, mean = 23, sd = 2) = 0.0001\n\n\nTwo Key Operations\n\nCalculate probability/density: Given parameters, what’s the probability of observing a value?\nGenerate samples: Given parameters, simulate random observations\n\n\n\nDistributions You’ll See\n\n\n\n\n\n\n\n\n\nDistribution\nUsed for\nParameters\nExample\n\n\n\n\nLog-normal\nDelays (e.g., incubation period)\nmeanlog, sdlog\nSymptom onset time\n\n\nGamma\nDelays (alternative)\nshape, rate\nHospital length of stay\n\n\nNegative Binomial\nCount data with overdispersion\nmu, phi\nDaily case counts\n\n\nNormal\nContinuous measurements\nmean, sd\nLog(Rt)\n\n\nBeta\nProportions\nalpha, beta\nReporting probability\n\n\n\n\n\nKey Concept: Parameters vs Data\n\nData: What we observe (onset dates, test results)\nParameters: What we want to learn (mean delay, Rt)\nModel: How parameters generate data\n\n\n\nBayesian Inference in a Nutshell\nThe generative model can produce output which looks like data given a set of parameters \\(\\theta\\)\nIdea of Bayesian inference: treat \\(\\theta\\) as random variables (with a probability distribution) and condition on data: posterior probability p(\\(\\theta\\) | data) as target of inference.\nUsing Bayes’ rule: \\[p(\\theta | \\text{data}) = \\frac{p(\\text{data} | \\theta) p(\\theta)}{p(\\text{data})}\\]\n\np(data | θ) is the likelihood\np(θ) is the prior\np(data) is a normalisation constant\n\nIn words: (posterior) ∝ (normalised likelihood) × (prior)\n\n\nMCMC: Getting Samples from the Posterior\nMarkov-chain Monte Carlo (MCMC) is a method to generate samples of \\(\\theta\\) that come from the posterior distribution given data. This is our target of inference.\nMany flavours of MCMC exist:\n\nMetropolis-Hastings\nHamiltonian Monte Carlo (what Stan uses)\nGibbs sampling\n\nStan uses the No-U-TURN sampler a form of Hamiltonian Monte Carlo sampler to efficiently explore the posterior distribution and generate samples. This has been shown to be efficient across a wide range of models It’s main limitation is that it doesn’t support discrete latent parameters.",
    "crumbs": [
      "Stan Reference"
    ]
  },
  {
    "objectID": "reference/stan.html#stan-basics-for-this-course",
    "href": "reference/stan.html#stan-basics-for-this-course",
    "title": "Stan Reference",
    "section": "3. Stan Basics for This Course",
    "text": "3. Stan Basics for This Course\n\nModel Structure\nEvery Stan model has three essential blocks:\n\ndata {\n  // What we observe\n  int&lt;lower=0&gt; N;  // number of observations\n  array[N] real y; // the observations\n}\n\nparameters {\n  // What we want to estimate\n  real&lt;lower=0&gt; mean_delay;\n  real&lt;lower=0&gt; sd_delay;\n}\n\nmodel {\n  // How parameters relate to data\n  // Prior\n  mean_delay ~ normal(5, 2);\n  sd_delay ~ normal(2, 1);\n  \n  // Likelihood\n  y ~ lognormal(log(mean_delay), sd_delay);\n}\n\n\n\nRunning Stan from R\n\nlibrary(cmdstanr)\nlibrary(nfidd)\n\n# Load a model\nmodel &lt;- nfidd_cmdstan_model(\"delays\")\n\n# Prepare data\nstan_data &lt;- list(\n  N = length(observations),\n  y = observations\n)\n\n# Fit model\nfit &lt;- nfidd_sample(model, data = stan_data)\n\n# Extract results\nsummarise_draws(fit)",
    "crumbs": [
      "Stan Reference"
    ]
  },
  {
    "objectID": "reference/stan.html#debugging-tips",
    "href": "reference/stan.html#debugging-tips",
    "title": "Stan Reference",
    "section": "4. Debugging Tips",
    "text": "4. Debugging Tips\n\nCheck Your Priors\n\nDo your priors make sense?\nWhat range of predictions do they lead to?\nDoes this make sense for the data you’re modelling?\n\nBefore fitting your model, simulate from your priors to check they make sense:\n\n# Example: checking delay priors\nprior_mean_delay &lt;- abs(rnorm(1000, mean = 1.5, sd = 0.5))\nprior_sd_delay &lt;- abs(rnorm(1000, mean = 0.5, sd = 0.1))\n\n# Simulate delays from these priors\nprior_delays &lt;- rlnorm(\n  1000, \n  meanlog = log(prior_mean_delay), \n  sdlog = prior_sd_delay\n)\n\n# Plot to check if reasonable\nhist(prior_delays, breaks = 50)\n\n\n\n\n\n\n\nquantile(prior_delays, c(0.025, 0.5, 0.975))\n\n     2.5%       50%     97.5% \n0.3895514 1.4303321 4.7254487 \n\n\n\n\nCheck your model\n\nDoes your model make sense?\nDoes it have the parameters you expect?\nDoes it have the data you expect?\n\n\n\nCheck your data\n\nDoes your data make sense?\nDoes it have the variables you expect?\nDoes it have the right number of observations?\n\n\n\nCheck your results\n\nDoes your model fit the data?\nDo the parameter estimates make sense?\n\nUse posterior predictive checks to see if your model can reproduce data similar to what you observed: - Does it capture the central tendency of the data? - Does it get the right amount of variability? - Can it reproduce extreme values appropriately?\nCompare your posterior predictions to the observed data using density overlays, empirical CDFs, and summary statistics.\n\n\nCommon Issues & Solutions\n\n\n\n\n\n\n\n\n\nIssue\nSymptom\nWhat\nSolution\n\n\n\n\nDivergent transitions\nWarning message\nSampler can’t explore posterior due to geometry of the posterior\nIncrease adapt_delta\n\n\nTreedepth\nWarning message\nSampler hit maximum tree depth before completing trajectory (complex posterior geometry requires longer integration paths)\nIncrease max_treedepth (e.g., to 12 or 15)\n\n\nLow ESS\nESS &lt; 400\nPoor mixing between chains\nRun more iterations\n\n\nHigh Rhat\nRhat &gt; 1.01\nChains sampling different distributions (not converged)\nCheck model specification or run more warmup iterations\n\n\nInitialization failed\nError on startup\nNumerical instability at start\nSet init values\n\n\n\n\n\nUnderstanding Divergences\nDivergent transitions occur when the sampler encounters regions of the posterior that are difficult to explore. This often happens when:\n\nThe posterior has areas of high curvature\nParameters are on very different scales\nThere are strong correlations between parameters\n\nSolutions:\n\n# Increase adapt_delta (default is 0.8)\nfit &lt;- nfidd_sample(\n  model, \n  data = stan_data,\n  adapt_delta = 0.95  # or even 0.99 for difficult models\n)\n\nor reparameterise your model.",
    "crumbs": [
      "Stan Reference"
    ]
  },
  {
    "objectID": "reference/stan.html#going-deeper",
    "href": "reference/stan.html#going-deeper",
    "title": "Stan Reference",
    "section": "5. Going Deeper",
    "text": "5. Going Deeper\n\nCourse Models\n\nSee /inst/stan/ for all model code\nEach model has comments explaining the approach\n\n\n\nExternal Resources\n\nStan User’s Guide\nPrior Choice Wiki\nBayesian Workflow\nStan Forums - helpful community",
    "crumbs": [
      "Stan Reference"
    ]
  },
  {
    "objectID": "sessions/R-estimation-and-the-renewal-equation.html",
    "href": "sessions/R-estimation-and-the-renewal-equation.html",
    "title": "\\(R_t\\) estimation and the renewal equation",
    "section": "",
    "text": "In the last session we used the idea of convolutions as a way to interpret individual time delays at a population level. In that session, we linked symptom onsets back to infections. Now we want to link infections themselves together over time, knowing that current infections were infected by past infections. Correctly capturing this transmission process is crucial to modelling infections in the present and future.\n\n\n\nIntroduction to the reproduction number\n\n\n\n\nThe aim of this session is to introduce the renewal equation as an infection generating process, and to show how it can be used to estimate a time-varying reproduction number.\n\n\n\n\n\n\nNoteSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/R-estimation-and-the-renewal-equation.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and tidyr packages for data wrangling, ggplot2 library for plotting, the tidybayes package for extracting results of the inference, and the purrr package for functional programming.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\nlibrary(\"purrr\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "$R_t$ estimation and the renewal equation"
    ]
  },
  {
    "objectID": "sessions/R-estimation-and-the-renewal-equation.html#slides",
    "href": "sessions/R-estimation-and-the-renewal-equation.html#slides",
    "title": "\\(R_t\\) estimation and the renewal equation",
    "section": "",
    "text": "Introduction to the reproduction number",
    "crumbs": [
      "$R_t$ estimation and the renewal equation"
    ]
  },
  {
    "objectID": "sessions/R-estimation-and-the-renewal-equation.html#objectives",
    "href": "sessions/R-estimation-and-the-renewal-equation.html#objectives",
    "title": "\\(R_t\\) estimation and the renewal equation",
    "section": "",
    "text": "The aim of this session is to introduce the renewal equation as an infection generating process, and to show how it can be used to estimate a time-varying reproduction number.\n\n\n\n\n\n\nNoteSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/R-estimation-and-the-renewal-equation.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and tidyr packages for data wrangling, ggplot2 library for plotting, the tidybayes package for extracting results of the inference, and the purrr package for functional programming.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\nlibrary(\"purrr\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "$R_t$ estimation and the renewal equation"
    ]
  },
  {
    "objectID": "sessions/R-estimation-and-the-renewal-equation.html#source-file",
    "href": "sessions/R-estimation-and-the-renewal-equation.html#source-file",
    "title": "\\(R_t\\) estimation and the renewal equation",
    "section": "",
    "text": "The source file of this session is located at sessions/R-estimation-and-the-renewal-equation.qmd.",
    "crumbs": [
      "$R_t$ estimation and the renewal equation"
    ]
  },
  {
    "objectID": "sessions/R-estimation-and-the-renewal-equation.html#libraries-used",
    "href": "sessions/R-estimation-and-the-renewal-equation.html#libraries-used",
    "title": "\\(R_t\\) estimation and the renewal equation",
    "section": "",
    "text": "In this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and tidyr packages for data wrangling, ggplot2 library for plotting, the tidybayes package for extracting results of the inference, and the purrr package for functional programming.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\nlibrary(\"purrr\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.",
    "crumbs": [
      "$R_t$ estimation and the renewal equation"
    ]
  },
  {
    "objectID": "sessions/R-estimation-and-the-renewal-equation.html#initialisation",
    "href": "sessions/R-estimation-and-the-renewal-equation.html#initialisation",
    "title": "\\(R_t\\) estimation and the renewal equation",
    "section": "",
    "text": "We set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "$R_t$ estimation and the renewal equation"
    ]
  },
  {
    "objectID": "sessions/R-estimation-and-the-renewal-equation.html#simulating-a-geometric-random-walk",
    "href": "sessions/R-estimation-and-the-renewal-equation.html#simulating-a-geometric-random-walk",
    "title": "\\(R_t\\) estimation and the renewal equation",
    "section": "Simulating a geometric random walk",
    "text": "Simulating a geometric random walk\nYou can have a look at an R function for performing the geometric random walk:\n\ngeometric_random_walk\n\nfunction (init, noise, std) \n{\n    n &lt;- length(noise) + 1\n    x &lt;- numeric(n)\n    x[1] &lt;- log(init)\n    for (i in 2:n) {\n        x[i] &lt;- x[i - 1] + noise[i - 1] * std\n    }\n    exp(x)\n}\n&lt;bytecode: 0x5653f8180e10&gt;\n&lt;environment: namespace:nfidd&gt;\n\n\n\n\n\n\n\n\nTipTake 2 minutes\n\n\n\nLook at this function and try to understand what it does. Note that we use the fact that we can generate a random normally distributed variable \\(X\\) with mean 0 and standard deviation \\(\\sigma\\) by mutiplying a standard normally distributed variable (i.e., mean 0 and standard deviation 1) \\(Y\\) with \\(\\sigma\\). Using this non-centred parameterisation for efficiency) will improve our computational efficency later when using an equivalent function in stan\n\n\nWe can use this function to simulate a random walk (shown in black below) and compare it to a normal(1,1) prior (shown in red below) that we used in the previous model:\n\nR &lt;- geometric_random_walk(init = 1, noise = rnorm(100), std = 0.1)\ndata &lt;- tibble(t = seq_along(R), R = exp(R))\n\n# Generate normal(1,1) prior samples for comparison\nnormal_prior &lt;- rnorm(100, mean = 1, sd = 1)\nnormal_data &lt;- tibble(t = seq_along(normal_prior), R = normal_prior)\n\nggplot(data, aes(x = t, y = R)) +\n  geom_line() +\n  geom_line(data = normal_data, aes(x = t, y = R), colour = \"red\") +\n  labs(title = \"Simulated data from a random walk model\",\n       subtitle = \"Random walk (black) vs Normal(1,1) prior (red)\",\n       x = \"Time\",\n       y = \"R\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipTake 2 minutes\n\n\n\nRepeat this multiple times, either with the same parameters or changing some to get a feeling for what this does compared to the normal(1,1) prior.",
    "crumbs": [
      "$R_t$ estimation and the renewal equation"
    ]
  },
  {
    "objectID": "sessions/R-estimation-and-the-renewal-equation.html#estimating-r_t-with-a-geometric-random-walk-prior",
    "href": "sessions/R-estimation-and-the-renewal-equation.html#estimating-r_t-with-a-geometric-random-walk-prior",
    "title": "\\(R_t\\) estimation and the renewal equation",
    "section": "Estimating \\(R_t\\) with a geometric random walk prior",
    "text": "Estimating \\(R_t\\) with a geometric random walk prior\nWe can now include this in a stan model,\n\nrw_mod &lt;- nfidd_cmdstan_model(\"estimate-inf-and-r-rw\")\nrw_mod\n\n 1: functions {\n 2:   #include \"functions/convolve_with_delay.stan\"\n 3:   #include \"functions/renewal.stan\"\n 4:   #include \"functions/geometric_random_walk.stan\"\n 5: }\n 6: \n 7: data {\n 8:   int n;                // number of days\n 9:   int I0;              // number initially infected\n10:   array[n] int obs;     // observed symptom onsets\n11:   int gen_time_max;     // maximum generation time\n12:   array[gen_time_max] real gen_time_pmf;  // pmf of generation time distribution\n13:   int&lt;lower = 1&gt; ip_max; // max incubation period\n14:   array[ip_max + 1] real ip_pmf;\n15: }\n16: \n17: parameters {\n18:   real&lt;lower = 0&gt; init_R;         // initial reproduction number\n19:   array[n-1] real rw_noise; // random walk noise\n20:   real&lt;lower = 0&gt; rw_sd; // random walk standard deviation\n21: }\n22: \n23: transformed parameters {\n24:   array[n] real R = geometric_random_walk(init_R, rw_noise, rw_sd);\n25:   array[n] real infections = renewal(I0, R, gen_time_pmf);\n26:   array[n] real onsets = convolve_with_delay(infections, ip_pmf);\n27: }\n28: \n29: model {\n30:   // priors\n31:   init_R ~ normal(1, 0.5) T[0, ];\n32:   rw_noise ~ std_normal();\n33:   rw_sd ~ normal(0, 0.05) T[0, ];\n34:   obs ~ poisson(onsets);\n35: }\n\n\nNote that the model is very similar to the one we used earlier, but with the addition of the random walk model for the reproduction number using a function in stan that does the same as our R function of the same name we defined.\nWe can now generate estimates from this model:\n\ndata &lt;- list(\n  n = length(obs) - 1,\n  obs = obs[-1],\n  I0 = inf_ts$infections[1],\n  gen_time_max = length(gen_time_pmf),\n  gen_time_pmf = gen_time_pmf,\n  ip_max = length(ip_pmf) - 1,\n  ip_pmf = ip_pmf\n)\nr_rw_inf_fit &lt;- nfidd_sample(\n  rw_mod, data = data, max_treedepth = 12, \n  init = \\() list(init_R = 1, rw_sd = 0.01)\n)\n\n\nr_rw_inf_fit\n\n    variable     mean   median   sd  mad       q5      q95 rhat ess_bulk\n lp__        21467.47 21467.52 8.65 8.51 21452.73 21481.08 1.01      802\n init_R          1.62     1.62 0.20 0.19     1.31     1.97 1.00     1372\n rw_noise[1]     0.14     0.14 0.99 1.03    -1.46     1.73 1.00     2310\n rw_noise[2]     0.11     0.08 0.96 0.95    -1.46     1.68 1.00     2283\n rw_noise[3]     0.07     0.11 0.96 0.95    -1.56     1.66 1.00     2107\n rw_noise[4]     0.01     0.01 1.00 1.01    -1.58     1.67 1.00     2649\n rw_noise[5]    -0.02    -0.04 0.99 1.00    -1.64     1.58 1.00     2174\n rw_noise[6]    -0.11    -0.11 1.01 1.00    -1.74     1.53 1.01     2567\n rw_noise[7]    -0.18    -0.16 1.01 1.08    -1.82     1.48 1.00     2337\n rw_noise[8]    -0.16    -0.17 1.00 1.01    -1.79     1.53 1.00     2304\n ess_tail\n     1155\n     1370\n     1508\n     1404\n     1760\n     1678\n     1618\n     1184\n     1583\n     1214\n\n # showing 10 of 566 rows (change via 'max_rows' argument or 'cmdstanr_max_rows' option)\n\n\n\nAs this is a more complex model we have increased the max_treedepth parameter to 12 to allow for more complex posterior distributions and we have also provided an initialisation for the init_R and rw_sd parameters to help the sampler find the right region of parameter space. This is a common technique when fitting more complex models and is needed as it is hard a priori to know where the sampler should start.\n\nWe can again extract and visualise the posteriors in the same way as earlier.\n\nrw_posteriors &lt;- r_rw_inf_fit |&gt;\n  gather_draws(infections[infection_day], R[infection_day]) |&gt;\n  ungroup() |&gt;\n  mutate(infection_day = infection_day - 1) |&gt;\n  filter(.draw %in% sample(.draw, 100))\n\n\nrw_inf_posterior &lt;- rw_posteriors |&gt;\n  filter(.variable == \"infections\")\nggplot(mapping = aes(x = infection_day)) +\n  geom_line(\n    data = rw_inf_posterior, mapping = aes(y = .value, group = .draw), alpha = 0.1\n  ) +\n  geom_line(data = inf_ts, mapping = aes(y = infections), colour = \"red\") +\n  labs(title = \"Infections, estimated (grey) and observed (red)\", \n       subtitle = \"Model 3: renewal equation with random walk\")\n\n\n\n\n\n\n\n\nand reproduction numbers\n\nrw_r_inf_posterior &lt;- rw_posteriors |&gt;\n  filter(.variable == \"R\") |&gt;\n  filter(.draw %in% sample(.draw, 100))\nggplot(\n  data = rw_r_inf_posterior,\n  mapping = aes(x = infection_day, y = .value, group = .draw)\n) +\n  geom_line(alpha = 0.1) +\n  labs(title = \"Estimated R\", \n       subtitle = \"Model 3: renewal equation with random walk\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipTake 2 minutes\n\n\n\nWhat do you think of these estimates? In particular, what do you think of the estimates at the beginning and end of the outbreak? Are they consistent with the true Rt trajectory and with each other? Are they consistent with the estimates from the previous model?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nThe estimates are smoothest so far, and the model is able to capture the true Rt trajectory more accurately than the previous model. Unlike the previous model, the model is able to capture the true Rt trajectory at the end of the outbreak with variance increasing towards the date of estimation. The infection estimates are the least uncertain from any model and potentially overly certain as they don’t fully cover the observed infections.",
    "crumbs": [
      "$R_t$ estimation and the renewal equation"
    ]
  },
  {
    "objectID": "sessions/R-estimation-and-the-renewal-equation.html#comparing-the-models",
    "href": "sessions/R-estimation-and-the-renewal-equation.html#comparing-the-models",
    "title": "\\(R_t\\) estimation and the renewal equation",
    "section": "Comparing the models",
    "text": "Comparing the models\nWe can now plot all the Rt trajectories from the models together to compare them.\n\n## earlier posteriors\nr_posterior &lt;- r_posterior |&gt;\n  mutate(data = \"infections\")\nr_inf_posterior &lt;- r_inf_posterior |&gt;\n  mutate(data = \"onsets (normal)\")\nrw_r_inf_posterior &lt;- rw_r_inf_posterior |&gt;\n  mutate(data = \"onsets (random walk)\")\n\nall_posteriors &lt;- rbind(\n  r_inf_posterior,\n  rw_r_inf_posterior,\n  r_posterior\n)\n\nggplot(\n  all_posteriors,\n  mapping = aes(x = infection_day, y = .value, group = .draw,\n                colour = data)\n) +\n  geom_line(alpha = 0.1) +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(\n    title = \"Rt estimates from renewal equation models\",\n    subtitle = paste(\n      \"Estimates from infections, from symptom onsets, and from onsets with a\",\n      \"random walk\"\n    )\n  ) +\n  guides(colour = guide_legend(override.aes = list(alpha = 1))) + \n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipTake 2 minutes\n\n\n\nRevisit your answers to the previous questions in this session. What are the key differences between the Rt estimates from the models? Which model do you think is the best fit for the data? Which model is the most realistic?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWe can see that the estimates are smoother when using the random walk model for the reproduction number, compared to the normal model. The model that fits directly to infections has the lowest uncertainty, which we would expect as it doesn’t have to infer the number of infections from symptom onsets but even here the reproduction number estimates are unrealistically noisy due to the assumption of independence between infections each day when infection counts are low. The random walk model is the most realistic model, as it is able to capture the true Rt trajectory more accurately than the normal model. The model that fits directly to infections is the best fit for the data, but depends on the availability of infections data which in practice is never available.\n\n\n\n\n\n\n\n\n\nTipTake 2 minutes\n\n\n\nCompare the results across the models used in this session, and the one used in the session on convolutions. How do the models vary in the number of parameters that need to be estimated? How do the assumptions about the infections time series differ between the models? What do you notice about the level of uncertainty in the estimates of infections and \\(R_t\\) over the course of the time series?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWe can see that using the renewal model as generative model we recover the time series of infections more accurately compared to previously when we assumed independent numbers of infections each day and that using a more believable model (i.e the geometric random walk) for the reproduction number improves things even more. Of course, this is helped by the fact that the data was generated by a model similar to the renewal model used for inference.",
    "crumbs": [
      "$R_t$ estimation and the renewal equation"
    ]
  },
  {
    "objectID": "sessions/R-estimation-and-the-renewal-equation.html#challenge",
    "href": "sessions/R-estimation-and-the-renewal-equation.html#challenge",
    "title": "\\(R_t\\) estimation and the renewal equation",
    "section": "Challenge",
    "text": "Challenge\n\nWe have used symptom onsets under the assumption that every infected person develops symptoms. Earlier we also created a time series of hospitalisation under the assumption that only a proportion (e.g., 30%) of symptomatic individuals get hospitalised. How would you change the model in this case? What are the implications for inference?\nIf you have time you could try re-running the experiment with different \\(R_t\\) trajectories (using the renewal() function to simulate data) and delay distributions to see whether results change.",
    "crumbs": [
      "$R_t$ estimation and the renewal equation"
    ]
  },
  {
    "objectID": "sessions/R-estimation-and-the-renewal-equation.html#methods-in-practice",
    "href": "sessions/R-estimation-and-the-renewal-equation.html#methods-in-practice",
    "title": "\\(R_t\\) estimation and the renewal equation",
    "section": "Methods in practice",
    "text": "Methods in practice\n\nEpiEstim provides a range of accessible tools for estimating \\(R_t\\), using a simpler approach than the model we have used here.\nEpiNow2 (Abbott et al. 2025) package again implements a range of models similar to the model we have used here.\nEpidemia Bhatt et al. (2020) implements a regression model approach to \\(R_t\\) estimation.\nIn this course we focused on the instantaneous reproduction number. An alternative is the case reproduction number implemented in Wallinga and Teunis (2004).\nWe face many choices when estimating the reproduction number. Brockhaus et al. (2023) explores the impact of these choices on resulting estimates.\nGostic et al. (2020) has further guidance on best practice.",
    "crumbs": [
      "$R_t$ estimation and the renewal equation"
    ]
  },
  {
    "objectID": "sessions/R-estimation-and-the-renewal-equation.html#references",
    "href": "sessions/R-estimation-and-the-renewal-equation.html#references",
    "title": "\\(R_t\\) estimation and the renewal equation",
    "section": "References",
    "text": "References\n\n\nAbbott, Sam, Joel Hellewell, Katharine Sherratt, Katelyn Gostic, Joe Hickson, Hamada S. Badr, Michael DeWitt, James M. Azam, EpiForecasts, and Sebastian Funk. 2025. “EpiNow2: Estimate Real-Time Case Counts and Time-Varying Epidemiological Parameters.”\n\n\nBhatt, Samir, Neil Ferguson, Seth Flaxman, Axel Gandy, Swapnil Mishra, and James A. Scott. 2020. “Semi-Mechanistic Bayesian Modeling of COVID-19 with Renewal Processes.” arXiv. https://doi.org/10.48550/arXiv.2012.00394.\n\n\nBrockhaus, Elisabeth K., Daniel Wolffram, Tanja Stadler, Michael Osthege, Tanmay Mitra, Jonas M. Littek, Ekaterina Krymova, et al. 2023. “Why Are Different Estimates of the Effective Reproductive Number so Different? A Case Study on COVID-19 in Germany.” PLOS Computational Biology 19 (11): e1011653. https://doi.org/10.1371/journal.pcbi.1011653.\n\n\nGostic, Katelyn M., Lauren McGough, Edward B. Baskerville, Sam Abbott, Keya Joshi, Christine Tedijanto, Rebecca Kahn, et al. 2020. “Practical Considerations for Measuring the Effective Reproductive Number, Rt.” PLOS Computational Biology 16 (12): e1008409. https://doi.org/10.1371/journal.pcbi.1008409.\n\n\nWallinga, Jacco, and Peter Teunis. 2004. “Different Epidemic Curves for Severe Acute Respiratory Syndrome Reveal Similar Impacts of Control Measures.” American Journal of Epidemiology 160 (6): 509–16. https://doi.org/10.1093/aje/kwh255.",
    "crumbs": [
      "$R_t$ estimation and the renewal equation"
    ]
  },
  {
    "objectID": "sessions/delay-distributions.html",
    "href": "sessions/delay-distributions.html",
    "title": "Delay distributions",
    "section": "",
    "text": "Every chain of infectious disease transmission starts with one person infecting another. But we rarely observe these infection events directly. Instead, we typically collect epidemiological data from events that occur after the time of infection: symptom onsets, hospitalisations and discharge, etc. We often then need to understand how much time has passed between these events. For example, the incubation period might be used to determine the appropriate length of quarantine. We also need to have some estimate for these delays when interpreting new data, in order to understand what is happening now 1 and predict what might happen in the future. However, the length of time between any of these events might be highly variable (between individuals) and fundamentally uncertain (across the population as a whole). We capture this variability using probability distributions, which is the focus of this session.\n\n\n\nIntroduction to epidemiological delays\n\n\n\n\nThe aim of this session is for you to familiarise yourself with the concept of delay distributions used to describe reporting in infectious disease epidemiology. First we’ll look at this working forwards from infections in an outbreak. We’ll use R to probabilistically simulate delays from infection to reporting symptoms and hospitalisations. Then we’ll work only from this set of outcome data and use a stan model to estimate the parameters of one specific delay (in this example, symptom onset to hospitalisation).\n\n\n\n\n\n\nNoteSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/delay-distributions.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the ggplot2 package for plotting, the dplyr and tidyr packages to wrangle data, the lubridate package to deal with dates, the posterior and tidybayes packages for investigating the results of the inference conducted with stan.\n\nlibrary(\"nfidd\")\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"lubridate\")\nlibrary(\"posterior\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(1234)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Delay distributions"
    ]
  },
  {
    "objectID": "sessions/delay-distributions.html#slides",
    "href": "sessions/delay-distributions.html#slides",
    "title": "Delay distributions",
    "section": "",
    "text": "Introduction to epidemiological delays",
    "crumbs": [
      "Delay distributions"
    ]
  },
  {
    "objectID": "sessions/delay-distributions.html#objectives",
    "href": "sessions/delay-distributions.html#objectives",
    "title": "Delay distributions",
    "section": "",
    "text": "The aim of this session is for you to familiarise yourself with the concept of delay distributions used to describe reporting in infectious disease epidemiology. First we’ll look at this working forwards from infections in an outbreak. We’ll use R to probabilistically simulate delays from infection to reporting symptoms and hospitalisations. Then we’ll work only from this set of outcome data and use a stan model to estimate the parameters of one specific delay (in this example, symptom onset to hospitalisation).\n\n\n\n\n\n\nNoteSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/delay-distributions.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the ggplot2 package for plotting, the dplyr and tidyr packages to wrangle data, the lubridate package to deal with dates, the posterior and tidybayes packages for investigating the results of the inference conducted with stan.\n\nlibrary(\"nfidd\")\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"lubridate\")\nlibrary(\"posterior\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(1234)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Delay distributions"
    ]
  },
  {
    "objectID": "sessions/delay-distributions.html#source-file",
    "href": "sessions/delay-distributions.html#source-file",
    "title": "Delay distributions",
    "section": "",
    "text": "The source file of this session is located at sessions/delay-distributions.qmd.",
    "crumbs": [
      "Delay distributions"
    ]
  },
  {
    "objectID": "sessions/delay-distributions.html#libraries-used",
    "href": "sessions/delay-distributions.html#libraries-used",
    "title": "Delay distributions",
    "section": "",
    "text": "In this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the ggplot2 package for plotting, the dplyr and tidyr packages to wrangle data, the lubridate package to deal with dates, the posterior and tidybayes packages for investigating the results of the inference conducted with stan.\n\nlibrary(\"nfidd\")\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"lubridate\")\nlibrary(\"posterior\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.",
    "crumbs": [
      "Delay distributions"
    ]
  },
  {
    "objectID": "sessions/delay-distributions.html#initialisation",
    "href": "sessions/delay-distributions.html#initialisation",
    "title": "Delay distributions",
    "section": "",
    "text": "We set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(1234)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Delay distributions"
    ]
  },
  {
    "objectID": "sessions/delay-distributions.html#load-the-outbreak-data",
    "href": "sessions/delay-distributions.html#load-the-outbreak-data",
    "title": "Delay distributions",
    "section": "Load the outbreak data",
    "text": "Load the outbreak data\nWe will work with a data set that is included in the nfidd R package that you installed initially. The column infection_time is a linelist of infections from our example outbreak, given as a decimal number of days that have passed since the initial infection of the outbreak. It can be loaded with the data command.\n\ndata(infection_times)\nhead(infection_times)\n\n  infection_time\n1       0.000000\n2       2.236708\n3       4.091861\n4       7.347199\n5       8.990060\n6       4.635069\n\n### visualise the infection curve\nggplot(infection_times, aes(x = infection_time)) +\n  geom_histogram(binwidth = 1) +\n  scale_x_continuous(n.breaks = 10) +\n  labs(x = \"Infection time (in days)\", y = \"Number of infections\",\n       title = \"Infections during an outbreak\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn reality, data from infectious disease surveillance will usually be given as dates, not decimals; and those will usually not represent infection, but an observed outcome such as symptom onset or hospital admission. For now we don’t want to spend too much time manipulating dates in R, but we will get back to working with more realistic surveillance data later.",
    "crumbs": [
      "Delay distributions"
    ]
  },
  {
    "objectID": "sessions/delay-distributions.html#working-forwards-simulating-a-sequence-of-delays-after-infection",
    "href": "sessions/delay-distributions.html#working-forwards-simulating-a-sequence-of-delays-after-infection",
    "title": "Delay distributions",
    "section": "Working forwards: simulating a sequence of delays after infection",
    "text": "Working forwards: simulating a sequence of delays after infection\nWe would now like to simulate hospitalisations arising from this outbreak. We will start with our data on infection times, and work forwards to symptom onsets and then hospitalisations. We’ll make the following assumptions about the process from infection to hospital admission:\n\nInfection to symptoms:\n\nWe’ll assume all infections cause symptoms.\nTime from infection to symptom onset (incubation period): We assume that the incubation period is gamma-distributed with shape 5 and rate 1, i.e. a mean of 5 days.\n\nSymptoms to hospital admission:\n\nWe’ll assume that 30% of symptomatic cases become hospitalised.\nTime from symptom onset to hospital admission: We assume that this delay is lognormally distributed, with meanlog 1.75 and sdlog 0.5, corresponding to a mean delay of about a week.\n\n\nLet’s put these assumptions into practice by simulating some data, adding onset and hospitalisation times (in decimal number of days after the first infection) to the infection times. We’ll use random values for each infection from the probability distributions we’ve assumed above.\n\n\n\n\n\n\nTipTake a few minutes\n\n\n\nThroughout the course, we repeatedly use some small chunks of code. Instead of copying these between sessions, we’ve put them into functions in the nfidd R package that we can use when needed. To see exactly what the function does, just type the function name, e.g. add_delays for the function below.\n\n\n\ndf &lt;- add_delays(infection_times)\n\n\n\n\n\n\n\nTipTake 2 minutes\n\n\n\nHave a look at the add_delays() function and try to understand its inner workings. You can also access a help file for this function, just like any other R function, using ?add_delays.\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nThe add_delays() function takes a dataframe with infection_time and creates a complete simulated dataset:\n\nAdds onset_time by sampling gamma-distributed incubation delays (rgamma(n(), shape = 5, rate = 1))\nAdds hosp_time by sampling lognormal-distributed delays from onset (rlnorm(n(), meanlog = 1.75, sdlog = 0.5))\nSets hosp_time to NA for 70% of cases (only 30% are hospitalised) using rbinom()\nReturns a dataframe with all three time columns\n\n\n\n\nNow we can plot infections, hospitalisations and onsets.\n\n# convert our data frame to long format\ndfl &lt;- df |&gt;\n  pivot_longer(\n    cols = c(infection_time, onset_time, hosp_time),\n    names_to = \"type\", values_to = \"time\"\n  ) |&gt; \n  mutate(type = ordered(type, \n    levels = c(\"infection_time\", \"onset_time\", \"hosp_time\"), \n    labels = c(\"Infections\", \"Symptom onsets\", \"Hospitalisations\")\n  ))\n# plot\nggplot(dfl, aes(x = time)) +\n  geom_histogram(position = \"dodge\", binwidth = 1) +\n  facet_wrap(~ type, ncol = 1) +\n  xlab(\"Time (in days)\") +\n  ylab(\"Count\")\n\nWarning: Removed 4524 rows containing non-finite outside the scale range\n(`stat_bin()`).",
    "crumbs": [
      "Delay distributions"
    ]
  },
  {
    "objectID": "sessions/delay-distributions.html#estimating-delay-distributions-from-outcome-data",
    "href": "sessions/delay-distributions.html#estimating-delay-distributions-from-outcome-data",
    "title": "Delay distributions",
    "section": "Estimating delay distributions from outcome data",
    "text": "Estimating delay distributions from outcome data\nAs mentioned above, our data set of infection, symptom onset and hospitalisation times is not the typical data set we encounter in infectious disease surveillance. In reality, we don’t have infection dates, and we also have to deal with missing data, incomplete observations, data entry errors etc. For now, let us just assume we have a data set only including symptom onset times and some hospitalisation times.\nLet’s look at a specific problem: we would like to estimate how long it takes for people to become hospitalised after becoming symptomatic. This might be an important delay to know about, for example when modelling and forecasting hospitalisations, or more generally for estimating required hospital capacity.\nTo do this we can use our data with stan to model the delay from symptom onset to hospitalisation, with the same assumption that it follows a lognormal distribution.\n\n\n\n\n\n\nTipTake a few minutes\n\n\n\nThis is where we apply the Bayesian inference concepts from the slides. We’re using Stan to estimate the parameters of a lognormal distribution (meanlog and sdlog) from our observed delay data. The nfidd_cmdstan_model() function loads a pre-written Stan model for this purpose.\n\n\n\nmod &lt;- nfidd_cmdstan_model(\"lognormal\")\nmod\n\n 1: // lognormal_model.stan\n 2: data {\n 3:   int&lt;lower=0&gt; n; // number of data points\n 4:   array[n] real y; // data\n 5: }\n 6: \n 7: parameters {\n 8:   real meanlog;\n 9:   real&lt;lower=0&gt; sdlog;\n10: }\n11: \n12: model {\n13:   meanlog ~ normal(0, 10);  // prior distribution\n14:   sdlog ~ normal(0, 10) T[0, ]; // prior distribution\n15: \n16:   y ~ lognormal(meanlog, sdlog);\n17: }\n\n\n\n\n\n\n\n\nTipTake a few minutes\n\n\n\nLook at the Stan model code above and try to understand its structure. What are the data, parameters, and model blocks doing? What priors are being used?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nThe lognormal Stan model has three main blocks:\n\nData block: Specifies we need n (number of observations) and y (the delay data)\nParameters block: We want to estimate meanlog and sdlog (the two parameters of the lognormal distribution)\nModel block:\n\nSets weakly informative normal priors for both parameters\nsdlog is constrained to be positive (standard deviations must be positive)\nThe likelihood states that our data y follows a lognormal distribution with these parameters\n\n\n\n\n\n\n\n\n\n\n\nNoteStan reference materials\n\n\n\n\n\nFor more information on the Stan based tools used in this session:\n\nStan reference guide - covers probability distributions (log-normal, gamma), Bayesian inference basics, and Stan model structure\nUsing our Stan models guide - practical guide to working with NFIDD’s Stan models, including nfidd_cmdstan_model() and nfidd_sample() functions used below\n\n\n\n\nLet’s estimate the onset-to-hospitalisation delay using the simulated data set we created above. Do we recover the parameters used for simulation?\nWe can do this by sampling from the model’s posterior distribution by feeding it our simulated data set.\n\n## Specify the time from onset to hospitalisation\ndf_onset_to_hosp &lt;- df |&gt;\n  mutate(onset_to_hosp = hosp_time - onset_time) |&gt; \n  # exclude infections that didn't result in hospitalisation\n  drop_na(onset_to_hosp)\n## Use the data to sample from the model posterior\nres &lt;- nfidd_sample(\n  mod,\n  data = list(\n    n = nrow(df_onset_to_hosp),\n    y = df_onset_to_hosp$onset_to_hosp\n  )\n)\n\n\n\n\n\n\n\nNoteWhat is sampling?\n\n\n\nStan has used Markov Chain Monte Carlo (MCMC) to generate samples from the posterior distribution of our parameters. The posterior distribution is the conditional probability distribution of the parameters given the observed data - p(parameters | data). This represents our updated beliefs about the parameter values after combining our prior knowledge with the evidence from the observed delays. Using Bayes’ theorem: posterior ∝ likelihood × prior. Each sample represents a plausible set of parameter values given our data and priors.\n\n\nTo find out more about the arguments that can be passed to cmdstanr::sample via the nfidd_sample() function we used here, you can use ?cmdstanr::sample. To see the estimates, we can use:\n\nres$summary() ## could also simply type `res`\n\n# A tibble: 3 × 10\n  variable      mean    median      sd     mad        q5      q95  rhat ess_bulk\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__     -1322.    -1322.    0.982   0.703   -1324.    -1.32e+3  1.00    1092.\n2 meanlog      1.73      1.73  0.0116  0.0115      1.71   1.75e+0  1.00    1835.\n3 sdlog        0.493     0.493 0.00781 0.00800     0.481  5.06e-1  1.00    1591.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\n\n\n\n\n\n\nTipTake a few minutes\n\n\n\nLook at the summary output above. What do the columns mean? What are the values of meanlog and sdlog? How do they compare to the true values we used in simulation (meanlog = 1.75, sdlog = 0.5)?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nThe summary shows key statistics for each parameter:\n\nMean: Average value across all posterior samples\nMedian: Middle value when samples are sorted\nSD: Standard deviation of the posterior samples (uncertainty)\nMAD: Median absolute deviation (robust measure of uncertainty)\nQ5, Q95: 5th and 95th percentiles (90% credible interval)\nRhat: Convergence diagnostic (should be close to 1.0)\nESS: Effective sample size (should be &gt; 400 for reliable estimates)\n\nThe meanlog and sdlog estimates should be close to the true simulation values (1.75 and 0.5), showing our model successfully recovered the parameters.\nFor more details on interpreting Stan output, see the Stan reference guide.\n\n\n\nThese estimates should look similar to what we used in the simulations.\nWe can also calculate the mean and standard deviation of the lognormal distribution from the estimated parameters:\n\nres |&gt;\n  summarise_lognormal()\n\n      mean             sd       \n Min.   :6.145   Min.   :3.038  \n 1st Qu.:6.316   1st Qu.:3.282  \n Median :6.369   Median :3.339  \n Mean   :6.372   Mean   :3.343  \n 3rd Qu.:6.424   3rd Qu.:3.400  \n Max.   :6.654   Max.   :3.621  \n\n\nThis shows us the mean and standard deviation of the onset-to-hospitalisation delay in days, which is often more interpretable than the lognormal parameters. We can plot the resulting probability density functions.\n\n\n\n\n\n\nNote\n\n\n\nEvery time we sample from the model posterior, we create 4000 iterations. That is a lot of data to plot, so instead, when we produce plots we’ll use a random sample of 100 iterations. You’ll see this throughout the course.\n\n\n\n## get shape and rate samples from the posterior\nres_df &lt;- res |&gt;\n  as_draws_df() |&gt;\n  filter(.draw %in% sample(.draw, 100)) # sample 100 draws\n\n## find the value (x) that includes 99% of the cumulative density\nmax_x &lt;- max(qlnorm(0.99, meanlog = res_df$meanlog, sdlog = res_df$sdlog))\n\n## calculate density on grid of x values\nx &lt;- seq(0, max_x, length.out = 100)\nres_df &lt;- res_df |&gt;\n  crossing(x = x) |&gt; ## add grid to data frame\n  mutate(density = dlnorm(x, meanlog, sdlog))\n\n## plot\nggplot(res_df, aes(x = x, y = density, group = .draw)) +\n  geom_line(alpha = 0.3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipTake a few minutes\n\n\n\nLook at the plot above. What do you see? Why are there multiple lines? What does this tell us about our uncertainty in the delay distribution?\n\n\n\n\n\n\n\n\nNoteSolutions\n\n\n\n\n\nThe plot shows multiple semi-transparent lines, each representing a different plausible delay distribution:\n\nMultiple lines: Each line represents one possible lognormal distribution based on a different sample from the posterior\nUncertainty visualisation: The spread of lines shows our uncertainty about the true delay distribution - we’re not certain about the exact shape\nLimited data: This uncertainty comes from having limited data - with more data, the lines would cluster more tightly together\nParameter trade-offs: Different combinations of meanlog and sdlog can produce very similar-looking distributions, making it hard to pin down exact parameter values\nRight-skewed shape: All curves show the characteristic right-skewed shape of lognormal distributions, appropriate for delays\nMost likely region: Where lines cluster together shows where we’re most confident about the distribution shape\n\nThis visualisation captures both our estimate of the delay distribution and our uncertainty about it.",
    "crumbs": [
      "Delay distributions"
    ]
  },
  {
    "objectID": "sessions/delay-distributions.html#challenge",
    "href": "sessions/delay-distributions.html#challenge",
    "title": "Delay distributions",
    "section": "Challenge",
    "text": "Challenge\n\nIn this session we were in the enviable situation of knowing which distribution was used to generate the data. With real data, of course, we don’t have this information available.\nImpact of sample size: Try subsampling your data to see how fewer observations affect your parameter estimates and uncertainty.\nSubsample to 100 observations and refit the model to compare results:\n\ndf_small &lt;- df_onset_to_hosp |&gt; slice_sample(n = 100)\n\nDifferent delay lengths: The updated add_delays() function now allows you to change the delay parameters.\nTry longer delays and see how this affects your parameter recovery:\n\ndf_long &lt;- add_delays(infection_times, \n                     hosp_params = list(meanlog = 2.5, sdlog = 0.5))\n\nDifferent variability of the delay: Try changing the variability of the delay to see how it affects uncertainty in estimates.\nMore variable delays (higher sdlog):\n\ndf_variable &lt;- add_delays(infection_times,\n                         hosp_params = list(meanlog = 1.75, sdlog = 1.0))\n\nLess variable delays (lower sdlog):\n\ndf_precise &lt;- add_delays(infection_times,\n                        hosp_params = list(meanlog = 1.75, sdlog = 0.2))\n\nDifferent type of distribution: Try using a completely different distribution for the delays.\nUse gamma distribution instead of lognormal:\n\ndf_gamma &lt;- add_delays(infection_times,\n                      hosp_fun = rgamma,\n                      hosp_params = list(shape = 2, rate = 0.3))\n\nDifferent fitted distribution: Try fitting a different distribution to the data. For example, use the gamma Stan model instead of lognormal:\n\nmod_gamma &lt;- nfidd_cmdstan_model(\"gamma\")\n\n\nSee the Using our Stan models guide for more available models\n\nTry using one of the methods in practice below to fit a distribution to the data.",
    "crumbs": [
      "Delay distributions"
    ]
  },
  {
    "objectID": "sessions/delay-distributions.html#methods-in-practice",
    "href": "sessions/delay-distributions.html#methods-in-practice",
    "title": "Delay distributions",
    "section": "Methods in practice",
    "text": "Methods in practice\n\nThere are many tools for fitting a distribution to a set of observed delays. For example, we could implement a regression approach in e.g. brms, or use a package such as fitdistrplus or the ‘naive’ model in epidist.",
    "crumbs": [
      "Delay distributions"
    ]
  },
  {
    "objectID": "sessions/delay-distributions.html#references",
    "href": "sessions/delay-distributions.html#references",
    "title": "Delay distributions",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Delay distributions"
    ]
  },
  {
    "objectID": "sessions/delay-distributions.html#footnotes",
    "href": "sessions/delay-distributions.html#footnotes",
    "title": "Delay distributions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMuch like Alice through the Looking Glass, it often seems like we are running to stay in the same place!↩︎",
    "crumbs": [
      "Delay distributions"
    ]
  },
  {
    "objectID": "sessions/examples-of-available-tools.html",
    "href": "sessions/examples-of-available-tools.html",
    "title": "Examples of available tools",
    "section": "",
    "text": "In the course so far we introduced key concepts and methods in nowcasting and forecasting of infectious disease dynamics, using a combination of R and (mostly) stan. Being able to understand these concepts and implement them in stan comes with the ability to create and adapt models that are tailor made to any given situation and specific characteristics of any given data set. At the same time, there is value in using and contributing to open-source tools that implement some or all of the methods we have encountered in the course. These tools have varying levels of flexibility to deal with different kinds of epidemiological problems and data. Whilst they will never have the flexibility of completely custom approaches, using and contributing to them avoids duplication, improves the chances of finding errors, and helps discussing and ultimately enforcing best practice.\n\n\n\nExamples of tools\n\n\n\n\nThe aim of this session is to try out some of the tools that are available that use some of the ideas we have discussed in this course.\n\n\n\n\n\n\nNoteSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/examples-of-available-tools.qmd.\n\n\n\nIn order to install the tools mentioned below, refer to the installation instructions supplied with each of the packages.",
    "crumbs": [
      "Examples of available tools"
    ]
  },
  {
    "objectID": "sessions/examples-of-available-tools.html#slides",
    "href": "sessions/examples-of-available-tools.html#slides",
    "title": "Examples of available tools",
    "section": "",
    "text": "Examples of tools",
    "crumbs": [
      "Examples of available tools"
    ]
  },
  {
    "objectID": "sessions/examples-of-available-tools.html#objective",
    "href": "sessions/examples-of-available-tools.html#objective",
    "title": "Examples of available tools",
    "section": "",
    "text": "The aim of this session is to try out some of the tools that are available that use some of the ideas we have discussed in this course.\n\n\n\n\n\n\nNoteSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/examples-of-available-tools.qmd.\n\n\n\nIn order to install the tools mentioned below, refer to the installation instructions supplied with each of the packages.",
    "crumbs": [
      "Examples of available tools"
    ]
  },
  {
    "objectID": "sessions/examples-of-available-tools.html#source-file",
    "href": "sessions/examples-of-available-tools.html#source-file",
    "title": "Examples of available tools",
    "section": "",
    "text": "The source file of this session is located at sessions/examples-of-available-tools.qmd.",
    "crumbs": [
      "Examples of available tools"
    ]
  },
  {
    "objectID": "sessions/examples-of-available-tools.html#installing-tools",
    "href": "sessions/examples-of-available-tools.html#installing-tools",
    "title": "Examples of available tools",
    "section": "",
    "text": "In order to install the tools mentioned below, refer to the installation instructions supplied with each of the packages.",
    "crumbs": [
      "Examples of available tools"
    ]
  },
  {
    "objectID": "sessions/examples-of-available-tools.html#epiestim",
    "href": "sessions/examples-of-available-tools.html#epiestim",
    "title": "Examples of available tools",
    "section": "EpiEstim",
    "text": "EpiEstim\nEpiEstim implements the renewal equation on a time series of infections in a Bayesian framework, i.e. the model in estimate-r.stan in the session on \\(R_t\\) estimation. In combination with the projections package in can be used for forecasting.\nThe EpiEstim vignette is a good starting point for a walkthrough of \\(R_t\\) estimation and forecasting.",
    "crumbs": [
      "Examples of available tools"
    ]
  },
  {
    "objectID": "sessions/examples-of-available-tools.html#epinow2",
    "href": "sessions/examples-of-available-tools.html#epinow2",
    "title": "Examples of available tools",
    "section": "EpiNow2",
    "text": "EpiNow2\nEpiNow2 implements the renewal equation on a time series of delayed outcomes including nowcasts with a known reporting delay distribution, as well as forecasts using a Gaussian Process or random walk model, i.e. the models introduced here except the joint inference of nowcasts and delays and/or reproduction numbers.\nThe example of nowcasting, \\(R_t\\) estimation and forecasting with EpiNow2 is a good place to find out more about how to use EpiNow2 with linelist data.",
    "crumbs": [
      "Examples of available tools"
    ]
  },
  {
    "objectID": "sessions/examples-of-available-tools.html#epinowcast",
    "href": "sessions/examples-of-available-tools.html#epinowcast",
    "title": "Examples of available tools",
    "section": "Epinowcast",
    "text": "Epinowcast\nEpinowcast was created to replace EpiNow2. It implements all the models discussed in the course with a frontend to work with line lists, data by reference and report date or cumulative data snapshots. Whilst this model can also be used for forecasts (as we have seen) this does not currently have a user-facing interface in the package.\nThe vignette on estimating the effective reproduction number in real-time for a single timeseries with reporting delays is a good example of joint nowcasting and reproduction number estimation from an aggregated time series of reference (outcome) and reporting counts. This example does not start from a line list but does contain information about using the package for transforming between line list and appropriately aggregated data.",
    "crumbs": [
      "Examples of available tools"
    ]
  },
  {
    "objectID": "sessions/examples-of-available-tools.html#going-further",
    "href": "sessions/examples-of-available-tools.html#going-further",
    "title": "Examples of available tools",
    "section": "Going further",
    "text": "Going further\nThere are many other R packages that can be used for \\(R_t\\) estimation, nowcasting and forecasting. We invite readers to suggest further additions on our discussion board.",
    "crumbs": [
      "Examples of available tools"
    ]
  },
  {
    "objectID": "sessions/forecast-evaluation.html",
    "href": "sessions/forecast-evaluation.html",
    "title": "Forecast evaluation",
    "section": "",
    "text": "So far we have focused on visualising forecasts, including confronting them with events that were observed after the forecast was made. Besides visualising the forecasts, we can also summarise performance quantitatively. In this session you will get to know several ways of assessing different aspects of forecast performance.\n\n\n\nForecast evaluation\n\n\n\n\nThe aim of this session is to introduce the concept of forecasting, using a simple model, and forecasting evaluation.\n\n\n\n\n\n\nNoteSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/forecasting-evaluation.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and package for data wrangling, ggplot2 library for plotting and the scoringutils package for evaluating forecasts.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"scoringutils\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(12345)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Forecast evaluation"
    ]
  },
  {
    "objectID": "sessions/forecast-evaluation.html#slides",
    "href": "sessions/forecast-evaluation.html#slides",
    "title": "Forecast evaluation",
    "section": "",
    "text": "Forecast evaluation",
    "crumbs": [
      "Forecast evaluation"
    ]
  },
  {
    "objectID": "sessions/forecast-evaluation.html#objectives",
    "href": "sessions/forecast-evaluation.html#objectives",
    "title": "Forecast evaluation",
    "section": "",
    "text": "The aim of this session is to introduce the concept of forecasting, using a simple model, and forecasting evaluation.\n\n\n\n\n\n\nNoteSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/forecasting-evaluation.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and package for data wrangling, ggplot2 library for plotting and the scoringutils package for evaluating forecasts.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"scoringutils\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(12345)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Forecast evaluation"
    ]
  },
  {
    "objectID": "sessions/forecast-evaluation.html#source-file",
    "href": "sessions/forecast-evaluation.html#source-file",
    "title": "Forecast evaluation",
    "section": "",
    "text": "The source file of this session is located at sessions/forecasting-evaluation.qmd.",
    "crumbs": [
      "Forecast evaluation"
    ]
  },
  {
    "objectID": "sessions/forecast-evaluation.html#libraries-used",
    "href": "sessions/forecast-evaluation.html#libraries-used",
    "title": "Forecast evaluation",
    "section": "",
    "text": "In this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and package for data wrangling, ggplot2 library for plotting and the scoringutils package for evaluating forecasts.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"scoringutils\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.",
    "crumbs": [
      "Forecast evaluation"
    ]
  },
  {
    "objectID": "sessions/forecast-evaluation.html#initialisation",
    "href": "sessions/forecast-evaluation.html#initialisation",
    "title": "Forecast evaluation",
    "section": "",
    "text": "We set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(12345)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Forecast evaluation"
    ]
  },
  {
    "objectID": "sessions/forecast-evaluation.html#the-forecasting-paradigm",
    "href": "sessions/forecast-evaluation.html#the-forecasting-paradigm",
    "title": "Forecast evaluation",
    "section": "The forecasting paradigm",
    "text": "The forecasting paradigm\nThe general principle underlying forecast evaluation is to maximise sharpness subject to calibration (Gneiting and Raftery 2007). This means that statements about the future should be correct (calibration) and should aim to have narrow uncertainty (sharpness).",
    "crumbs": [
      "Forecast evaluation"
    ]
  },
  {
    "objectID": "sessions/forecast-evaluation.html#scoring-your-forecast",
    "href": "sessions/forecast-evaluation.html#scoring-your-forecast",
    "title": "Forecast evaluation",
    "section": "Scoring your forecast",
    "text": "Scoring your forecast\nWe now summarise performance quantitatively by using scoring metrics. Whilst some of these metrics are more useful for comparing models, many can be also be useful for understanding the performance of a single model.\n\n\n\n\n\n\nTip\n\n\n\nIn this session, we’ll use “proper” scoring rules: these are scoring rules that make sure no model can get better scores than the true model, i.e. the model used to generate the data. Of course we usually don’t know this (as we don’t know the “true model” for real-world data) but proper scoring rules incentivise forecasters to make their best attempt at reproducing its behaviour. For a comprehensive text on proper scoring rules and their mathematical properties, we recommend the classic paper by Gneiting and Raftery (2007).\n\n\nWe will use the {scoringutils} package to calculate these metrics. Our first step is to convert our forecasts into a format that the {scoringutils} package can use. We will use as_forecast_sample() to do this:\n\nsc_forecasts &lt;- rw_forecasts |&gt;\n  left_join(onset_df, by = \"day\") |&gt;\n  filter(!is.na(.value)) |&gt;\n  as_forecast_sample(\n    forecast_unit = c(\n      \"origin_day\", \"horizon\", \"model\"\n    ),\n    observed = \"onsets\",\n    predicted = \".value\",\n    sample_id = \".draw\"\n  )\nsc_forecasts\n\nForecast type: sample\n\n\nForecast unit:\n\n\norigin_day, horizon, and model\n\n\n\n        sample_id predicted observed origin_day horizon       model\n            &lt;int&gt;     &lt;num&gt;    &lt;int&gt;      &lt;num&gt;   &lt;int&gt;      &lt;char&gt;\n     1:         1         9        2         22       1 Random walk\n     2:         2         5        2         22       1 Random walk\n     3:         3         5        2         22       1 Random walk\n     4:         4         3        2         22       1 Random walk\n     5:         5         5        2         22       1 Random walk\n    ---                                                            \n223996:       996         1        3        127      14 Random walk\n223997:       997         1        3        127      14 Random walk\n223998:       998         1        3        127      14 Random walk\n223999:       999         0        3        127      14 Random walk\n224000:      1000         1        3        127      14 Random walk\n\n\nAs you can see this has created a forecast object which has a print method that summarises the forecasts.\n\n\n\n\n\n\nTipTake 2 minutes\n\n\n\nWhat important information is in the forecast object?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nThe forecast unit which is the origin day, horizon, and model\nThe type of forecast which is a sample forecast\n\n\n\n\nEverything seems to be in order. We can now use the scoringutils package to calculate some metrics. We will use the default sample metrics (as our forecasts are in sample format) and score our forecasts.\n\nsc_scores &lt;- sc_forecasts |&gt;\n  score()\n\nsc_scores\n\n     origin_day horizon       model   bias        dss     crps overprediction\n          &lt;num&gt;   &lt;int&gt;      &lt;char&gt;  &lt;num&gt;      &lt;num&gt;    &lt;num&gt;          &lt;num&gt;\n  1:         22       1 Random walk  0.477  2.0793761 0.800163          0.302\n  2:         22       2 Random walk  0.611  2.5778676 1.154219          0.584\n  3:         22       3 Random walk  0.066  2.2577877 0.656429          0.000\n  4:         22       4 Random walk  0.482  2.8847541 1.203126          0.460\n  5:         22       5 Random walk  0.631  3.4041524 1.686931          0.888\n ---                                                                         \n220:        127      10 Random walk  0.216  0.9035654 0.298445          0.000\n221:        127      11 Random walk -0.883  3.9779852 2.031831          0.000\n222:        127      12 Random walk -0.994 24.3459996 6.051919          0.000\n223:        127      13 Random walk  0.617  1.4141084 0.496038          0.234\n224:        127      14 Random walk -0.846  2.5788329 1.542148          0.000\n     underprediction dispersion log_score    mad ae_median   se_mean\n               &lt;num&gt;      &lt;num&gt;     &lt;num&gt;  &lt;num&gt;     &lt;num&gt;     &lt;num&gt;\n  1:           0.000   0.498163 1.7674254 1.4826         1  2.461761\n  2:           0.000   0.570219 1.9905507 2.9652         2  4.721929\n  3:           0.000   0.656429 1.9607737 2.9652         0  0.546121\n  4:           0.000   0.743126 2.1527223 2.9652         2  5.466244\n  5:           0.000   0.798931 2.3370219 2.9652         3 10.686361\n ---                                                                \n220:           0.000   0.298445 0.4056026 1.4826         0  0.351649\n221:           1.764   0.267831 3.2071257 1.4826         3  6.817321\n222:           5.802   0.249919 5.4246093 1.4826         7 45.468049\n223:           0.000   0.262038 0.8987562 1.4826         1  1.411344\n224:           1.274   0.268148 2.7098403 1.4826         2  3.802500\n\n\n\n\n\n\n\n\nNoteLearning more about the output of score()\n\n\n\n\n\nSee the documentation for ?get_metrics.forecast_sample for information on the default metrics for forecasts that are represented as samples (in our case the samples generated by the stan model).\n\n\n\n\nAt a glance\nBefore we look in detail at the scores, we can use summarise_scores to get a quick overview of the scores (by default calculating a mean across all forecasts). Don’t worry if you don’t understand all the scores yet, we will go some of them in more detail in the next section and you can find more information in the {scoringutils} documentation.\n\nsc_scores |&gt;\n  summarise_scores(by = \"model\")\n\n         model      bias      dss     crps overprediction underprediction\n        &lt;char&gt;     &lt;num&gt;    &lt;num&gt;    &lt;num&gt;          &lt;num&gt;           &lt;num&gt;\n1: Random walk 0.2106518 6.295836 13.32493       7.828018       0.8525804\n   dispersion log_score      mad ae_median  se_mean\n        &lt;num&gt;     &lt;num&gt;    &lt;num&gt;     &lt;num&gt;    &lt;num&gt;\n1:   4.644333  3.927755 19.29697  18.04464 1606.823\n\n\n\n\n\n\n\n\nTipTake 2 minutes\n\n\n\nBefore we look in detail at the scores, what do you think the scores are telling you?\n\n\n\n\nContinuous ranked probability score\n\nWhat is the Continuous Ranked Probability Score (CRPS)?\nFor point forecasts (single value predictions), forecast accuracy is commonly measured using the Mean Absolute Error (MAE), which calculates the average (mean) absolute difference between predicted and observed values. For probabilistic forecasts, where the forecast is a distribution rather than a single point estimate (i.e. like ours), we can use the Continuous Ranked Probability Score (CRPS). The CRPS is a proper scoring rule that generalises MAE to probabilistic forecasts. Note that for deterministic forecasts, CRPS reduces to MAE.\nThe CRPS can be thought about as the combination of two key aspects of forecasting: 1. The accuracy of the forecast in terms of how close the predicted values are to the observed value. 2. The confidence of the forecast in terms of the spread of the predicted values.\nBy balancing these two aspects, the CRPS provides a comprehensive measure of the quality of probabilistic forecasts.\n\n\n\n\n\n\nTipKey things to note about the CRPS\n\n\n\n\nSmall values are better\nAs it is an absolute scoring rule it can be difficult to use to compare forecasts across scales.\n\n\n\n\n\n\n\n\n\nTipMathematical Definition (optional)\n\n\n\n\n\nThe CRPS for a predictive distribution characterised by a cumulative distribution function \\(F\\) and observed value \\(x\\) is calculated as\n\\[\nCRPS(F, x) = \\int_{-\\infty}^{+\\infty} \\left( F(y) - \\mathbb{1} ({\\y \\geq x})^2 \\right).\n\\]\nFor distributions with a finite first moment (a mean exists and it is finite), the CRPS can be expressed as:\n\\[\nCRPS(F, y) = \\mathbb{E}_{X \\sim F}[|X - y|] - \\frac{1}{2} \\mathbb{E}_{X, X' \\sim F}[|X - X'|]\n\\]\nwhere \\(X\\) and \\(X'\\) are independent random variables sampled from the distribution \\(F\\). To calculate this we simply replace \\(X\\) and \\(X'\\) by samples from our posterior distribution and sum over all possible combinations.\n\n\n\nWhilst the CRPS is a very useful metric it can be difficult to interpret in isolation. It is often useful to compare the CRPS of different models or to compare the CRPS of the same model under different conditions. For example, lets compare the CRPS across different forecast horizons.\n\nsc_scores |&gt;\n  summarise_scores(by = \"horizon\") |&gt;\n  ggplot(aes(x = horizon, y = crps)) +\n  geom_point() +\n  labs(title = \"CRPS by daily forecast horizon\",\n       subtitle = \"Summarised across all forecasts\")\n\n\n\n\n\n\n\n\nand at different time points.\n\nsc_scores |&gt;\n  summarise_scores(by = \"origin_day\") |&gt;\n  ggplot(aes(x = origin_day, y = crps)) +\n  geom_point() +\n  labs(title = \"CRPS by forecast start date\",\n       subtitle = \"Summarised across all forecasts\", x = \"forecast date\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipTake 5 minutes\n\n\n\nHow do the CRPS scores change based on forecast date? How do the CRPS scores change with forecast horizon? What does this tell you about the model?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nThe CRPS scores increase for forecast dates where incidence is higher.\nThe CRPS scores increase with forecast horizon.\nAs the CRPS is an absolute measure it is hard to immediately know if the CRPS increasing with forecast date indicates that the model is performing worse.\nHowever, the CRPS increasing with forecast horizon is a sign that the model is struggling to capture the longer term dynamics of the epidemic.\n\n\n\n\n\n\n\nPIT histograms\nAs well as the CRPS we can also look at the calibration and bias of the model. Calibration is the agreement between the forecast probabilities and the observed frequencies. Bias is a measure of how likely the model is to over or under predict the observed values.\nThere are many ways to assess calibration and bias but one common way is to use a probability integral transform (PIT) histogram. This is a histogram of the cumulative distribution of function of a forecast evaluated at the observed value.\n\n\n\n\n\n\nTipInterpreting the PIT histogram\n\n\n\n\nIdeally PIT histograms should be uniform.\nIf is a U shape then the model is overconfident and if it is an inverted U shape then the model is underconfident.\nIf it is skewed then the model is biased towards the direction of the skew.\n\n\n\n\n\n\n\n\n\nTipMathematical Definition (optional)\n\n\n\n\n\n\nContinuous Case\nFor a continuous random variable \\(X\\) with cumulative distribution function (CDF) \\(F_X\\), the PIT is defined as:\n\\[\nY = F_X(X)\n\\]\nwhere \\(Y\\) is uniformly distributed on \\([0, 1]\\).\n\n\nInteger Case\nWhen dealing with integer forecasts, the standard PIT does not yield a uniform distribution even if the forecasts are perfectly calibrated (Czado, Gneiting, and Held 2009). To remedy this, there are different solutions (the get_pit_histogram() function used below uses the non-randomised version by default):\n\nRandomised PIT\nOne way to use integer count is a randomised version of the PIT. For an integer-valued random variable \\(X\\) with CDF \\(F_X\\), the randomised PIT is defined as:\n\\[\nU = F_X(k) + v \\cdot (F_X(k) - F_X(k-1))\n\\]\nwhere:\n\n\\(k\\) is the observed integer value,\n\\(F_X(k)\\) is the CDF evaluated at \\(k\\),\n\\(v\\) is a random variable uniformly distributed on \\([0, 1]\\).\n\nThis transformation ensures that \\(U\\) is uniformly distributed on \\([0, 1]\\) if the predictive distribution \\(F_X\\) is correctly specified.\n\n\nNon-randomised PIT\nA second option is to plot a slightly different variable:\n\\[\nG(u) = \\begin{cases}\n0, & u \\leq F_X(k-1), \\\\\n(u - F_X(k-1))/(F_X(k) - F_X(k-1)), & F_X(k-1) &lt; u &lt; F_X(k), \\\\\n1, & u \\geq F_X(k),\n\\end{cases}\n\\]\nif \\(k \\geq 1\\) and\n\\[\nG(u) = \\begin{cases}\nu/F_X(0), & u \\leq F_X(0), \\\\\n1, & u \\geq F_X(0),\n\\end{cases}\n\\]\nif \\(k=0\\) where as before\n\n\\(k\\) is the observed integer value,\n\\(F_X(k)\\) is the CDF evaluated at \\(k\\).\n\nAgain, the transformed G(u) is uniformly distributed on \\([0, 1]\\) if the predictive distribution \\(F_X\\) is correctly specified.\n\n\n\n\n\nLet’s first look at the overall PIT histogram.\n\nsc_forecasts |&gt;\n  get_pit_histogram() |&gt;\n  ggplot(aes(x = mid, y = density)) +\n  geom_col() +\n  labs(title = \"PIT histogram\", x = \"Predicted quantiles\", y = \"Density\")\n\n\n\n\n\n\n\n\nAs before lets look at the PIT histogram by forecast horizon. To save space we will group horizons into a few days each:\n\nsc_forecasts |&gt;\n  mutate(group_horizon = case_when(\n    horizon &lt;= 3 ~ \"1-3\",\n    horizon &lt;= 7 ~ \"4-7\",\n    horizon &lt;= 14 ~ \"8-14\"\n  )) |&gt;\n  get_pit_histogram(by = \"group_horizon\") |&gt;\n  ggplot(aes(x = mid, y = density)) +\n  geom_col() +\n  facet_wrap(~group_horizon) +\n  labs(title = \"PIT by forecast horizon (days)\", x = \"Predicted quantiles\")\n\n\n\n\n\n\n\n\nand then for different forecast dates.\n\nsc_forecasts |&gt;\n  get_pit_histogram(by = \"origin_day\") |&gt;\n  ggplot(aes(x = mid, y = density)) +\n  geom_col() +\n  facet_wrap(~origin_day) +\n  labs(title = \"PIT by forecast date\", x = \"Predicted quantiles\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipTake 5 minutes\n\n\n\nWhat do you think of the PIT histograms? Do they look well calibrated? Do they look biased?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nIt looks like the model is biased towards overpredicting and that this bias gets worse at longer forecast horizons.\nLooking over forecast dates it looks like much of this bias is coming from near the outbreak peak where the model is consistently overpredicting but the model is also over predicting at other times.",
    "crumbs": [
      "Forecast evaluation"
    ]
  },
  {
    "objectID": "sessions/forecast-evaluation.html#scoring-on-the-log-scale",
    "href": "sessions/forecast-evaluation.html#scoring-on-the-log-scale",
    "title": "Forecast evaluation",
    "section": "Scoring on the log scale",
    "text": "Scoring on the log scale\nWe can also score on the logarithmic scale. This can be useful if we are interested in the relative performance of the model at different scales of the data, for example if we are interested in the model’s performance at capturing the exponential growth phase of the epidemic. In some sense scoring in this way can be an approximation of scoring the effective reproduction number estimates. Doing this directly can be difficult as the effective reproduction number is a latent variable and so we cannot directly score it.\nWe again use scoringutils but first transform both the forecasts and observations to the log scale.\n\nlog_sc_forecasts &lt;- sc_forecasts |&gt;\n  transform_forecasts(\n    fun = log_shift,\n    offset = 1,\n    append = FALSE\n  )\n\nlog_scores &lt;- log_sc_forecasts |&gt;\n  score()\n\nFor more on scoring on the log scale see this paper on scoring forecasts on transformed scales.\n\nAt a glance\n\nlog_scores |&gt;\n  summarise_scores(by = \"model\")\n\n         model      bias        dss      crps overprediction underprediction\n        &lt;char&gt;     &lt;num&gt;      &lt;num&gt;     &lt;num&gt;          &lt;num&gt;           &lt;num&gt;\n1: Random walk 0.1698929 -0.6932839 0.2385248      0.1172615      0.02994692\n   dispersion log_score       mad ae_median  se_mean\n        &lt;num&gt;     &lt;num&gt;     &lt;num&gt;     &lt;num&gt;    &lt;num&gt;\n1: 0.09131641 0.5302044 0.3940836 0.3242059 0.196584\n\n\n\n\n\n\n\n\nTipTake 2 minutes\n\n\n\nBefore we look in detail at the scores, what do you think the scores are telling you? How do you think they will differ from the scores on the natural scale?\n\n\n\n\nCRPS\n\nlog_scores |&gt;\n  summarise_scores(by = \"horizon\") |&gt;\n  ggplot(aes(x = horizon, y = crps)) +\n  geom_point() +\n  labs(title = \"CRPS by daily forecast horizon, scored on the log scale\")\n\n\n\n\n\n\n\n\nand across different forecast dates\n\nlog_scores |&gt;\n  summarise_scores(by = \"origin_day\") |&gt;\n  ggplot(aes(x = origin_day, y = crps)) +\n  geom_point() +\n  labs(title = \"CRPS by forecast date, scored on the log scale\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipTake 5 minutes\n\n\n\nHow do the CRPS scores change based on forecast date? How do the CRPS scores change with forecast horizon? What does this tell you about the model?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nAs for the natural scale CRPS scores increase with forecast horizon but now the increase appears to be linear vs exponential.\nThere has been a reduction in the CRPS scores for forecast dates near the outbreak peak compared to other forecast dates but this is still the period where the model is performing worst.",
    "crumbs": [
      "Forecast evaluation"
    ]
  },
  {
    "objectID": "sessions/forecast-evaluation.html#challenge",
    "href": "sessions/forecast-evaluation.html#challenge",
    "title": "Forecast evaluation",
    "section": "Challenge",
    "text": "Challenge\n\nIn which other ways could we summarise the performance of the forecasts?\nWhat other metrics could we use?\nThere is no one-size-fits-all approach to forecast evaluation, often you will need to use a combination of metrics to understand the performance of your model and typically the metrics you use will depend on the context of the forecast. What attributes of the forecast are most important to you?\nOne useful way to think about evaluating forecasts is to consider exploring the scores as a data analysis in its own right. For example, you could look at how the scores change over time, how they change with different forecast horizons, or how they change with different models. This can be a useful way to understand the strengths and weaknesses of your model. Explore some of these aspects using the scores from this session.",
    "crumbs": [
      "Forecast evaluation"
    ]
  },
  {
    "objectID": "sessions/forecast-evaluation.html#methods-in-practice",
    "href": "sessions/forecast-evaluation.html#methods-in-practice",
    "title": "Forecast evaluation",
    "section": "Methods in practice",
    "text": "Methods in practice\n\nThere are many other metrics that can be used to evaluate forecasts. The documentation for the {scoringutils} package has a good overview of these metrics and how to use them.",
    "crumbs": [
      "Forecast evaluation"
    ]
  },
  {
    "objectID": "sessions/forecasting-models.html",
    "href": "sessions/forecasting-models.html",
    "title": "Forecasting models",
    "section": "",
    "text": "We can classify models along a spectrum by how much they include an understanding of underlying processes, or mechanisms; or whether they emphasise drawing from the data using a statistical approach. In this session, we’ll start with the renewal model that we’ve been using and explore adding both more mechanistic structure and then more statistical structure to the model. We’ll again evaluate these models to see what effect these different approaches might have.\n\n\n\nIntroduction to the spectrum of forecasting models\n\n\n\n\nThe aim of this session is to introduce some common forecasting models and to evaluate them.\n\n\n\n\n\n\nCaution\n\n\n\nNone of the models introduced in this section are designed for real-world use!\n\n\n\n\n\n\n\n\nNoteSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/forecasting-models.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and tidyr packages for data wrangling, ggplot2 library for plotting, the tidybayes package for extracting results of the inference and the scoringutils package for evaluating forecasts.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\nlibrary(\"scoringutils\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Forecasting models"
    ]
  },
  {
    "objectID": "sessions/forecasting-models.html#slides",
    "href": "sessions/forecasting-models.html#slides",
    "title": "Forecasting models",
    "section": "",
    "text": "Introduction to the spectrum of forecasting models",
    "crumbs": [
      "Forecasting models"
    ]
  },
  {
    "objectID": "sessions/forecasting-models.html#objectives",
    "href": "sessions/forecasting-models.html#objectives",
    "title": "Forecasting models",
    "section": "",
    "text": "The aim of this session is to introduce some common forecasting models and to evaluate them.\n\n\n\n\n\n\nCaution\n\n\n\nNone of the models introduced in this section are designed for real-world use!\n\n\n\n\n\n\n\n\nNoteSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/forecasting-models.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and tidyr packages for data wrangling, ggplot2 library for plotting, the tidybayes package for extracting results of the inference and the scoringutils package for evaluating forecasts.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\nlibrary(\"scoringutils\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Forecasting models"
    ]
  },
  {
    "objectID": "sessions/forecasting-models.html#source-file",
    "href": "sessions/forecasting-models.html#source-file",
    "title": "Forecasting models",
    "section": "",
    "text": "The source file of this session is located at sessions/forecasting-models.qmd.",
    "crumbs": [
      "Forecasting models"
    ]
  },
  {
    "objectID": "sessions/forecasting-models.html#libraries-used",
    "href": "sessions/forecasting-models.html#libraries-used",
    "title": "Forecasting models",
    "section": "",
    "text": "In this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and tidyr packages for data wrangling, ggplot2 library for plotting, the tidybayes package for extracting results of the inference and the scoringutils package for evaluating forecasts.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\nlibrary(\"scoringutils\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.",
    "crumbs": [
      "Forecasting models"
    ]
  },
  {
    "objectID": "sessions/forecasting-models.html#initialisation",
    "href": "sessions/forecasting-models.html#initialisation",
    "title": "Forecasting models",
    "section": "",
    "text": "We set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Forecasting models"
    ]
  },
  {
    "objectID": "sessions/forecasting-models.html#adding-more-mechanistic-structure-to-the-renewal-model",
    "href": "sessions/forecasting-models.html#adding-more-mechanistic-structure-to-the-renewal-model",
    "title": "Forecasting models",
    "section": "Adding more mechanistic structure to the renewal model",
    "text": "Adding more mechanistic structure to the renewal model\nOne way to potentially improve the renewal model is to add more mechanistic structure. In the forecast evaluation session, we saw that the renewal model was making unbiased forecasts when the reproduction number was constant but that it overestimated the number of cases when the reproduction number was reducing due to susceptible depletion.\n\n\n\n\n\n\nWarning\n\n\n\nThis is slightly cheating as we know the future of this outbreak and so can make a model to match. This is easy to do and important to watch for if wanting to make generalisable methods.\n\n\nThis suggests that we should add a term to the renewal model which captures the depletion of susceptibles. One way to do this is to add a term which is proportional to the number of susceptibles in the population. This is the idea behind the SIR model which is a simple compartmental model of infectious disease transmission. If we assume that susceptible depletion is the only mechanism which is causing the reproduction number to change, we can write the reproduction model as:\n\\[\nR_t = \\frac{S_{t-1}}{N} R_0\n\\]\n\n\n\n\n\n\nNote\n\n\n\nThis approximates susceptible depletion as a linear function of the number of susceptibles in the population. This is a simplification but it is a good starting point.\n\n\n\n\n\n\n\n\nNoteWhat behaviour would we expect from this model?\n\n\n\n\n\n\nn &lt;- 100\nN &lt;- 1000\nR0 &lt;- 1.5\nS &lt;- rep(NA, n)\nS[1] &lt;- N\nRt &lt;- rep(NA, n) ## reproduction number\nRt[1] &lt;- R0\nI &lt;- rep(NA, n)\nI[1] &lt;- 1\nfor (i in 2:n) {\n  Rt[i] &lt;- (S[i-1]) / N * R0\n  I[i] &lt;- I[i-1] * Rt[i]\n  S[i] &lt;- S[i-1] - I[i]\n}\n\ndata &lt;- tibble(t = 1:n, Rt = Rt)\n\nggplot(data, aes(x = t, y = Rt)) +\n  geom_line() +\n  labs(title = \"Simulated data from an SIR model\",\n       x = \"Time\",\n       y = \"Rt\")\n\n\n\n\n\n\n\n\n\n\n\nThe key assumptions we are making here are:\n\nThe population is constant and we roughly know the size of the population.\nThe reproduction number only changes due to susceptible depletion\nThe number of new cases at each time is proportional to the number of susceptibles in the population.\n\nWe’ve coded this up as a stan model in stan/mechanistic-r.stan. See stan/functions/pop_bounded_renewal.stan for the function which calculates the reproduction number. Let’s load the model:\n\nmech_mod &lt;- nfidd_cmdstan_model(\"mechanistic-r\")\nmech_mod\n\n 1: functions {\n 2:   #include \"functions/convolve_with_delay.stan\"\n 3:   #include \"functions/pop_bounded_renewal.stan\"\n 4: }\n 5: \n 6: data {\n 7:   int n;                // number of days\n 8:   int I0;              // number initially infected\n 9:   array[n] int obs;     // observed symptom onsets\n10:   int gen_time_max;     // maximum generation time\n11:   array[gen_time_max] real gen_time_pmf;  // pmf of generation time distribution\n12:   int&lt;lower = 1&gt; ip_max; // max incubation period\n13:   array[ip_max + 1] real ip_pmf;\n14:   int h;                // number of days to forecast\n15:   array[2] real N_prior;      // prior for total population\n16: }\n17: \n18: transformed data {\n19:    int m = n + h;\n20: }\n21: \n22: parameters {\n23:   real&lt;lower = 0&gt; R;    // initial reproduction number\n24:   real&lt;lower = 0&gt; N;   // total population\n25: }\n26: \n27: transformed parameters {\n28:   array[m] real infections = pop_bounded_renewal(I0, R, gen_time_pmf, N, m);\n29:   array[m] real onsets = convolve_with_delay(infections, ip_pmf);\n30: }\n31: \n32: model {\n33:   // priors\n34:   R ~ normal(1, 0.5) T[0,];\n35:   N ~ normal(N_prior[1], N_prior[2]) T[0,];\n36:   obs ~ poisson(onsets[1:n]);\n37: }\n38: \n39: generated quantities {\n40:   array[h] real forecast;\n41:   if (h &gt; 0) {\n42:     for (i in 1:h) {\n43:       forecast[i] = poisson_rng(onsets[n + i]);\n44:     }\n45:   }\n46: }",
    "crumbs": [
      "Forecasting models"
    ]
  },
  {
    "objectID": "sessions/forecasting-models.html#adding-more-statistical-structure-to-the-renewal-model",
    "href": "sessions/forecasting-models.html#adding-more-statistical-structure-to-the-renewal-model",
    "title": "Forecasting models",
    "section": "Adding more statistical structure to the renewal model",
    "text": "Adding more statistical structure to the renewal model\nAdding more mechanistic structure is not always possible and, if we don’t specify mechanisms correctly, might make forecasts worse. Rather than adding more mechanistic structure to the renewal model, we could add more statistical structure with the aim of improving performance. Before we do this, we need to think about what we want from a forecasting model. As we identified above, we want a model which is unbiased and which has good short-term forecasting properties. We know that we want it to be able to adapt to trends in the reproduction number and that we want it to be able to capture the noise in the data. A statistical term that can be used to describe a time series with a trend is saying that the time series is non-stationary. More specifically, a stationary time series is defined as one whose statistical properties, such as mean and variance, do not change over time. In infectious disease epidemiology, this would only be expected for endemic diseases without external seasonal influence.\nThe random walk model we used in the forecasting concepts session is a special case of a more general class of models called autoregressive (AR) models. AR models are a class of models which predict the next value in a time series as a linear combination of the previous values in the time series. The random walk model is specifically a special case of an AR(1) model where the next value in the time series is predicted as the previous value, multiplied by a value between 1 and -1 , plus some noise. This becomes a random walk when the multipled value is 0.\nFor the log-transformed reproduction number (\\(log(R_t)\\)), the model is:\n\\[\nlog(R_t) = \\phi log(R_{t-1}) + \\epsilon_t\n\\]\nwhere \\(\\epsilon_t\\) is a normally distributed error term with mean 0 and standard deviation \\(\\sigma\\) and \\(\\phi\\) is a parameter between -1 and 1. If we restrict \\(\\phi\\) to be between 0 and 1, we get a model which is biased towards a reproduction number of 1 but which can still capture trends in the data that decay over time.\n\n\n\n\n\n\nNoteWhat behaviour would we expect from this model?\n\n\n\n\n\n\nn &lt;- 100\nphi &lt;- 0.4\nsigma &lt;- 0.1\nlog_R &lt;- rep(NA, n)\nlog_R[1] &lt;- rnorm(1, 0, sigma)\nfor (i in 2:n) {\n  log_R[i] &lt;- phi * log_R[i-1] + rnorm(1, 0, sigma)\n}\ndata &lt;- tibble(t = 1:n, R = exp(log_R))\n\nggplot(data, aes(x = t, y = R)) +\n  geom_line() +\n  labs(title = \"Simulated data from an exponentiated AR(1) model\",\n       x = \"Time\",\n       y = \"R\")\n\n\n\n\n\n\n\n\n\n\n\nHowever, we probably don’t want a model which is biased towards a reproduction number of 1 (unless we have good reason to believe that is the expected behaviour). So what should we do?\nReturning to the idea that the reproduction number is a non-stationary time series, as we expect to have a trend in the reproduction numbers we want to capture, we can use a method from the field of time series analysis called differencing to make the time series stationary. This involves taking the difference between consecutive values in the time series. For the log-transformed reproduction number, this would be:\n\\[\nlog(R_t) - log(R_{t-1}) = \\phi (log(R_{t-1}) - log(R_{t-2})) + \\epsilon_t\n\\]\n\n\n\n\n\n\nNoteWhat behaviour would we expect from this model?\n\n\n\n\n\nAgain we look at an R function that implements this model:\n\ngeometric_diff_ar\n\nfunction (init, noise, std, damp) \n{\n    n &lt;- length(noise) + 1\n    x &lt;- numeric(n)\n    x[1] &lt;- log(init)\n    x[2] &lt;- x[1] + noise[1] * std\n    for (i in 3:n) {\n        x[i] &lt;- x[i - 1] + damp * (x[i - 1] - x[i - 2]) + noise[i - \n            1] * std\n    }\n    exp(x)\n}\n&lt;bytecode: 0x564366616720&gt;\n&lt;environment: namespace:nfidd&gt;\n\n\nWe can use this function to simulate a differenced AR process:\n\nR &lt;- geometric_diff_ar(init = 1, noise = rnorm(100), std = 0.1, damp = 0.1)\n\ndata &lt;- tibble(t = seq_along(R), R = R)\n\nggplot(data, aes(x = t, y = R)) +\n  geom_line() +\n  labs(title = \"Simulated data from an exponentiated AR(1) model with differencing\",\n       x = \"Time\",\n       y = \"R\")\n\n\n\n\n\n\n\n\n\n\n\nWe’ve coded up a model that uses this differenced AR process as a stan model in stan/statistical-r.stan. See stan/functions/geometic_diff_ar.stan for the function which calculates the reproduction number. Lets load the model:\n\nstat_mod &lt;- nfidd_cmdstan_model(\"statistical-r\")\nstat_mod\n\n 1: functions {\n 2:   #include \"functions/convolve_with_delay.stan\"\n 3:   #include \"functions/renewal.stan\"\n 4:   #include \"functions/geometric_diff_ar.stan\"\n 5: }\n 6: \n 7: data {\n 8:   int n;                // number of days\n 9:   int I0;              // number initially infected\n10:   array[n] int obs;     // observed symptom onsets\n11:   int gen_time_max;     // maximum generation time\n12:   array[gen_time_max] real gen_time_pmf;  // pmf of generation time distribution\n13:   int&lt;lower = 1&gt; ip_max; // max incubation period\n14:   array[ip_max + 1] real ip_pmf;\n15:   int h;                // number of days to forecast\n16: }\n17: \n18: transformed data {\n19:    int m = n + h;\n20: }\n21: \n22: parameters {\n23:   real init_R;         // initial reproduction number\n24:   array[m-1] real rw_noise; // random walk noise\n25:   real&lt;lower = 0&gt; rw_sd; // random walk standard deviation\n26:   real&lt;lower = 0, upper = 1&gt; damp; // damping\n27: }\n28: \n29: transformed parameters {\n30:   array[m] real R = geometric_diff_ar(init_R, rw_noise, rw_sd, damp);\n31:   array[m] real &lt;upper = 1e5&gt; infections = renewal(I0, R, gen_time_pmf);\n32:   array[m] real onsets = convolve_with_delay(infections, ip_pmf);\n33: }\n34: \n35: model {\n36:   // priors\n37:   init_R ~ normal(1, 0.5);\n38:   rw_noise ~ std_normal();\n39:   rw_sd ~ normal(0, 0.01) T[0,];\n40:   damp ~ normal(1, 0.05) T[0, 1];\n41:   obs ~ poisson(onsets[1:n]);\n42: }\n43: \n44: generated quantities {\n45:   array[h] real forecast;\n46:   if (h &gt; 0) {\n47:     for (i in 1:h) {\n48:       forecast[i] = poisson_rng(onsets[n + i]);\n49:     }\n50:   }\n51: }",
    "crumbs": [
      "Forecasting models"
    ]
  },
  {
    "objectID": "sessions/forecasting-models.html#visualising-your-forecast",
    "href": "sessions/forecasting-models.html#visualising-your-forecast",
    "title": "Forecasting models",
    "section": "Visualising your forecast",
    "text": "Visualising your forecast\n\nforecasts |&gt;\n  filter(.draw %in% sample(.draw, 100)) |&gt;\n  ggplot(aes(x = day)) +\n  geom_line(aes(y = .value, group = interaction(.draw, origin_day), col = origin_day), alpha = 0.1) +\n  geom_point(data = onset_df |&gt;\n    filter(day &gt;= 21),\n    aes(x = day, y = onsets), color = \"black\") +\n  scale_color_binned(type = \"viridis\") +\n  facet_wrap(~model) +\n  lims(y = c(0, 500))\n\nWarning: Removed 73 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nAs for the single forecast it is helpful to also plot the forecast on the log scale.\n\nforecasts |&gt;\n  filter(.draw %in% sample(.draw, 100)) |&gt;\n  ggplot(aes(x = day)) +\n  geom_line(\n    aes(y = .value, group = interaction(.draw, origin_day), col = origin_day),\n    alpha = 0.1\n  ) +\n  geom_point(data = onset_df, aes(x = day, y = onsets), color = \"black\") +\n  scale_y_log10(limits = c(NA, 500)) +\n  scale_color_binned(type = \"viridis\") +\n  facet_wrap(~model)\n\nWarning in scale_y_log10(limits = c(NA, 500)): log-10 transformation introduced infinite values.\nlog-10 transformation introduced infinite values.\n\n\nWarning: Removed 57 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipTake 5 minutes\n\n\n\nHow do these forecasts compare? Which do you prefer?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nHow do these forecasts compare?\n\nThe more mechanistic model captures the downturn in the data very well.\nPast the peak all models were comparable.\nThe more statistical model captures the downturn faster than the random walk but less fast than the more mechanistic mode.\nThe more statistical model sporadically predicts a more rapidly growing outbreak than occurred early on.\nThe more statistical model is more uncertain than the mechanistic model but less uncertain than the random walk.\n\nWhich do you prefer?\n\nThe more mechanistic model seems to be the best at capturing the downturn in the data and the uncertainty in the forecasts seems reasonable.\nIf we weren’t confident in the effective susceptible population the AR model might be preferable.",
    "crumbs": [
      "Forecasting models"
    ]
  },
  {
    "objectID": "sessions/forecasting-models.html#scoring-your-forecast",
    "href": "sessions/forecasting-models.html#scoring-your-forecast",
    "title": "Forecasting models",
    "section": "Scoring your forecast",
    "text": "Scoring your forecast\n\nsc_forecasts &lt;- forecasts |&gt;\n  left_join(onset_df, by = \"day\") |&gt;\n  filter(!is.na(.value)) |&gt;\n  as_forecast_sample(\n    forecast_unit = c(\n      \"origin_day\", \"horizon\", \"model\"\n    ),\n    observed = \"onsets\",\n    predicted = \".value\",\n    sample_id = \".draw\"\n  )\nsc_forecasts\n\nForecast type: sample\n\n\nForecast unit:\n\n\norigin_day, horizon, and model\n\n\n\n        sample_id predicted observed origin_day horizon            model\n            &lt;int&gt;     &lt;num&gt;    &lt;int&gt;      &lt;num&gt;   &lt;int&gt;           &lt;char&gt;\n     1:         1         9        1         22       1      Random walk\n     2:         2         5        1         22       1      Random walk\n     3:         3         5        1         22       1      Random walk\n     4:         4         3        1         22       1      Random walk\n     5:         5         5        1         22       1      Random walk\n    ---                                                                 \n671996:       996         4        4        127      14 More mechanistic\n671997:       997         2        4        127      14 More mechanistic\n671998:       998         1        4        127      14 More mechanistic\n671999:       999         2        4        127      14 More mechanistic\n672000:      1000         1        4        127      14 More mechanistic\n\n\nEverything seems to be in order. We can now use the scoringutils package to calculate some metrics as we did in the forecasting evaluation session.\n\nsc_scores &lt;- sc_forecasts |&gt;\n  score()\n\nsc_scores\n\n     origin_day horizon            model   bias      dss     crps\n          &lt;num&gt;   &lt;int&gt;           &lt;char&gt;  &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n  1:         22       1      Random walk  0.784 2.945204 1.452163\n  2:         22       2      Random walk  0.611 2.577868 1.154219\n  3:         22       3      Random walk  0.970 4.692758 3.122429\n  4:         22       4      Random walk  0.697 3.411297 1.811126\n  5:         22       5      Random walk  0.413 3.011516 1.160931\n ---                                                             \n668:        127      10 More mechanistic  0.522 2.008170 0.842723\n669:        127      11 More mechanistic  0.891 3.578588 1.897985\n670:        127      12 More mechanistic  0.583 1.957965 0.852483\n671:        127      13 More mechanistic -0.632 2.017257 1.035316\n672:        127      14 More mechanistic -0.438 1.440467 0.666765\n     overprediction underprediction dispersion log_score    mad ae_median\n              &lt;num&gt;           &lt;num&gt;      &lt;num&gt;     &lt;num&gt;  &lt;num&gt;     &lt;num&gt;\n  1:          0.954           0.000   0.498163  2.080917 1.4826         2\n  2:          0.584           0.000   0.570219  1.990551 2.9652         2\n  3:          2.466           0.000   0.656429  3.411361 2.9652         4\n  4:          1.068           0.000   0.743126  2.366781 2.9652         3\n  5:          0.362           0.000   0.798931  2.194073 2.9652         2\n ---                                                                     \n668:          0.368           0.000   0.474723  1.849430 1.4826         1\n669:          1.428           0.000   0.469985  2.426058 1.4826         3\n670:          0.402           0.000   0.450483  1.725281 1.4826         1\n671:          0.000           0.612   0.423316  1.980471 1.4826         2\n672:          0.000           0.272   0.394765  1.711154 1.4826         1\n       se_mean\n         &lt;num&gt;\n  1:  6.599761\n  2:  4.721929\n  3: 22.458121\n  4: 11.142244\n  5:  5.148361\n ---          \n668:  2.380849\n669:  9.114361\n670:  2.499561\n671:  2.729104\n672:  0.923521\n\n\n\n\n\n\n\n\nNoteLearning more about the output of score()\n\n\n\n\n\nSee the documentation for ?get_metrics.forecast_sample for information on the default sample metrics.\n\n\n\n\nAt a glance\nLet’s summarise the scores by model first.\n\nsc_scores |&gt;\n  summarise_scores(by = \"model\")\n\n              model         bias      dss      crps overprediction\n             &lt;char&gt;        &lt;num&gt;    &lt;num&gt;     &lt;num&gt;          &lt;num&gt;\n1:      Random walk  0.188834821 6.484737 14.182342       8.545973\n2: More statistical -0.002852679 6.941513 13.064969       6.616107\n3: More mechanistic  0.223825893 5.861359  7.129628       2.449732\n   underprediction dispersion log_score       mad ae_median   se_mean\n             &lt;num&gt;      &lt;num&gt;     &lt;num&gt;     &lt;num&gt;     &lt;num&gt;     &lt;num&gt;\n1:       0.9920357   4.644333  4.100649 19.296966 19.178571 1658.1132\n2:       2.1035268   4.345335  4.169110 17.715084 17.808036 1281.4927\n3:       2.6849821   1.994914  3.830505  8.518331  9.870536  215.3877\n\n\n\n\n\n\n\n\nTipTake 2 minutes\n\n\n\nBefore we look in detail at the scores, what do you think the scores are telling you? Which model do you think is best?\n\n\n\n\nContinuous ranked probability score\nAs in the forecast evaluation session, we will start by looking at the CRPS by horizon and forecast date.\n\n\n\n\n\n\nTipReminder: Key things to note about the CRPS\n\n\n\n\nSmall values are better\nWhen scoring absolute values (e.g. number of cases) it can be difficult to compare forecasts across scales (i.e., when case numbers are different, for example between locations or at different times).\n\n\n\nFirst by forecast horizon.\n\nsc_scores |&gt;\n  summarise_scores(by = c(\"model\", \"horizon\")) |&gt;\n  ggplot(aes(x = horizon, y = crps, col = model)) +\n  geom_point()\n\n\n\n\n\n\n\n\nand across different forecast dates\n\nsc_scores |&gt;\n  summarise_scores(by = c(\"origin_day\", \"model\")) |&gt;\n  ggplot(aes(x = origin_day, y = crps, col = model)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipTake 5 minutes\n\n\n\nHow do the CRPS values change based on forecast date? How do the CRPS values change with forecast horizon?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nHow do the CRPS values change based on forecast horizon?\n\nAll models have increasing CRPS with forecast horizon.\nThe more mechanistic model has the lowest CRPS at all forecast horizon.\nThe more stastical model starts to outperform the random walk model at longer time horizons.\n\nHow do the CRPS values change with forecast date?\n\nThe more statistical model does particularly poorly around the peak of the outbreak but outperforms the random walk model.\nThe more mechanistic model does particularly well around the peak of the outbreak versus all other models\nThe more statistical model starts to outperform the other models towards the end of the outbreak.\n\n\n\n\n\n\nPIT histograms\n\n\n\n\n\n\nTipReminder: Interpreting the PIT histogram\n\n\n\n\nIdeally PIT histograms should be uniform.\nIf is a U shape then the model is overconfident and if it is an inverted U shape then the model is underconfident.\nIf it is skewed then the model is biased towards the direction of the skew.\n\n\n\nLet’s first look at the overall PIT histogram.\n\nsc_forecasts |&gt;\n  get_pit_histogram(by = \"model\") |&gt;\n  ggplot(aes(x = mid, y = density)) +\n  geom_col() +\n  facet_wrap(~model) +\n  labs(x = \"Predicted quantiles\")\n\n\n\n\n\n\n\n\nAs before let’s look at the PIT histogram by forecast horizon (to save space we will group horizons)\n\nsc_forecasts |&gt;\n  mutate(group_horizon = case_when(\n    horizon &lt;= 3 ~ \"1-3\",\n    horizon &lt;= 7 ~ \"4-7\",\n    horizon &lt;= 14 ~ \"8-14\"\n  )) |&gt;\n  get_pit_histogram(by = c(\"model\", \"group_horizon\")) |&gt;\n  ggplot(aes(x = mid, y = density)) +\n  geom_col() +\n  facet_grid(vars(model), vars(group_horizon)) +\n  labs(x = \"Predicted quantiles\")\n\n\n\n\n\n\n\n\nand then for different forecast dates.\n\nsc_forecasts |&gt;\n  get_pit_histogram(by = c(\"model\", \"origin_day\")) |&gt;\n  ggplot(aes(x = mid, y = density)) +\n  geom_col() +\n  facet_grid(vars(model), vars(origin_day)) +\n  labs(x = \"Predicted quantiles\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipTake 5 minutes\n\n\n\nWhat do you think of the PIT histograms?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWhat do you think of the PIT histograms?\n\nThe more mechanistic model is reasonably well calibrated but has a slight tendency to be overconfident.\nThe random walk is biased towards overpredicting.\nThe more statistical model is underconfident.\nAcross horizons the more mechanistic model is only liable to underpredict at the longest horizons.\nThe random walk model is initially relatively unbiased and well calibrated but becomes increasingly likely to overpredict as the horizon increases.\nThe forecast date stratified PIT histograms are hard to interpret. We may need to find other ways to visualise bias and calibration at this level of stratification (see the {scoringutils} documentation for some ideas).",
    "crumbs": [
      "Forecasting models"
    ]
  },
  {
    "objectID": "sessions/forecasting-models.html#scoring-on-the-log-scale",
    "href": "sessions/forecasting-models.html#scoring-on-the-log-scale",
    "title": "Forecasting models",
    "section": "Scoring on the log scale",
    "text": "Scoring on the log scale\nAgain as in the forecast evaluation session, we will also score the forecasts on the log scale.\n\nlog_sc_forecasts &lt;- sc_forecasts |&gt;\n  transform_forecasts(\n    fun = log_shift,\n    offset = 1,\n    append = FALSE\n  )\n\nlog_sc_scores &lt;- log_sc_forecasts |&gt;\n  score()\n\n\n\n\n\n\n\nTip\n\n\n\nReminder: For more on scoring on the log scale see Bosse et al. (2023) on scoring forecasts on transformed scales.\n\n\n\nAt a glance\n\nlog_sc_scores |&gt;\n  summarise_scores(by = \"model\")\n\n              model        bias        dss      crps overprediction\n             &lt;char&gt;       &lt;num&gt;      &lt;num&gt;     &lt;num&gt;          &lt;num&gt;\n1:      Random walk  0.15266071 -0.2341287 0.2681074     0.13590332\n2: More statistical -0.03717857 -0.1785175 0.2849160     0.09898555\n3: More mechanistic  0.18365179 -0.6443524 0.2231066     0.14764543\n   underprediction dispersion log_score       mad ae_median   se_mean\n             &lt;num&gt;      &lt;num&gt;     &lt;num&gt;     &lt;num&gt;     &lt;num&gt;     &lt;num&gt;\n1:      0.04088770 0.09131641 0.7445322 0.3940836 0.3640382 0.2470004\n2:      0.08190782 0.10402266 0.8150220 0.4324431 0.3880622 0.2754773\n3:      0.02404854 0.05141266 0.4494840 0.2202521 0.3034561 0.1898262\n\n\n\n\n\n\n\n\nTipTake 2 minutes\n\n\n\nBefore we look in detail at the scores, what do you think the scores are telling you? Which model do you think is best?\n\n\n\n\nCRPS\n\nlog_sc_scores |&gt;\n  summarise_scores(by = c(\"model\", \"horizon\")) |&gt;\n  ggplot(aes(x = horizon, y = crps, col = model)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nlog_sc_scores |&gt;\n  summarise_scores(by = c(\"origin_day\", \"model\")) |&gt;\n  ggplot(aes(x = origin_day, y = crps, col = model)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipTake 5 minutes\n\n\n\nHow do the CRPS values on the log scale compare to the scores on the original scale?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nThe performance of the mechanistic model is more variable across forecast horizon than on the natural scale.\nOn the log scale the by horizon performance of the random walk and more statistical mdoel is more comparable than on the log scale.\nThe period of high incidence dominates the origin day stratified scores less on the log scale. We see that all models performed less well early and late on.",
    "crumbs": [
      "Forecasting models"
    ]
  },
  {
    "objectID": "sessions/forecasting-models.html#challenge",
    "href": "sessions/forecasting-models.html#challenge",
    "title": "Forecasting models",
    "section": "Challenge",
    "text": "Challenge\n\nWe have only looked at three forecasting models here. There are many more models that could be used. For example, we could use a more complex mechanistic model which captures more of the underlying dynamics of the data generating process. We could also use a more complex statistical model which captures more of the underlying structure of the data.\nWe could also combine the more mechanistic and more statistical models to create a hybrid model which captures the best of both worlds (maybe).\nWe could also use a more complex scoring rule to evaluate the forecasts. For example, we could use a multivariate scoring rule which captures more of the structure of the data.\nConsider how forecasting approaches might differ in non-outbreak settings: What additional factors might you need to account for when applying these methods to routine surveillance data? Think about seasonal patterns, long-term trends, holiday effects, or other cyclical behaviours that might influence transmission dynamics and reporting patterns.",
    "crumbs": [
      "Forecasting models"
    ]
  },
  {
    "objectID": "sessions/forecasting-models.html#methods-in-practice",
    "href": "sessions/forecasting-models.html#methods-in-practice",
    "title": "Forecasting models",
    "section": "Methods in practice",
    "text": "Methods in practice\n\nThe statistical models we’ve discussed here represent just a subset of available forecasting approaches. For a comprehensive treatment of time series forecasting models, including ARIMA models as a generalisation of autoregressive approaches with additional features for differencing and moving averages, see Hyndman and Athanasopoulos (2021). These models offer a different workflow for thinking about forecasting. Whilst Bayesian approaches work forward from domain knowledge to specify data-generating processes, traditional time series methods work backward from observed patterns using standard toolkits (transformations, differencing, residual analysis) to achieve stationarity and improve predictions. Both approaches ultimately aim to capture the same underlying data-generating process but from different perspectives. Something to think about!\nGaussian process models (as implemented in EpiNow2) provide another flexible approach that can generalise many of these statistical models depending on kernel choice.",
    "crumbs": [
      "Forecasting models"
    ]
  },
  {
    "objectID": "sessions/forecasting-models.html#references",
    "href": "sessions/forecasting-models.html#references",
    "title": "Forecasting models",
    "section": "References",
    "text": "References\n\n\nBosse, Nikos I., Sam Abbott, Anne Cori, Edwin van Leeuwen, Johannes Bracher, and Sebastian Funk. 2023. “Scoring Epidemiological Forecasts on Transformed Scales.” Edited by James M McCaw. PLOS Computational Biology 19 (8): e1011393. https://doi.org/10.1371/journal.pcbi.1011393.\n\n\nHyndman, Rob J, and George Athanasopoulos. 2021. Forecasting: Principles and Practice. 3rd ed. Melbourne, Australia: OTexts.",
    "crumbs": [
      "Forecasting models"
    ]
  },
  {
    "objectID": "sessions/joint-nowcasting.html",
    "href": "sessions/joint-nowcasting.html",
    "title": "Nowcasting with an unknown reporting delay",
    "section": "",
    "text": "In the last session we introduced the idea of nowcasting using a simple model. However, this approach had problems: we didn’t fully account for uncertainty, or for example observation error in the primary events, and it’s not a fully generative model of the data reporting process. And as we saw, if we get the delay distribution wrong, we can get the nowcast very wrong.\nA better approach is to jointly estimate the delay distribution together with the nowcast. We can do this by using information from multiple snapshots of the data as it changes over time (using a data structure called the “reporting triangle”). In this session, we’ll introduce this approach to joint estimation in nowcasting. At the end we’ll then demonstrate a way to combine this with our previous work estimating the reproduction number, steadily improving our real time outbreak model.\n\n\n\nIntroduction to joint estimation of delays and nowcasts\n\n\n\n\nThis session aims to introduce how to do nowcasting if the reporting delay distribution is unknown.\n\n\n\n\n\n\nNoteSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/joint-nowcasting.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and tidyr packages for data wrangling, ggplot2 library for plotting, and the tidybayes package for extracting results of the inference.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models. Finally, we set an option to not warn about the partial definition of initial conditions.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)\noptions(cmdstanr_warn_inits = FALSE)",
    "crumbs": [
      "Nowcasting with an unknown reporting delay"
    ]
  },
  {
    "objectID": "sessions/joint-nowcasting.html#slides",
    "href": "sessions/joint-nowcasting.html#slides",
    "title": "Nowcasting with an unknown reporting delay",
    "section": "",
    "text": "Introduction to joint estimation of delays and nowcasts",
    "crumbs": [
      "Nowcasting with an unknown reporting delay"
    ]
  },
  {
    "objectID": "sessions/joint-nowcasting.html#objectives",
    "href": "sessions/joint-nowcasting.html#objectives",
    "title": "Nowcasting with an unknown reporting delay",
    "section": "",
    "text": "This session aims to introduce how to do nowcasting if the reporting delay distribution is unknown.\n\n\n\n\n\n\nNoteSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/joint-nowcasting.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and tidyr packages for data wrangling, ggplot2 library for plotting, and the tidybayes package for extracting results of the inference.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models. Finally, we set an option to not warn about the partial definition of initial conditions.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)\noptions(cmdstanr_warn_inits = FALSE)",
    "crumbs": [
      "Nowcasting with an unknown reporting delay"
    ]
  },
  {
    "objectID": "sessions/joint-nowcasting.html#source-file",
    "href": "sessions/joint-nowcasting.html#source-file",
    "title": "Nowcasting with an unknown reporting delay",
    "section": "",
    "text": "The source file of this session is located at sessions/joint-nowcasting.qmd.",
    "crumbs": [
      "Nowcasting with an unknown reporting delay"
    ]
  },
  {
    "objectID": "sessions/joint-nowcasting.html#libraries-used",
    "href": "sessions/joint-nowcasting.html#libraries-used",
    "title": "Nowcasting with an unknown reporting delay",
    "section": "",
    "text": "In this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and tidyr packages for data wrangling, ggplot2 library for plotting, and the tidybayes package for extracting results of the inference.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.",
    "crumbs": [
      "Nowcasting with an unknown reporting delay"
    ]
  },
  {
    "objectID": "sessions/joint-nowcasting.html#initialisation",
    "href": "sessions/joint-nowcasting.html#initialisation",
    "title": "Nowcasting with an unknown reporting delay",
    "section": "",
    "text": "We set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models. Finally, we set an option to not warn about the partial definition of initial conditions.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)\noptions(cmdstanr_warn_inits = FALSE)",
    "crumbs": [
      "Nowcasting with an unknown reporting delay"
    ]
  },
  {
    "objectID": "sessions/joint-nowcasting.html#motivation",
    "href": "sessions/joint-nowcasting.html#motivation",
    "title": "Nowcasting with an unknown reporting delay",
    "section": "Motivation",
    "text": "Motivation\nSo far we have assumed that the delay distribution is known. In practice, this is often not the case and we need to estimate it from the data. As we discussed in the session on biases in delay distributions, this can be done using individual data and then passing this estimate to a simple nowcasting model. However, this has the disadvantage that the nowcasting model does not take into account the uncertainty in the delay distribution or observation error of the primary events (and potentially some other issues we will cover in this session).\nIn the nowcasting concepts session we also saw that getting the delay distribution wrong can lead to very poor nowcasts.\nA better approach is to jointly estimate the delay distribution together with the nowcast. This builds on the data simulation approach we used in the concepts session, but now we model the entire process at the population level with observation error.",
    "crumbs": [
      "Nowcasting with an unknown reporting delay"
    ]
  },
  {
    "objectID": "sessions/joint-nowcasting.html#extending-our-data-simulation-approach",
    "href": "sessions/joint-nowcasting.html#extending-our-data-simulation-approach",
    "title": "Nowcasting with an unknown reporting delay",
    "section": "Extending our data simulation approach",
    "text": "Extending our data simulation approach\nRecall from the nowcasting concepts session that we simulated individual reporting delays and then aggregated them. Now we’ll simulate the same process but at the population level with observation error - for the same reasons we made this change in the convolutions session.\nFirst, let’s generate our simulated onset dataset as before:\n\ngen_time_pmf &lt;- make_gen_time_pmf()\nip_pmf &lt;- make_ip_pmf()\nonset_df &lt;- simulate_onsets(\n  make_daily_infections(infection_times), gen_time_pmf, ip_pmf\n)\nhead(onset_df)\n\n# A tibble: 6 × 3\n    day onsets infections\n  &lt;dbl&gt;  &lt;int&gt;      &lt;int&gt;\n1     1      0          0\n2     2      0          1\n3     3      0          0\n4     4      1          2\n5     5      0          1\n6     6      0          1\n\ncutoff &lt;- 71\n\nWe then need to simulate the reporting delays:\n\nreporting_delay_pmf &lt;- censored_delay_pmf(\n  rlnorm, max = 15, meanlog = 1, sdlog = 0.5\n)\nplot(reporting_delay_pmf)\n\n\n\n\n\n\n\n\nWe can then simulate the data by day, onset day, and reporting day by applying the reporting delay distribution to the onsets (notice how this is nearly identical to the convolution we saw in the convolutions session except that we are not applying the sum):\n\nreporting_triangle &lt;- onset_df |&gt;\n  filter(day &lt; cutoff) |&gt;\n  mutate(\n    reporting_delay = list(\n      tibble(d = 0:15, reporting_delay = reporting_delay_pmf)\n    )\n  ) |&gt;\n  unnest(reporting_delay) |&gt;\n  mutate(\n    reported_onsets = rpois(n(), onsets * reporting_delay)\n  ) |&gt;\n  mutate(reported_day = day + d)\n\ntail(reporting_triangle)\n\n# A tibble: 6 × 7\n    day onsets infections     d reporting_delay reported_onsets reported_day\n  &lt;dbl&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt; &lt;table[1d]&gt;               &lt;int&gt;        &lt;dbl&gt;\n1    70     76        115    10 0.0028236523                  0           80\n2    70     76        115    11 0.0014853431                  0           81\n3    70     76        115    12 0.0007961839                  0           82\n4    70     76        115    13 0.0004601063                  0           83\n5    70     76        115    14 0.0002700624                  0           84\n6    70     76        115    15 0.0001920444                  0           85\n\n\nWe also apply the Poisson observation error to the reported onsets to capture our uncertainty in the reporting process (remember we have lost uncertainty in the onsets as we are not individually simulating the reporting delays).\nTo recover the onsets onsets by day we group by day and then sum reported onsets across report days.\n\nnoisy_onsets_df &lt;- reporting_triangle |&gt;\n  summarise(noisy_onsets = sum(reported_onsets), .by = day)\n\ntail(noisy_onsets_df)\n\n# A tibble: 6 × 2\n    day noisy_onsets\n  &lt;dbl&gt;        &lt;int&gt;\n1    65           66\n2    66           77\n3    67           71\n4    68           84\n5    69           84\n6    70           81\n\n\nAs we only observed reported cases up to the current day we need to filter it to only include the data we have observed:\n\nfiltered_reporting_triangle &lt;- reporting_triangle |&gt;\n  filter(reported_day &lt;= max(day))\n\ntail(noisy_onsets_df)\n\n# A tibble: 6 × 2\n    day noisy_onsets\n  &lt;dbl&gt;        &lt;int&gt;\n1    65           66\n2    66           77\n3    67           71\n4    68           84\n5    69           84\n6    70           81\n\n\nFinally, we sum this to get the counts we actually observe. This is the same as the date we corrected for right truncation in the nowcasting concepts session.\n\navailable_onsets &lt;- filtered_reporting_triangle |&gt;\n  summarise(available_onsets = sum(reported_onsets), .by = day)\n\ntail(available_onsets)\n\n# A tibble: 6 × 2\n    day available_onsets\n  &lt;dbl&gt;            &lt;int&gt;\n1    65               61\n2    66               67\n3    67               44\n4    68               32\n5    69               14\n6    70                0",
    "crumbs": [
      "Nowcasting with an unknown reporting delay"
    ]
  },
  {
    "objectID": "sessions/joint-nowcasting.html#understanding-the-data-structure",
    "href": "sessions/joint-nowcasting.html#understanding-the-data-structure",
    "title": "Nowcasting with an unknown reporting delay",
    "section": "Understanding the data structure",
    "text": "Understanding the data structure\nNotice how this simulation creates a similar, but different, data structure than in the concepts session.\n\n\n\n\n\n\nNoteThree-dimensional epidemiological data\n\n\n\nSo far we’ve worked with data that has two dimensions. Such as:\n\nOnset day: when symptoms began\nOnset counts: the number of cases that occurred on each day\n\nNow we are introducing a third dimension:\n\nReport day: when the case enters our surveillance system\n\nThis means that to recover the true onset counts we need to sum across the third dimension and we may not be able to do this if all the data has not been reported yet (i.e. we have right truncation in our data). This is just a reformulation of the nowcasting problem we saw in the nowcasting concepts session.\n\n\nThis richer data structure contains information about both the delay distribution and the final expected counts so we can use it to jointly estimate both which was not possible with the simpler data structure we used in the nowcasting concepts session.",
    "crumbs": [
      "Nowcasting with an unknown reporting delay"
    ]
  },
  {
    "objectID": "sessions/joint-nowcasting.html#mathematical-formulation",
    "href": "sessions/joint-nowcasting.html#mathematical-formulation",
    "title": "Nowcasting with an unknown reporting delay",
    "section": "Mathematical formulation",
    "text": "Mathematical formulation\nWe can formalise this process mathematically. The total number of onsets on day \\(t\\) is: \\[\nN_{t} = \\sum_{d=0}^{D} n_{t,d}\n\\]\nwhere \\(n_{t,d}\\) is the number of onsets on day \\(t\\) that are reported on day \\(t+d\\), and \\(D\\) is the maximum delay.\nWe model each component as: \\[\n  n_{t,d} \\mid \\lambda_{t},p_{t,d} \\sim \\text{Poisson} \\left(\\lambda_{t} \\times p_{t,d} \\right),\\ t=1,...,T.\n\\]\nwhere:\n\n\\(\\lambda_{t}\\) is the expected number of onsets on day \\(t\\)\n\\(p_{t,d}\\) is the probability that an onset on day \\(t\\) is reported on day \\(t+d\\)\n\nThis approach jointly estimates the delay distribution (\\(p_{t,d}\\)) and the underlying onset counts (\\(\\lambda_{t}\\)) from the observed data.\n\n\n\n\n\n\nImportantModelling options\n\n\n\nWe now have two main modelling options:\n\nHow we model the expected number of onsets \\(\\lambda_{t}\\)\nHow we model the probability of reporting \\(p_{t,d}\\)\n\nWe will explore these in the next section.\n\n\nA key insight is that we can split each \\(n_{t,d}\\) into observed and unobserved components:\n\\[n_{t,d} = n_{t,d}^{obs} + n_{t,d}^{miss}\\]\nwhere:\n\n\\(n_{t,d}^{obs}\\) is what we observe (when \\(t+d \\leq\\) current day)\n\\(n_{t,d}^{miss}\\) is what we need to estimate (when \\(t+d &gt;\\) current day)\n\nThe joint model uses the observed data to estimate both the delay distribution and the underlying onset counts, which then allows us to predict the missing entries.\n\n\n\n\n\n\nNoteThe reporting triangle\n\n\n\n\n\nThis data structure is sometimes called the “reporting triangle” in the literature because when visualised as a matrix (with onset days as rows and reporting days as columns), the observed data (\\(n_{t,d}^{obs}\\)) creates a triangular shape.\n\n\n\nReporting triangle visualisation (by Johannes Bracher)\n\n\nNowcasting aims to complete this triangle by estimating the missing entries (\\(n_{t,d}^{miss}\\)).\n\n\n\nCompleted reporting triangle (by Johannes Bracher)\n\n\nOnce completed, we sum across the rows to get our nowcast of the true onset counts.",
    "crumbs": [
      "Nowcasting with an unknown reporting delay"
    ]
  },
  {
    "objectID": "sessions/joint-nowcasting.html#fitting-the-joint-model",
    "href": "sessions/joint-nowcasting.html#fitting-the-joint-model",
    "title": "Nowcasting with an unknown reporting delay",
    "section": "Fitting the joint model",
    "text": "Fitting the joint model\nFor fitting the joint model specification follows our data simulation approach but we have to make choices about how to model the expected number of onsets \\(\\lambda_{t}\\) and the probability of reporting \\(p_{t,d}\\).\nAs usual, we start by loading the model:\n\njoint_mod &lt;- nfidd_cmdstan_model(\"joint-nowcast\")\njoint_mod\n\n 1: functions {\n 2:   #include \"functions/geometric_random_walk.stan\"\n 3:   #include \"functions/observe_onsets_with_delay.stan\"\n 4:   #include \"functions/combine_obs_with_predicted_obs_rng.stan\"\n 5: }\n 6: \n 7: data {\n 8:   int n;                // number of days\n 9:   int m;                // number of reports\n10:   array[n] int p;       // number of observations per day\n11:   array[m] int obs;     // observed symptom onsets\n12:   int d;                // number of reporting delays\n13: }\n14: \n15: transformed data{\n16:   array[n] int P = to_int(cumulative_sum(p));\n17:   array[n] int D = to_int(cumulative_sum(rep_array(d, n)));\n18: }\n19: \n20: parameters {\n21:   real&lt;lower=0&gt; init_onsets;\n22:   array[n-1] real rw_noise;\n23:   real&lt;lower=0&gt; rw_sd;\n24:   simplex[d] reporting_delay; // reporting delay distribution\n25: }\n26: \n27: transformed parameters {\n28:   array[n] real onsets = geometric_random_walk(init_onsets, rw_noise, rw_sd);\n29:   array[m] real onsets_by_report = observe_onsets_with_delay(onsets, reporting_delay, P, p);\n30: }\n31: \n32: model {\n33:   // Prior\n34:   init_onsets ~ normal(1, 5) T[0,];\n35:   rw_noise ~ std_normal();\n36:   rw_sd ~ normal(0, 0.05) T[0,];\n37:   reporting_delay ~ dirichlet(rep_vector(1, d));\n38:   // Likelihood\n39:   obs ~ poisson(onsets_by_report);\n40: }\n41: \n42: generated quantities {\n43:   array[d*n] real complete_onsets_by_report = observe_onsets_with_delay(onsets, reporting_delay, D, rep_array(d, n));\n44:   array[n] int nowcast = combine_obs_with_predicted_obs_rng(obs, complete_onsets_by_report, P, p, d, D);\n45: }\n46: \n\n\n\n\n\n\n\n\nTipModel details\n\n\n\n\n\nThis time we won’t go into details of the model. For now, it is important that you understand the concept, but as the models get more complex, we hope that you trust us that the model does what we describe above.\nOne thing to note is that we are now fitting the initial number of symptom onsets (init_onsets). This is different from earlier when we had to pass the initial number of infections (I0) as data. In most situations, this number would be unknown, so what we do here is closer to what one would do in the real world.\n\n\n\n\n\n\n\n\n\nNoteTake two minutes\n\n\n\nWhat are the models we have picked for the onsets (\\(\\lambda_{t}\\)) and the reporting delay distribution (\\(p_{t,d}\\))?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n\\(\\lambda_{t}\\) is modelled using a geometric random walk.\n\\(p_{t,d}\\) is modelled using a Dirichlet distribution (i.e. a multinomial distribution with a constraint that the sum of the probabilities is 1 - this allows the delay distribution to be flexible).\n\n\n\n\nWe then fit it to data:\n\njoint_data &lt;- list(\n  n = length(unique(filtered_reporting_triangle$day)), # number of days\n  m = nrow(filtered_reporting_triangle),               # number of reports\n  p = filtered_reporting_triangle |&gt;\n   group_by(day) |&gt;\n   filter(d == max(d)) |&gt;\n   mutate(d = d + 1) |&gt;\n   pull(d),            # number of observations per day\n  obs = filtered_reporting_triangle$reported_onsets,   # observed symptom onsets\n  d = 16               # number of reporting delays\n)\njoint_nowcast_fit &lt;- nfidd_sample(joint_mod, data = joint_data)\n\n\njoint_nowcast_fit\n\n    variable   mean median   sd  mad     q5    q95 rhat ess_bulk ess_tail\n lp__        528.30 528.79 7.02 6.72 516.25 539.14 1.00      721     1063\n init_onsets   0.76   0.71 0.30 0.29   0.35   1.31 1.00     2765     1513\n rw_noise[1]  -0.07  -0.10 1.01 1.01  -1.71   1.64 1.00     2833     1419\n rw_noise[2]   0.05   0.05 0.99 0.96  -1.62   1.70 1.00     4161     1377\n rw_noise[3]   0.21   0.23 0.94 0.95  -1.31   1.74 1.00     3223     1394\n rw_noise[4]   0.22   0.21 1.00 1.01  -1.45   1.90 1.00     3594     1340\n rw_noise[5]   0.33   0.34 0.98 1.01  -1.27   1.91 1.00     3543     1340\n rw_noise[6]   0.46   0.48 0.95 0.97  -1.07   2.01 1.00     3442     1417\n rw_noise[7]   0.09   0.09 0.96 0.96  -1.45   1.68 1.00     3676     1273\n rw_noise[8]  -0.24  -0.23 0.95 0.96  -1.78   1.34 1.01     3133     1619\n\n # showing 10 of 2348 rows (change via 'max_rows' argument or 'cmdstanr_max_rows' option)\n\n\n\n\n\n\n\n\nImportant\n\n\n\nOne benefit of this model is that, because we have decomposed the data into the reporting triangle, we can make a nowcast that uses the data we have available, augmented with predictions from the model. This should give us more accurate uncertainty estimates than the simple nowcasting models above (see stan/functions/combine_obs_with_predicted_obs_rng.stan but note the code is fairly involved).\n\n\nWe now extract this nowcast:\n\njoint_nowcast_onsets &lt;- joint_nowcast_fit |&gt;\n  gather_draws(nowcast[day]) |&gt;\n  ungroup() |&gt;\n  filter(.draw %in% sample(.draw, 100))\n\nFinally, we can plot the nowcast alongside the observed data:\n\nggplot(joint_nowcast_onsets, aes(x = day)) +\n  geom_col(\n    data = noisy_onsets_df, mapping = aes(y = noisy_onsets), alpha = 0.6\n  ) +\n  geom_line(mapping = aes(y = .value, group = .draw), alpha = 0.1) +\n  geom_point(data = available_onsets, mapping = aes(y = available_onsets))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nReminder: The points in this plot represent the data available when the nowcast was made (and so are truncated) whilst the bars represent the finally reported data (a perfect nowcast would exactly reproduce these).\n\n\n\n\n\n\n\n\nTipTake 5 minutes\n\n\n\nLook back at the last three nowcasts. How do they compare? What are the advantages and disadvantages of each? Could we improve the nowcasts further?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nThe simple nowcast struggled to capture the generative process of the data and so produced poor nowcasts. The nowcast with the geometric random walk was better but still struggled to capture the generative process of the data. The joint nowcast was the best of the three as it properly handled the uncertainty and allowed us to fit the delay distribution versus relying on known delays.\nHowever, the joint nowcast is still quite simple (in the sense that no detailed mechanism or reporting process is being modelled) and so may struggle to capture more complex patterns in the data. In particular, the prior model for the geometric random walk assumes that onsets are the same as the previous day with some statistical noise. This may not be a good assumption in a rapidly changing epidemic (where the reproduction number is not near 1).\nIn addition, whilst we say it is “quite simple” as should be clear from the code, it is quite complex and computationally intensive. This is because we are fitting a model to the reporting triangle, which is a much larger data set and so the model is relatively quite slow to fit.",
    "crumbs": [
      "Nowcasting with an unknown reporting delay"
    ]
  },
  {
    "objectID": "sessions/joint-nowcasting.html#challenge",
    "href": "sessions/joint-nowcasting.html#challenge",
    "title": "Nowcasting with an unknown reporting delay",
    "section": "Challenge",
    "text": "Challenge\n\nThe simple nowcast models we showed here assumed perfect knowledge of the delay distribution. What happens when you instead use an estimate of the delay distribution from the data? Try and do this using methods from session on biases in delay distributions and see how it affects the simple nowcast models.\nDespite being fairly involved, the joint nowcast model we used here is still quite simple and may struggle to capture more complex patterns in the data.",
    "crumbs": [
      "Nowcasting with an unknown reporting delay"
    ]
  },
  {
    "objectID": "sessions/joint-nowcasting.html#methods-in-practice",
    "href": "sessions/joint-nowcasting.html#methods-in-practice",
    "title": "Nowcasting with an unknown reporting delay",
    "section": "Methods in practice",
    "text": "Methods in practice\nIn practice, more complex methods are often needed to account for structure in the reporting process, time-varying delay distributions, or delays that vary by other factors (such as the age of cases). Consider how nowcasting approaches might differ in non-outbreak settings: What additional factors might you need to account for when applying these methods to routine surveillance data? Think about seasonal patterns, long-term trends, or other cyclical behaviours that might influence case reporting patterns.\n\nThis session focused on the role of the generative process in nowcasting. This is an area of active research but (Lison et al. 2024) gives a good overview of the current state of the art.\nThe epinowcast package implements a more complex version of the model we have used here. It is designed to be highly flexible and so can be used to model a wide range of different datasets.",
    "crumbs": [
      "Nowcasting with an unknown reporting delay"
    ]
  },
  {
    "objectID": "sessions/nowcasting.html",
    "href": "sessions/nowcasting.html",
    "title": "Nowcasting concepts",
    "section": "",
    "text": "So far we’ve explored the delays and biases of real-time infectious disease surveillance data, started to correct for these, and considered the underlying process that drives the evolution of epidemics (looking at the reproduction number and renewal equation). Next, we’ll focus on predicting new information about how infectious disease transmission is evolving in the present and future.\nWe know that we have incomplete information in the present because of delays in the observation process (reporting delays). The aim of nowcasting is to predict what an epidemiological time series will look like after all delayed reports are in, for which we need to account for the delays and biases we’ve already considered.\n\n\n\nIntroduction to nowcasting\n\n\n\n\nThis session aims to introduce the concept of nowcasting, and see how we can perform a nowcast if we know the underlying delay distribution.\n\n\n\n\n\n\nNoteSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/nowcasting.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and tidyr packages for data wrangling, ggplot2 library for plotting, and the tidybayes package for extracting results of the inference.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Nowcasting concepts"
    ]
  },
  {
    "objectID": "sessions/nowcasting.html#slides",
    "href": "sessions/nowcasting.html#slides",
    "title": "Nowcasting concepts",
    "section": "",
    "text": "Introduction to nowcasting",
    "crumbs": [
      "Nowcasting concepts"
    ]
  },
  {
    "objectID": "sessions/nowcasting.html#objectives",
    "href": "sessions/nowcasting.html#objectives",
    "title": "Nowcasting concepts",
    "section": "",
    "text": "This session aims to introduce the concept of nowcasting, and see how we can perform a nowcast if we know the underlying delay distribution.\n\n\n\n\n\n\nNoteSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/nowcasting.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and tidyr packages for data wrangling, ggplot2 library for plotting, and the tidybayes package for extracting results of the inference.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Nowcasting concepts"
    ]
  },
  {
    "objectID": "sessions/nowcasting.html#source-file",
    "href": "sessions/nowcasting.html#source-file",
    "title": "Nowcasting concepts",
    "section": "",
    "text": "The source file of this session is located at sessions/nowcasting.qmd.",
    "crumbs": [
      "Nowcasting concepts"
    ]
  },
  {
    "objectID": "sessions/nowcasting.html#libraries-used",
    "href": "sessions/nowcasting.html#libraries-used",
    "title": "Nowcasting concepts",
    "section": "",
    "text": "In this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and tidyr packages for data wrangling, ggplot2 library for plotting, and the tidybayes package for extracting results of the inference.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.",
    "crumbs": [
      "Nowcasting concepts"
    ]
  },
  {
    "objectID": "sessions/nowcasting.html#initialisation",
    "href": "sessions/nowcasting.html#initialisation",
    "title": "Nowcasting concepts",
    "section": "",
    "text": "We set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Nowcasting concepts"
    ]
  },
  {
    "objectID": "sessions/nowcasting.html#the-simplest-possible-nowcasting-model",
    "href": "sessions/nowcasting.html#the-simplest-possible-nowcasting-model",
    "title": "Nowcasting concepts",
    "section": "The simplest possible nowcasting model",
    "text": "The simplest possible nowcasting model\nHere we assume that the delay distribution is known and that we can use it to nowcast the most recent data. In practice, the delay distribution is often not known and needs to be estimated from the data. We could do this using methods from the session on biases in delay distributions.\nIn the session on convolutions we used delay distributions convolved with the infection times to estimate the time series of symptom onsets. A simple way to nowcast is to use the same approach but using the cumulative distribution function of the delay distribution rather than the probability density function and only apply it to the most recent data as this is the only data that can be subject to change (due to delays in reporting).\nWe will build intuition for this as usual using simulation. First we define the proportion reported using a delay distribution up to 15 days, again using a lognormal distribution with meanlog 1 and sdlog 0.5:\n\nproportion_reported &lt;- plnorm(1:15, 1, 0.5)\nplot(proportion_reported)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipThe plnorm function\n\n\n\nThe plnorm() function is related to the rlnorm() function we used earlier to simulate the individual level reporting delay, but instead it gives the cumulative distribution function rather than random samples. That is, it gives us the probability that a report is made on day 1 or earlier, day 2 or earlier, etc.\n\n\nWe can now use this delay distribution to nowcast the most recent data. Here we use the same simulation approach as in the renewal session (here we have created helper functions make_gen_time_pmf() and make_ip_pmf() to make it easier to re-use; we recommend to have a look at what these functions do), and apply the reporting_delay to the last 15 days of data.\n\ngen_time_pmf &lt;- make_gen_time_pmf()\nip_pmf &lt;- make_ip_pmf()\nonset_df &lt;- simulate_onsets(\n  make_daily_infections(infection_times), gen_time_pmf, ip_pmf\n)\nreported_onset_df &lt;- onset_df |&gt;\n  filter(day &lt; cutoff) |&gt;\n  mutate(proportion_reported = c(rep(1, n() - 15), rev(proportion_reported)),\n         reported_onsets = rpois(n(), onsets * proportion_reported)\n  )\ntail(reported_onset_df)\n\n# A tibble: 6 × 5\n    day onsets infections proportion_reported reported_onsets\n  &lt;dbl&gt;  &lt;int&gt;      &lt;int&gt;               &lt;dbl&gt;           &lt;int&gt;\n1    65     63         83              0.943               51\n2    66     64         75              0.889               58\n3    67     75         92              0.780               57\n4    68     78        113              0.578               34\n5    69     69        120              0.270               20\n6    70     84        115              0.0228               4\n\n\n\n\n\n\n\n\nTipTake 3 minutes\n\n\n\nSpend a few minutes trying to understand the code above. What is the proportion_reported? What is the reported_onsets?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nThe proportion_reported is the cumulative distribution function of the delay distribution. It gives the probability that a report is made on day 1 or earlier, day 2 or earlier, etc. Note that for days more that 15 days into the past, this is 1, meaning all onsets that occurred on that day have been reported.\nThe reported_onsets are the number of onsets that are reported on each day. This is calculated by multiplying the number of onsets by the proportion of onsets that are reported on each day. It has Poisson noise added to it to simulate the stochasticity in the reporting process.\n\n\nreported_onset_df |&gt;\n  ggplot(aes(x = day, y = reported_onsets)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n\n\nWe can now fit our first nowcasting model. Here we assume exactly the same generative process as we used for simulation and model the number of onsets as independent draws from a normal distribution.\n\nmod &lt;- nfidd_cmdstan_model(\"simple-nowcast\")\nmod\n\n 1: functions {\n 2:   #include \"functions/condition_onsets_by_report.stan\"\n 3: }\n 4: \n 5: data {\n 6:   int n;                // number of days\n 7:   array[n] int obs;     // observed symptom onsets\n 8:   int report_max;       // max reporting delay\n 9:   array[report_max + 1] real report_cdf;\n10: }\n11: \n12: parameters {\n13:   array[n] real&lt;lower = 0&gt; onsets;\n14: }\n15: \n16: transformed parameters {\n17:   array[n] real reported_onsets = condition_onsets_by_report(onsets, report_cdf);\n18: }\n19: \n20: model {\n21:   onsets ~ normal(5, 20) T[0,];\n22:   // Likelihood\n23:   obs ~ poisson(reported_onsets);\n24: }\n\n\n\n\n\n\n\n\nTipTake 2 minutes\n\n\n\nFamiliarise yourself with the model above. What does it do?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nOn line 2 we define a new function condition_onsets_by_report.stan which takes the number of onsets and reports and the delay distribution as input and returns the nowcasted number of onsets. To find out more about what this function does, you can inspect the condition_onsets_by_report R function or navigate to condition_onsets_by_report.stan.\nOn line 17, this function is used to calculate the nowcasted number of onsets and this is then used in the likelihood.\nOn line 21, we define the generative process for the number of onsets. Here we assume that onsets are independent with each drawn from a normal distribution.\n\n\n\n\nOnce again we can generate estimates from this model:\n\ndata &lt;- list(\n  n = nrow(reported_onset_df) - 1,\n  obs = reported_onset_df$reported_onsets[-1],\n  report_max = length(proportion_reported) - 1,\n  report_cdf = proportion_reported \n)\nsimple_nowcast_fit &lt;- nfidd_sample(mod, data = data)\n\n\nsimple_nowcast_fit\n\n  variable    mean  median   sd  mad      q5     q95 rhat ess_bulk ess_tail\n lp__      2342.51 2342.81 5.96 5.96 2332.23 2351.72 1.00      755     1093\n onsets[1]    0.97    0.72 0.93 0.71    0.05    2.90 1.00     2788     1196\n onsets[2]    1.00    0.69 1.00 0.71    0.05    3.00 1.00     2000      778\n onsets[3]    1.03    0.69 1.07 0.74    0.05    3.13 1.00     2592      955\n onsets[4]    1.02    0.74 1.00 0.75    0.05    2.98 1.00     2872     1173\n onsets[5]    1.07    0.72 1.08 0.75    0.06    3.23 1.00     3070     1060\n onsets[6]    1.02    0.68 1.06 0.71    0.05    3.06 1.00     2501     1072\n onsets[7]    2.04    1.72 1.48 1.22    0.34    4.89 1.00     3156     1356\n onsets[8]    1.99    1.66 1.35 1.18    0.39    4.59 1.00     3441     1312\n onsets[9]    1.02    0.70 1.02 0.75    0.04    3.04 1.00     2757      834\n\n # showing 10 of 139 rows (change via 'max_rows' argument or 'cmdstanr_max_rows' option)\n\n\nWe can now plot onsets alongside those nowcasted by the model:\n\nnowcast_onsets &lt;- simple_nowcast_fit |&gt;\n  gather_draws(onsets[day]) |&gt;\n  ungroup() |&gt;\n  filter(.draw %in% sample(.draw, 100)) |&gt;\n  mutate(day = day + 1)\n\n\nggplot(nowcast_onsets, aes(x = day)) +\n  geom_line(mapping = aes(y = .value, group = .draw), alpha = 0.1) +\n  geom_col(data = reported_onset_df, mapping = aes(y = onsets), alpha = 0.6) +\n  geom_point(data = reported_onset_df, mapping = aes(y = reported_onsets))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe points in this plot represent the data available when the nowcast was made (and so are truncated) whilst the bars represent the finally reported data (a perfect nowcast would exactly reproduce these).\n\n\n\n\n\n\n\n\nTip\n\n\n\nAs we found in the using delay distributions to model the data generating process of an epidemic session, this simple model struggles to recreate the true number of onsets. This is because it does not capture the generative process of the data (i.e. the transmission process and delays from infection to onset). In the next section we will see how we can use a model that does capture this generative process to improve our nowcasts.\n\n\n\n\n\n\n\n\nImportantSimple multiplicative nowcasting methods\n\n\n\nBefore moving to more complex models, it’s worth noting that there is an even simpler nowcasting method that simply divides currently observed counts by the cumulative distribution function (CDF) of the delay distribution. This approach is often called “multiplicative” or “scaling” nowcasting.\nWhile this method appears simple, it still embodies a model with specific assumptions that can be difficult to define precisely. Key challenges include:\n\nThe method struggles with noisy data, particularly when counts are low\nSparse data (e.g., zero counts) can lead to unstable estimates\nGenerating uncertainty\nThe implicit assumptions about the data generating process are not always clear\n\nThe baselinenowcast R package provides a set of tools for these types of methods, including approaches to handle both noise and sparsity in the data. These baseline methods can serve as useful benchmarks when developing more complex nowcasting models.",
    "crumbs": [
      "Nowcasting concepts"
    ]
  },
  {
    "objectID": "sessions/nowcasting.html#adding-in-a-geometric-random-walk-to-the-nowcasting-model",
    "href": "sessions/nowcasting.html#adding-in-a-geometric-random-walk-to-the-nowcasting-model",
    "title": "Nowcasting concepts",
    "section": "Adding in a geometric random walk to the nowcasting model",
    "text": "Adding in a geometric random walk to the nowcasting model\nAs we saw in the session on the renewal equation, a geometric random walk is a simple way to model multiplicative growth. Adding this into our simple nowcasting model may help us to better capture the generative process of the data and so produce a better nowcast.\nWe first load the model\n\nrw_mod &lt;- nfidd_cmdstan_model(\"simple-nowcast-rw\")\nrw_mod\n\n 1: functions {\n 2:   #include \"functions/geometric_random_walk.stan\"\n 3:   #include \"functions/condition_onsets_by_report.stan\"\n 4: }\n 5: \n 6: data {\n 7:   int n;                // number of days\n 8:   array[n] int obs;     // observed symptom onsets\n 9:   int report_max;       // max reporting delay\n10:   array[report_max + 1] real report_cdf;\n11: }\n12: \n13: parameters {\n14:   real&lt;lower=0&gt; init_onsets;\n15:   array[n-1] real rw_noise;\n16:   real&lt;lower=0&gt; rw_sd;\n17: }\n18: \n19: transformed parameters {\n20:   array[n] real onsets = geometric_random_walk(init_onsets, rw_noise, rw_sd);\n21:   array[n] real reported_onsets = condition_onsets_by_report(onsets, report_cdf);\n22: }\n23: \n24: model {\n25:   init_onsets ~ lognormal(0, 1) T[0,];\n26:   rw_noise ~ std_normal();\n27:   rw_sd ~ normal(0, 5) T[0,];\n28:   //Likelihood\n29:   obs ~ poisson(reported_onsets);\n30: }\n\n\nand then fit it\n\nrw_nowcast_fit &lt;- nfidd_sample(rw_mod, data = data)\n\n\nrw_nowcast_fit\n\n    variable    mean  median   sd  mad      q5     q95 rhat ess_bulk ess_tail\n lp__        2186.23 2186.49 8.09 8.20 2172.81 2199.02 1.01      459      959\n init_onsets    0.43    0.37 0.23 0.20    0.14    0.89 1.00     2009     1493\n rw_noise[1]   -0.20   -0.20 0.94 0.96   -1.75    1.34 1.00     3064     1645\n rw_noise[2]   -0.06   -0.08 0.95 0.92   -1.60    1.58 1.00     2400     1379\n rw_noise[3]    0.04    0.03 0.95 0.94   -1.53    1.61 1.01     2532     1353\n rw_noise[4]    0.20    0.23 0.98 0.99   -1.43    1.77 1.00     2635     1282\n rw_noise[5]    0.32    0.32 0.96 0.96   -1.20    1.95 1.00     3536     1570\n rw_noise[6]    0.44    0.44 1.02 1.06   -1.19    2.10 1.01     2876     1485\n rw_noise[7]    0.32    0.33 0.92 0.91   -1.17    1.82 1.00     3044     1246\n rw_noise[8]    0.12    0.09 0.96 0.94   -1.45    1.69 1.00     3174     1156\n\n # showing 10 of 209 rows (change via 'max_rows' argument or 'cmdstanr_max_rows' option)\n\n\nAgain we can extract the nowcasted onsets and plot them alongside the observed data:\n\nrw_nowcast_onsets &lt;- rw_nowcast_fit |&gt;\n  gather_draws(onsets[day]) |&gt;\n  ungroup() |&gt;\n  filter(.draw %in% sample(.draw, 100)) |&gt; ## sample 100 iterations randomly\n  mutate(day = day + 1)\n\n\nggplot(rw_nowcast_onsets, aes(x = day)) +\n  geom_col(data = reported_onset_df, mapping = aes(y = onsets), alpha = 0.6) +\n  geom_line(mapping = aes(y = .value, group = .draw), alpha = 0.1) +\n  geom_point(data = reported_onset_df, mapping = aes(y = reported_onsets))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipTake 2 minutes\n\n\n\nWhat do you think of the nowcast now? Does it look better than the previous one?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nThe nowcast better matches the ultimately observed data. The geometric random walk allows the model to capture the multiplicative growth in the data and so better capture that current indidence is related to past incidence.\nThis should be particularly true when the data is more truncated (i.e nearer to the date of the nowcast) as the geometric random walk allows the model to extrapolate incidence based on previous incidence rather than relying on the prior distribution as the simpler model did.\nHowever, the model is still quite simple and so may struggle to capture more complex patterns in the data. In particular, the prior model for the geometric random walk assumes that onsets are the same as the previous day with statistical noise. This may not be a good assumption in a rapidly changing epidemic (where the reproduction number is not near 1).",
    "crumbs": [
      "Nowcasting concepts"
    ]
  },
  {
    "objectID": "sessions/nowcasting.html#what-happens-if-we-get-the-delay-distribution-wrong",
    "href": "sessions/nowcasting.html#what-happens-if-we-get-the-delay-distribution-wrong",
    "title": "Nowcasting concepts",
    "section": "What happens if we get the delay distribution wrong?",
    "text": "What happens if we get the delay distribution wrong?\n\n\n\n\n\n\nNoteOptional section\n\n\n\nThis section can be skipped if time is limited. It demonstrates the importance of accurate delay distribution estimation.\n\n\nIn practice, we often do not know the delay distribution and so need to estimate it using the data at hand. In the session on biases in delay distributions we saw how we could do this using individual-level records. We will now look at what happens if we get the delay distribution wrong.\nWe use the same data as before but now assume that the delay distribution is a gamma distribution with shape 2 and rate 3. This is a very different distribution to the lognormal distribution we used to simulate the data.\n\nwrong_proportion_reported &lt;- pgamma(1:15, 2, 3)\nplot(wrong_proportion_reported)\n\n\n\n\n\n\n\n\nWe first need to update the data to use this new delay distribution:\n\nwrong_delay_data &lt;- data\nwrong_delay_data$report_cdf &lt;- wrong_proportion_reported\n\nWe now fit the nowcasting model with the wrong delay distribution:\n\ngamma_nowcast_fit &lt;- nfidd_sample(rw_mod, data = wrong_delay_data)\n\n\ngamma_nowcast_fit\n\n    variable    mean  median   sd  mad      q5     q95 rhat ess_bulk ess_tail\n lp__        2188.54 2188.85 8.24 7.96 2173.87 2201.57 1.01      492      970\n init_onsets    0.40    0.34 0.24 0.19    0.12    0.87 1.00     1942     1327\n rw_noise[1]   -0.26   -0.27 0.92 0.94   -1.79    1.31 1.00     2257     1407\n rw_noise[2]   -0.14   -0.15 0.96 0.94   -1.67    1.43 1.00     2124     1336\n rw_noise[3]    0.03    0.02 0.94 0.95   -1.48    1.60 1.00     2291     1567\n rw_noise[4]    0.14    0.14 0.92 0.93   -1.37    1.68 1.00     1693     1611\n rw_noise[5]    0.33    0.31 0.93 0.92   -1.19    1.88 1.00     2906     1407\n rw_noise[6]    0.47    0.47 0.87 0.90   -0.95    1.89 1.00     2238     1287\n rw_noise[7]    0.22    0.22 0.93 0.93   -1.30    1.81 1.00     2767     1415\n rw_noise[8]    0.07    0.05 0.95 0.95   -1.47    1.62 1.00     2023     1313\n\n # showing 10 of 209 rows (change via 'max_rows' argument or 'cmdstanr_max_rows' option)\n\n\nAgain we can extract the nowcast of symptom onsets and plot it alongside the observed data:\n\ngamma_nowcast_onsets &lt;- gamma_nowcast_fit |&gt;\n  gather_draws(onsets[day]) |&gt;\n  ungroup() |&gt;\n  filter(.draw %in% sample(.draw, 100)) |&gt;\n  mutate(day = day + 1)\n\n\nggplot(gamma_nowcast_onsets, aes(x = day)) +\n  geom_col(data = reported_onset_df, mapping = aes(y = onsets), alpha = 0.6) +\n  geom_line(mapping = aes(y = .value, group = .draw), alpha = 0.1) +\n  geom_point(data = reported_onset_df, mapping = aes(y = reported_onsets))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipTake 2 minutes\n\n\n\nWhat do you think of the nowcast now? How would you know you had the wrong delay if you didn’t have the true delay distribution?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nThe nowcast now looks very different to the observed data. This is because the model is using the wrong delay distribution to nowcast the data.\nIf you didn’t have the true delay distribution you would not know that you had the wrong delay distribution. This is why it is important to estimate the delay distribution from the data.\nIn practice, you would likely compare the nowcast to the observed data and if they did not match you would consider whether the delay distribution was the cause.",
    "crumbs": [
      "Nowcasting concepts"
    ]
  },
  {
    "objectID": "sessions/nowcasting.html#methods-in-practice",
    "href": "sessions/nowcasting.html#methods-in-practice",
    "title": "Nowcasting concepts",
    "section": "Methods in practice",
    "text": "Methods in practice\n\nWolffram et al., Collaborative nowcasting of COVID-19 hospitalization incidences in Germany compares the performance of a range of methods that were used in a nowcasting hub and investigates what might explain performance differences.\nThe baselinenowcast R package provides implementations of simple multiplicative nowcasting methods that can serve as baselines for comparison with more complex approaches.",
    "crumbs": [
      "Nowcasting concepts"
    ]
  },
  {
    "objectID": "sessions/nowcasting.html#references",
    "href": "sessions/nowcasting.html#references",
    "title": "Nowcasting concepts",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Nowcasting concepts"
    ]
  },
  {
    "objectID": "sessions/slides/forecast-evaluation.html#your-turn",
    "href": "sessions/slides/forecast-evaluation.html#your-turn",
    "title": "Forecast evaluation",
    "section": " Your Turn",
    "text": "Your Turn\n\nLoad forecasts from the model we have visualised previously.\nEvaluate the forecasts using proper scoring rules."
  },
  {
    "objectID": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#biases-in-epidemiological-delays",
    "href": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#biases-in-epidemiological-delays",
    "title": "Introduction to biases in epidemiological delays",
    "section": "Biases in epidemiological delays",
    "text": "Biases in epidemiological delays\nWhy might our estimates of epidemiological delays be biased?\n\n\n\ndata reliability and representativeness\n\n\n\nintrinsic issues with data collection and recording"
  },
  {
    "objectID": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#issue-1-double-censoring",
    "href": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#issue-1-double-censoring",
    "title": "Introduction to biases in epidemiological delays",
    "section": "Issue #1: Double censoring",
    "text": "Issue #1: Double censoring\n\nreporting of events usually as a date (not date + precise time)\nfor short delays this can make quite a difference\naccounting for it incorrectly can introduce more bias than doing nothing"
  },
  {
    "objectID": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#double-censoring-example",
    "href": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#double-censoring-example",
    "title": "Introduction to biases in epidemiological delays",
    "section": "Double censoring: example",
    "text": "Double censoring: example\nWe are trying to estimate an incubation period. For person A we know exposure happened on day 1 and symptom onset on day 3."
  },
  {
    "objectID": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#double-censoring-example-1",
    "href": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#double-censoring-example-1",
    "title": "Introduction to biases in epidemiological delays",
    "section": "Double censoring: example",
    "text": "Double censoring: example\nWe are trying to estimate an incubation period. For person A we know exposure happened on day 1 and symptom onset on day 3."
  },
  {
    "objectID": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#double-censoring-example-2",
    "href": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#double-censoring-example-2",
    "title": "Introduction to biases in epidemiological delays",
    "section": "Double censoring: example",
    "text": "Double censoring: example\nWe are trying to estimate an incubation period. For person A we know exposure happened on day 1 and symptom onset on day 3."
  },
  {
    "objectID": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#double-censoring-example-3",
    "href": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#double-censoring-example-3",
    "title": "Introduction to biases in epidemiological delays",
    "section": "Double censoring: example",
    "text": "Double censoring: example\nWe are trying to estimate an incubation period. For person A we know exposure happened on day 1 and symptom onset on day 3.\n\nThe true incubation period of A could be anywhere between 1 and 3 days (but not all equally likely)."
  },
  {
    "objectID": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#understanding-double-censoring-components",
    "href": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#understanding-double-censoring-components",
    "title": "Introduction to biases in epidemiological delays",
    "section": "Understanding double censoring components",
    "text": "Understanding double censoring components\nDouble interval censoring can be decomposed into:\n\nPrimary event censoring: Uncertainty about when the first event occurred within its reported day\n\n\nSecondary event censoring: Uncertainty about when the second event occurred within its reported day\n\n\n\n\n\n\n\n\nWarning\n\n\nA common mistake is to only account for secondary event censoring while ignoring primary event censoring."
  },
  {
    "objectID": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#issue-2-right-truncation",
    "href": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#issue-2-right-truncation",
    "title": "Introduction to biases in epidemiological delays",
    "section": "Issue #2: right truncation",
    "text": "Issue #2: right truncation\n\nreporting of events can be triggered by the secondary event\nin that case, longer delays might be missing because whilst the primary events have occurred the secondary events have not occurred yet"
  },
  {
    "objectID": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#example-right-truncation",
    "href": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#example-right-truncation",
    "title": "Introduction to biases in epidemiological delays",
    "section": "Example: right truncation",
    "text": "Example: right truncation\nWe are trying to estimate an incubation period. Each arrow represents one person with an associated pair of events (infection and symptom onset)."
  },
  {
    "objectID": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#example-right-truncation-1",
    "href": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#example-right-truncation-1",
    "title": "Introduction to biases in epidemiological delays",
    "section": "Example: right truncation",
    "text": "Example: right truncation\nWe are trying to estimate an incubation period. Each arrow represents one person with an associated pair of events (infection and symptom onset)."
  },
  {
    "objectID": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#example-right-truncation-2",
    "href": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#example-right-truncation-2",
    "title": "Introduction to biases in epidemiological delays",
    "section": "Example: right truncation",
    "text": "Example: right truncation\nWe are trying to estimate an incubation period. Each arrow represents one person with an associated pair of events (infection and symptom onset)."
  },
  {
    "objectID": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#example-right-truncation-3",
    "href": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#example-right-truncation-3",
    "title": "Introduction to biases in epidemiological delays",
    "section": "Example: right truncation",
    "text": "Example: right truncation\nWe are trying to estimate an incubation period. Each arrow represents one person with an associated pair of events (infection and symptom onset)\n\nOn the day of analysis we have not observed some onsets yet. The delay from infection to onset for these delays tended to be longer. This is made worse during periods of exponential growth."
  },
  {
    "objectID": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#example-right-truncation-4",
    "href": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#example-right-truncation-4",
    "title": "Introduction to biases in epidemiological delays",
    "section": "Example: right truncation",
    "text": "Example: right truncation\nWe are trying to estimate an incubation period. Each arrow represents one person with an associated pair of events (infection and symptom onset)\n\nWe need to account for infections with longer delays that we haven’t yet observed. We can best do this by using a lognormal distribution for the delays that we do have data on."
  },
  {
    "objectID": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#censoring-and-right-truncation",
    "href": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#censoring-and-right-truncation",
    "title": "Introduction to biases in epidemiological delays",
    "section": "Censoring and right truncation",
    "text": "Censoring and right truncation\n\nWhen analysing data from an outbreak in real time, we are likely to have double censoring and right truncation, making things worse\nIn the practical we will only look at the two separately to keep things simple"
  },
  {
    "objectID": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#your-turn",
    "href": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#your-turn",
    "title": "Introduction to biases in epidemiological delays",
    "section": " Your Turn",
    "text": "Your Turn\n\nSimulate epidemiological delays with biases\nEstimate parameters of a delay distribution, correcting for biases"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#epidemiological-events-disease-progression",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#epidemiological-events-disease-progression",
    "title": "Introduction to epidemiological delays",
    "section": "Epidemiological events: disease progression",
    "text": "Epidemiological events: disease progression\n\ninfection\nsymptom onset\nbecoming infectious\nhospital admission\ndeath"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#epidemiological-events-recovery",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#epidemiological-events-recovery",
    "title": "Introduction to epidemiological delays",
    "section": "Epidemiological events: recovery",
    "text": "Epidemiological events: recovery\n\npathogen clearance\nsymptoms clearance\nend of infectiousness\ndischarge from hospital"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#epidemiological-events-control",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#epidemiological-events-control",
    "title": "Introduction to epidemiological delays",
    "section": "Epidemiological events: control",
    "text": "Epidemiological events: control\n\nquarantine\nisolation\ntreatment"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#epidemiological-events-reporting",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#epidemiological-events-reporting",
    "title": "Introduction to epidemiological delays",
    "section": "Epidemiological events: reporting",
    "text": "Epidemiological events: reporting\n\nspecimen taken\nreport added to database"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#some-delays-have-names",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#some-delays-have-names",
    "title": "Introduction to epidemiological delays",
    "section": "Some delays have names",
    "text": "Some delays have names\ninfection to symptom onset\n\nIncubation period"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#some-delays-have-names-1",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#some-delays-have-names-1",
    "title": "Introduction to epidemiological delays",
    "section": "Some delays have names",
    "text": "Some delays have names\ninfection to becoming infectious\n\nLatent period"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#some-delays-have-names-2",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#some-delays-have-names-2",
    "title": "Introduction to epidemiological delays",
    "section": "Some delays have names",
    "text": "Some delays have names\nbecoming infectious to end of infectiousness\n\nInfectious period"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#some-delays-have-names-3",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#some-delays-have-names-3",
    "title": "Introduction to epidemiological delays",
    "section": "Some delays have names",
    "text": "Some delays have names\nhospital admission to discharge\n\nLength of stay"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#some-delays-have-names-4",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#some-delays-have-names-4",
    "title": "Introduction to epidemiological delays",
    "section": "Some delays have names",
    "text": "Some delays have names\nsymptom onset (person A) to symptom onset (person B, infected by A)\n\nSerial interval"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#some-delays-have-names-5",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#some-delays-have-names-5",
    "title": "Introduction to epidemiological delays",
    "section": "Some delays have names",
    "text": "Some delays have names\ninfection (person A) to infection (person B, infected by A)\n\nGeneration interval"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#why-do-we-want-to-know-these",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#why-do-we-want-to-know-these",
    "title": "Introduction to epidemiological delays",
    "section": "Why do we want to know these?",
    "text": "Why do we want to know these?"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#key-parameters-in-mathematical-models",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#key-parameters-in-mathematical-models",
    "title": "Introduction to epidemiological delays",
    "section": "Key parameters in mathematical models",
    "text": "Key parameters in mathematical models\n\nFerguson et al., 2020"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#key-parameters-in-mathematical-models-1",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#key-parameters-in-mathematical-models-1",
    "title": "Introduction to epidemiological delays",
    "section": "Key parameters in mathematical models",
    "text": "Key parameters in mathematical models\n\nWard et al., 2020"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#key-elements-of-infectious-disease-epidemiology",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#key-elements-of-infectious-disease-epidemiology",
    "title": "Introduction to epidemiological delays",
    "section": "Key elements of infectious disease epidemiology",
    "text": "Key elements of infectious disease epidemiology\n Nishiura et al., 2007"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#why-do-we-want-to-know-these-1",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#why-do-we-want-to-know-these-1",
    "title": "Introduction to epidemiological delays",
    "section": "Why do we want to know these?",
    "text": "Why do we want to know these?\n\nKey elements of infectious disease epidemiology\nIntricate relationship with nowcasting/forecasting"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#quantifying-delays",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#quantifying-delays",
    "title": "Introduction to epidemiological delays",
    "section": "Quantifying delays",
    "text": "Quantifying delays\n\nEpidemiological delays are variable between individuals\nWe can capture their variability using probability distributions"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#what-is-a-probability-distribution",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#what-is-a-probability-distribution",
    "title": "Introduction to epidemiological delays",
    "section": "What is a probability distribution?",
    "text": "What is a probability distribution?\n\nMathematical way to describe variability in delays\nShows how likely different delay values are\nCommon examples for delays:\n\nLog-normal: right-skewed, good for incubation periods\nGamma: flexible shape, alternative for delays"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#warning-two-levels-of-uncertainty",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#warning-two-levels-of-uncertainty",
    "title": "Introduction to epidemiological delays",
    "section": "Warning: Two levels of uncertainty",
    "text": "Warning: Two levels of uncertainty\n\nProbability distributions characterise variability in the delays between individuals\nParameters of the probability distribution can be uncertain\n\n\n\n\\[\n\\alpha \\sim \\mathrm{Normal}(mean = 5, sd = 0.1) \\\\\n\\beta \\sim \\mathrm{Normal}(mean = 1, sd = 0.1) \\\\\n\\]\nShowing probability density functions of lognormal distributions with shape \\(\\alpha\\) and rate \\(\\beta\\)."
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#statistical-inference-for-delays",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#statistical-inference-for-delays",
    "title": "Introduction to epidemiological delays",
    "section": "Statistical inference for delays",
    "text": "Statistical inference for delays\n\nWe observe delay data but don’t know true distribution parameters\nNeed to estimate parameters from incomplete, noisy data\nBayesian approach: prior knowledge + observed data → posterior estimates"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#stan-for-delay-estimation",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#stan-for-delay-estimation",
    "title": "Introduction to epidemiological delays",
    "section": "Stan for delay estimation",
    "text": "Stan for delay estimation\n\nStatistical software for Bayesian inference\nWe specify: data structure, parameters to estimate, model relationships\nStan returns parameter estimates with proper uncertainty quantification"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#your-turn",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#your-turn",
    "title": "Introduction to epidemiological delays",
    "section": " Your Turn",
    "text": "Your Turn\n\nSimulate epidemiological delays\nEstimate parameters of a delay distribution"
  },
  {
    "objectID": "sessions/slides/introduction-to-joint-estimation-of-nowcasting-and-reporting-delays.html#your-turn",
    "href": "sessions/slides/introduction-to-joint-estimation-of-nowcasting-and-reporting-delays.html#your-turn",
    "title": "Introduction to joint estimation of nowcasting and reporting delays",
    "section": " Your Turn",
    "text": "Your Turn\n\nSimulate data with delayed reporting\nPerform a joint estimation of the delay and nowcast\nUnderstand the limitations of the data generating process\nPerform a joint estimation of the delay, nowcast, and reproduction number"
  },
  {
    "objectID": "sessions/slides/introduction-to-reproduction-number.html#convolution-session",
    "href": "sessions/slides/introduction-to-reproduction-number.html#convolution-session",
    "title": "Introduction to the time-varying reproduction number",
    "section": "Convolution session",
    "text": "Convolution session\n\nfunctions {\n  #include \"functions/convolve_with_delay.stan\"\n}\n\ndata {\n  int n;            // number of time days\n  array[n] int obs; // observed onsets\n  int&lt;lower = 1&gt; ip_max; // max incubation period\n  // probability mass function of incubation period distribution (first index zero)\n  array[ip_max + 1] real ip_pmf;\n}\n\nparameters {\n  array[n] real&lt;lower = 0&gt; infections;\n}\n\ntransformed parameters {\n  array[n] real onsets = convolve_with_delay(infections, ip_pmf);\n}\n\nmodel {\n  // priors\n  infections ~ normal(0, 10) T[0, ];\n  obs ~ poisson(onsets);\n}\n\n\nPrior for infections at time \\(t\\) is independent from infections at all other time points. Is this reasonable?"
  },
  {
    "objectID": "sessions/slides/introduction-to-reproduction-number.html#what-if-we-convolve-infections-with-infections",
    "href": "sessions/slides/introduction-to-reproduction-number.html#what-if-we-convolve-infections-with-infections",
    "title": "Introduction to the time-varying reproduction number",
    "section": "What if we convolve infections with infections?",
    "text": "What if we convolve infections with infections?\nWe can extend the convolution concept from the last session:\n\nPrevious session: infections → symptoms (via incubation period)\nThis session: infections → infections (via generation time)"
  },
  {
    "objectID": "sessions/slides/introduction-to-reproduction-number.html#generation-time-infection-to-infection",
    "href": "sessions/slides/introduction-to-reproduction-number.html#generation-time-infection-to-infection",
    "title": "Introduction to the time-varying reproduction number",
    "section": "Generation time: infection to infection",
    "text": "Generation time: infection to infection\nHere rather than the time from infection (person A) to symptoms (person B, infected by A) we have the time from infection (person A) to infection (person B, infected by A).\nThis is known as the generation time distribution (\\(g(t)\\))."
  },
  {
    "objectID": "sessions/slides/introduction-to-reproduction-number.html#generation-time-notation",
    "href": "sessions/slides/introduction-to-reproduction-number.html#generation-time-notation",
    "title": "Introduction to the time-varying reproduction number",
    "section": "Generation time: notation",
    "text": "Generation time: notation\nWe can write the convolution as:\n\\[\nI_t = \\mathrm{scaling} \\times \\sum_{t' &lt; t} I_t' g(t - t')\n\\]\nHowever, unlike the infection to symptoms case here we don’t assume that the two time series have the same magnitude so need to introduce a scaling.\n\nWhat is this scaling?"
  },
  {
    "objectID": "sessions/slides/introduction-to-reproduction-number.html#the-renewal-equation-as-a-convolution",
    "href": "sessions/slides/introduction-to-reproduction-number.html#the-renewal-equation-as-a-convolution",
    "title": "Introduction to the time-varying reproduction number",
    "section": "The renewal equation as a convolution",
    "text": "The renewal equation as a convolution"
  },
  {
    "objectID": "sessions/slides/introduction-to-reproduction-number.html#the-scaling-factor-reproduction-number",
    "href": "sessions/slides/introduction-to-reproduction-number.html#the-scaling-factor-reproduction-number",
    "title": "Introduction to the time-varying reproduction number",
    "section": "The scaling factor: reproduction number",
    "text": "The scaling factor: reproduction number\nLet’s assume we have \\(I_0\\) infections at time 0, and the scaling doesn’t change in time.\nHow many people will they go on to infect?\n\n\\[\nI = \\mathrm{scaling} \\times \\sum_{t=0}^\\infty I_0 g(t) = \\mathrm{scaling} * I_0\n\\]\nThe scaling can be interpreted as the reproduction number \\(R_0\\) (assuming a susceptible population)."
  },
  {
    "objectID": "sessions/slides/introduction-to-reproduction-number.html#the-renewal-equation",
    "href": "sessions/slides/introduction-to-reproduction-number.html#the-renewal-equation",
    "title": "Introduction to the time-varying reproduction number",
    "section": "The renewal equation",
    "text": "The renewal equation\nIf \\(R_t\\) can change over time, it can be interpreted as the (“instantaneous”) reproduction number:\n\\[\nI_t = R_t \\times \\sum_{t' &lt; t} I_t' g(t - t')\n\\]\nWe can estimate \\(R_t\\) from a time series of infections using the renewal equation."
  },
  {
    "objectID": "sessions/slides/introduction-to-reproduction-number.html#your-turn",
    "href": "sessions/slides/introduction-to-reproduction-number.html#your-turn",
    "title": "Introduction to the time-varying reproduction number",
    "section": " Your Turn",
    "text": "Your Turn\n\nSimulate infections using the renewal equation\nEstimate reproduction numbers using a time series of infections\nCombine with delay distributions to jointly infer infections and R from a time series of outcomes"
  },
  {
    "objectID": "sessions/slides/tools.html#your-turn",
    "href": "sessions/slides/tools.html#your-turn",
    "title": "Examples of tools",
    "section": " Your Turn",
    "text": "Your Turn\nHow does each tool relate to key concepts in the course?\n\nEpiEstim\nEpiNow2\nEpinowcast"
  }
]