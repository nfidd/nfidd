---
title: "Biases in delay distributions"
order: 3
---

# Introduction

So far, we've looked at the uncertainty of the time delays between epidemiological events. The next challenge is that our information on these delays is usually biased, especially when we're analysing data in real time. We'll consider two types of biases that commonly occur in reported infectious disease data: 

- *Censoring*: when we know an event occurred at some time, but not exactly when.
- *Truncation*: when not enough time has passed for all the relevant epidemiological events to occur or be observed.

We can again handle these by including them as uncertain parameters in the modelling process.

## Slides

[Introduction to biases in epidemiological delays](slides/introduction-to-biases-in-epidemiological-delays)

## Objectives

In this session, we'll introduce censoring and right truncation as typical properties of the process that generates infectious disease data sets, using the delay from symptom onset to hospitalisation as an example.

::: {.callout-note collapse="true"}

# Setup

## Source file

The source file of this session is located at `sessions/biases-in-delay-distributions.qmd`.

## Libraries used

In this session we will use the `nfidd` package to load a data set of infection times and access stan models and helper functions, the `ggplot2` package for plotting, the `dplyr` and `tidyr` packages to wrangle data, the `lubridate` package to deal with dates, and the `tidybayes` package for extracting results from model fits.

```{r libraries, message = FALSE}
library("nfidd")
library("ggplot2")
library("dplyr")
library("tidyr")
library("lubridate")
library("tidybayes")
```

::: callout-tip
The best way to interact with the material is via the [Visual Editor](https://docs.posit.co/ide/user/ide/guide/documents/visual-editor.html) of RStudio.
:::

## Initialisation

We set a random seed for reproducibility. 
Setting this ensures that you should get exactly the same results on your computer as we do.
We also set an option that makes `cmdstanr` show line numbers when printing model code.
This is not strictly necessary but will help us talk about the models.

```{r}
set.seed(123)
options(cmdstanr_print_line_numbers = TRUE)
```

:::

# Load data

We will use the same simulated data set as in the [session on delay distributions](delay-distributions#simulating-delayed-epidemiological-data).

::: callout-note
Remember, in this outbreak we are assuming:

-   the incubation period is **gamma**-distributed with **shape 5** and **rate 1**, i.e. a mean of 5 days
-   the time from onset to hospital admission is **lognormally**-distributed, with **meanlog 1.75** and **sdlog 0.5**, i.e. a mean delay of about a week
:::

We use the same function we used previously to simulate symptom onset and hospitalisation data.

```{r inf_hosp_solution}
df <- add_delays(infection_times)
```

This creates the `df` data frame that we can inspect e.g. using

```{r show_df}
head(df)
```

# Dates, not days: censoring

So far, the data we've been using has represented time as a *continuous* variable: a numeric time since a given starting point.
Data on health outcomes are usually not recorded in this way.
Instead, we usually deal with *dates*: a discrete interval.
We refer to this restriction of the data, where we only know that an event has occurred within a given time period (e.g. a day) but not when exactly within that period, as *interval censoring*.

We can make our simulated dataset a bit more realistic by rounding down the infection times to an integer number.

```{r censored_times}
# Use the floor() function to round down to integers
df_dates <- df |>
  mutate(
    infection_time = floor(infection_time),
    onset_time = floor(onset_time),
    hosp_time = floor(hosp_time)
  )
head(df_dates)
```

::: {.callout-note}
As before we are still not working with dates but numbers.
This makes handling the data easier - we don't have to make any conversions before using the data in stan.
:::

Each of the numbers now represent the number of days that have passed since the start of the outbreak.
That is, each of the numbers correspond to a day.
In that sense, the data is more like typical data we get from infectious disease outbreaks, where we would usually have a line list with key events such as symptom onset or death reported by a *date*.
In statistical terms, we call the data *double interval censored*:

- "double" because the delays represent the time between two events that are both censored

- "interval" because all we know about the timings of the events is that they happened in a certain time interval (between 0:00 and 23:59 on the recorded day).

::: {.callout-note collapse="true"}
# Why are we so worried?

We'll be spending a lot of time thinking about a potentially "short" 24 hour time interval. In this session, we'll start to get a sense for how variation, uncertainty, and bias within this interval might combine to impact daily outbreak estimates.
The concepts and techniques that we use here also apply to longer time intervals that we might find in surveillance data, for example, reported case counts aggregated by week.
:::

## Estimating delay distributions accounting for censoring

Let's estimate the time from symptom onset to hospitalisation with the censored data.

A naïve approach to estimating the delay would be to ignore the fact that the data are censored.
To estimate the delay from onset to hospitalisation, we could just use the difference between the censored times, which is an integer (the number of days).

```{r integer-delays}
df_dates <- df_dates |>
  mutate(
    incubation_period = onset_time - infection_time,
    onset_to_hosp = hosp_time - onset_time
  )
```

To help us think about this lets summarise the original time based incubation period

```{r}
summary(df$onset_time - df$infection_time)
```

and then the date based incubation period as we observe it.

```{r}
summary(df_dates$incubation_period)
```

You should see that they have nearly the same means but different medians (by about half a day).

::: callout-tip
## Take 5 minutes

Fit the lognormal model used in the [session on delay distributions](delay-distributions#estimating-delay-distributions) to the estimates from the rounded data, i.e. using the `df_dates` data set.
Do you still recover the parameters that we put in?
:::

::: {.callout-note collapse="true"}
## Solution

```{r df_dates_solution, results = 'hide', message = FALSE}
mod <- nfidd_cmdstan_model("lognormal")
res <- nfidd_sample(mod,
  data = list(
    n = nrow(na.omit(df_dates)),
    y = na.omit(df_dates)$onset_to_hosp
  )
)
```

```{r df_dates_solution_summary}
res
```

We can calculate the mean and standard deviation to see the bias more clearly:

```{r df_dates_mean_sd_summary}
res |>
  summarise_lognormal()
```

Usually the estimates will be further from the "true" parameters than before when we worked with the unrounded data.
:::

There are many ad-hoc solutions to this problem that, for example, introduce a shift in the data by half a day to centre it on mid-day or discretise the distribution and use the difference between two cumulative density functions with a day, or two day interval.
Of these all but the two day interval approach introduce more bias than doing nothing as above.
See Park et al. for a detailed discussion of approximate approaches [@parkEstimatingEpidemiologicalDelay2024].

To properly account for double interval censoring, we need to modify the model to include the fact that we don't know when exactly on any given day the event happened.
For example, if we know that symptom onset of an individual occurred on 20 June, 2024, and they were admitted to hospital on 22 June, 2024, this could mean an onset-to-hospitalisation delay from 1 day (onset at 23:59 on the 20th, admitted at 0:01 on the 22nd) to 3 days (onset at 0:01 on the 20th, admitted at 23:59 on the 22nd).

We can use this in our delay estimation by making the exact time of the events based on the dates given part of the estimation procedure:

```{r censoring_adjusted_delay_model}
cmod <- nfidd_cmdstan_model("censored-delay-model")
cmod
```

::: callout-tip
## Take 5 minutes

Familiarise yourself with the model above.
Do you understand all the lines?
Which line(s) define the parameter prior distribution(s), which one(s) the likelihood, and which one(s) reflect that we have now provided the delay as the difference in integer days?
:::

::: {.callout-note collapse="true"}
## Solution

Lines 21-24 define the parametric prior distributions (for parameters meanlog and sdlog, and the estimates of exact times of events).
Line 27 defines the likelihood.
Lines 15-17 reflect the integer delays, adjusted by the estimated times of day.
:::

Now we can use this model to re-estimate the parameters of the delay distribution:

```{r censored_estimate, results = 'hide', message = FALSE}
cres <- nfidd_sample(cmod,
  data = list(
    n = nrow(na.omit(df_dates)),
    onset_to_hosp = na.omit(df_dates)$onset_to_hosp
  )
)
```

```{r censored_estimate_summary}
cres
```

We can also examine the mean and standard deviation of the estimated delay distribution:

```{r censored_mean_sd_summary}
cres |>
  summarise_lognormal()
```

::: callout-tip
## Take 10 minutes

Try re-simulating the delays using different parameters of the delay distribution.
Can you establish under which conditions the bias in estimation gets worse?
:::

# Real-time estimation: truncation

The data set we have looked at so far in this session is a "final" data set representing an outbreak that has come and gone.
However, information on delay distributions is often important during ongoing outbreaks as they can inform nowcasts and forecasts and help with broader interpretation of data.

Estimating delays in real time comes with particular challenges, as we know that our data are usually incomplete: not enough time has passed to observe all of the relevant events that could have happened.
We call this incompleteness *right truncation*.
This is a problem if we try to use an incomplete set of data about one type of event to infer the pattern of a previous epidemiological event. 
For example, our target might be to infer the "primary" event of infections. We want to do this using some observed data about a "secondary" event, such as recent symptom onsets. 
However, we are still in the middle of an outbreak.
Some infected cases haven't yet presented with symptoms. 
So we only have a partial, incomplete count to work with.
When we condition on an observed event with a delay in this way, we get **right truncation** of the target event. 
Our estimates might be biased, because we are missing some of the data to get the full picture.

This bias also changes over the course of an outbreak. If, for example, infections are exponentially increasing, then there will be disproportionately more people with recent symptom onset.
Without adjustment, this would artificially decrease the estimate of the mean delay compared to its true value for all infections that have in fact already occurred.
This happens because most infections are recent (due to the exponential increase), but later symptom onsets amongst these have not had a chance to happen yet.
So among a cohort of people with symptom onset at the same time, those with shorter delays to onset would be over-represented.

Once again, we can simulate this effect, for example by imagining we would like to make an estimate on day 70 of our outbreak.
Let us work with the original, un-censored data for the time from onset to hospitalisation so as to look at the issue of truncation in isolation:

```{r truncated_df}
df_realtime <- df |>
  mutate(onset_to_hosp = hosp_time - onset_time) |>
  filter(hosp_time <= 70)
```

Depending on where we cut off observation (i.e. what time) in our outbreak will impact the amount of truncation our data has.
Ending observation close to periods of exponential growth will lead to more truncation than if we cut off observation towards the end of an outbreak when it has been stable or infections have been reducing for some time.

## Estimating delay distributions accounting for truncation

If we take the naïve mean of delays we get an underestimate as expected:

```{r mean_truncated}
# truncated mean delay
mean(df_realtime$onset_to_hosp)
# compare with the mean delay over the full outbreak
mean(df$hosp_time - df$onset_time, na.rm=TRUE)
```

::: callout-tip
## Take 5 minutes

Fit the lognormal model used above to the estimates from the truncated data, i.e. using the `df_realtime` data set.
How far away from the "true" parameters do you end up?
:::

::: {.callout-note collapse="true"}
## Solution

```{r df_realtime_solution, results = 'hide', message = FALSE}
res <- nfidd_sample(mod,
  data = list(
    n = nrow(na.omit(df_realtime)),
    y = na.omit(df_realtime)$onset_to_hosp
  )
)
```

```{r df_realtime_solution_summary}
res
```

The mean and standard deviation show the underestimation due to truncation:

```{r df_realtime_mean_sd_summary}
res |>
  summarise_lognormal()
```
:::

Once again, we can write a model that adjusts for truncation, by re-creating the simulated truncation effect in the stan model:

```{r truncation_adjusted_delay_model}
tmod <- nfidd_cmdstan_model("truncated-delay-model")
tmod
```

::: callout-tip
## Take 5 minutes

Familiarise yourself with the model above.
Which line introduces the truncation, i.e. the fact that we have not been able to observe hospitalisation times beyond the cutoff of (here) 70 days?
:::

::: {.callout-note collapse="true"}
## Solution

Line 17 defines the upper limit of `onset_to_hosp` as `time_since_onset`.
:::

Now we can use this model to re-estimate the parameters of the delay distribution:

```{r truncated_estimate, results = 'hide', message = FALSE}
tres <- nfidd_sample(tmod,
  data = list(
    n = nrow(df_realtime),
    onset_to_hosp = df_realtime$onset_to_hosp, 
    time_since_onset = 70 - df_realtime$onset_time
  )
)
```

```{r truncated_estimate_summary}
tres
```

Let's also check the mean and standard deviation of the truncation-adjusted delay distribution:

```{r truncated_mean_sd_summary}
tres |>
  summarise_lognormal()
```

::: callout-tip
## Take 10 minutes
Try estimating the delays at different times (i.e. varying the observation cut-off) and also try re-simulating the delays using different parameters of the delay distribution.
Can you establish under which conditions the bias in estimation gets worse?
:::

# Going further

## Challenge

-   We have looked at censoring and truncation separately, but in reality often both are present. Can you combine the two in a model?
-   The solutions we introduced for addressing censoring and truncation are only some possible ones for the censoring problem.
There are other solutions that reduce the biases from estimation even further. 
For a full overview, see the review by [@parkEstimatingEpidemiologicalDelay2024].

## Methods in practice

-   The [`primarycensored`](https://github.com/epinowcast/primarycensored) R package provides a comprehensive framework for dealing with censored and truncated delay distributions.
    It implements methods techniques for handling primary event censoring, secondary event censoring, and right truncation in a unified and efficient manner.
-   The [`epidist`](https://github.com/epinowcast/epidist) package extends `primarycensored` to work with `brms` for estimating epidemiological delay distributions.
    It enables flexible modelling including time-varying components, spatial effects, and partially pooled estimates of demographic characteristics.

# Wrap up

The learning objectives for this session were:

- Understanding of how censoring affects the estimation and interpretation of epidemiological delay distributions
- Ability to estimate parameters of probability distributions from observed delays, taking into account censoring, using R
- Understanding of right truncation in epidemiolgical data
- Ability to estimate parameters of probability distributions from observed delays, taking into account truncation, in R

::: {#refs}
:::
