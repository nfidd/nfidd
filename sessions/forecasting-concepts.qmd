---
title: "Forecasting concepts"
order: 7
---

```{r echo = FALSE}
set.seed(123)
```

# Objectives

The aim of this session is to introduce the concept of forecasting, using a simple model, and forecasting evaluation.

# Libraries used

In this session we will use the `nfidd` package to load the data set of infection times, the `dplyr` and `tidyr` packages for data wrangling, `ggplot2` library for plotting, the `here` library to find the stan model, and the `cmdstanr` library for using stan.
We will also use the `tidybayes` package for extracting results of the inference.

```{r libraries, message = FALSE}
library("nfidd")
library("dplyr")
library("tidyr")
library("ggplot2")
library("here")
library("cmdstanr")
library("tidybayes")
```

::: {.callout-tip}
The best way to interact with the material is via the [Visual Editor](https://docs.posit.co/ide/user/ide/guide/documents/visual-editor.html) of RStudio.
If not using the Visual Editor please remember that the code in the session needs to be run inside the course repository so that the `here()` commands below find the stan model files.
:::

# Introduction to forecasting as an epidemiological problem, and its relationship with nowcasting and R estimation

See the lecture slides.

# What is forecasting?

Forecasting is the process of making predictions about the future based on past and present data.
In the context of infectious disease epidemiology, forecasting is the process of predicting the future course of an epidemic based on past and present data.
Here we focus on forecasting observed data (i.e. onsets) but forecasts can also be made for other quantities of interest such as the number of infections, the reproduction number, or the number of deaths.
Epidemiological forecasting is closely related to nowcasting and, when using mechanistic approaches, estimation of the reproduction number. In fact, the model we will use for forecasting is the same as the model we used for nowcasting and estimation of the reproduction number. The only difference is that we will extend the model into the future and for the purpose of this session we will remove the nowcasting part of the model.

# Extending a model into the future

The model we introduced in the [renewal session]() the reproduction number using a random walk, then used a discrete renewal process to model the number of infections, and convolved these with a delay distribution to model the number of onsets with Poisson observation error.
Based on what we found in the [nowcasting session]() this seems like a reasonable model for the data and so we might want to use it to forecast into the future.
We can do this by extending the model into the future by simulating the model forward in time. 
This is known as a generative model.
We actually already introduced a model that could do this in the renewal session (remember `h` from the data list in the renewal session?).
So what does this look like in code? Lets load in the model again and take a look.

```{r load_model}
mod <- cmdstan_model(here("stan", "estimate-inf-and-r-rw.stan"))
mod$print(line_numbers = TRUE)
```

::: .callout-tip
## Take 5 minutes
What have we changed in the model to make it a forecasting model?
Do you see any limitations of this approach?
:::

::: {.callout-note collapse="true"}
## Solution
- What have we changed in the model to make it a forecasting model?
  - Added the `h` parameter to the data list to specify the number of days to forecast into the future.
  - Added the `m` parameter as a piece of transformed data that is the total number of days to include in the model (i.e. the number of days in the data plus the number of days to forecast).
  - `m` is then used in all arrays in the model rather than `n`. This means that `rw_noise` is now `m-1` long, and  `R`, `onsets`, `infections` and `onsets` are `m` long.
  - As there are only `n` observations in the data in the likelihood we only use the first `n` elements of `onsets`.
  - To include observation error in the forecast a `generated quantities` block has been added which takes the last `h` onsets as the mean of a Poisson distribution and samples from this distribution to get the forecasted onsets.
- Do you see any limitations of this approach?
  - Including `h` in the `parameters` and `model` blocks increases the number of parameters and amount of work we have to do when fitting the model.
  It would be more computationally efficient to have a separate model for forecasting.
:::

Before we can forecast we need some data to fit the model to and the compare the forecasts to. We will use the same simulated data as in the renewal and nowcasting sessions. We also need to pick a point to forecast from. We will use day 41 in line with the nowcasting session.

```{r, load-simulated-onset}
source(here::here("snippets", "simulate-onsets.r"))
onset_df

cutoff <- 41

filtered_onset_df <- onset_df |>
  filter(day <= cutoff)
```

## Fitting the model and forecast for 28 days into the future

We can now fit the model to the data and then forecast into the future. This should look very similar to the code we used in the renewal session but with the addition of a non-zero `h` in the data list.

```{r fit_model}
horizon <- 28

data <- list(
  n =nrow(filtered_onset_df),
  I0 = 1,
  obs = filtered_onset_df$onsets,
  gen_time_max = length(gen_time_pmf),
  gen_time_pmf = gen_time_pmf,
  ip_max = length(ip_pmf) - 1,
  ip_pmf = ip_pmf,
  h = horizon # Here we set the number of days to forecast into the future
)
rw_forecast <- mod$sample(
  data = data, parallel_chains = 4, refresh = 0, show_exceptions = FALSE, show_messages = FALSE, adapt_delta = 0.95)
rw_forecast
```

## Visualising the forecast

We can now visualise the forecast. We will first extract the forecast and then plot the forecasted onsets alongside the observed onsets.

```{r extract-forecast}
forecast <- rw_forecast |>
  gather_draws(forecast[day]) |>
  mutate(day = day + cutoff)

target_onsets <- onset_df |>
  filter(day > cutoff) |>
  filter(day <= cutoff + horizon)
```

```{r plot_forecast}
forecast |>
  filter(.draw <= 100) |>
  ggplot(aes(x = day)) +
  geom_line(alpha = 0.1, aes(y = .value, group = .draw)) +
  geom_point(data = target_onsets, aes(x = day, y = onsets), color = "black")
```

::: {.callout-tip}
## Take 5 minutes
What do you think of this forecast?
Did the model do a good job?
Is there another way you could visualise the forecast that might be more informative?
:::

::: {.callout-note collapse="true"}
## Solution
- On the face of it the forecast looks very poor with some very high predictions compared to the data.
- Based on this visualisation it is hard to tell if the model is doing a good job but it seems like it is not.
- As outbreaks are generally considered to be exponential processes it might be more informative to plot the forecast on the log scale.

```{r plot_forecast_log}
forecast |>
  filter(.draw <= 100) |>
  ggplot(aes(x = day)) +
  geom_line(alpha = 0.1, aes(y = .value, group = .draw)) +
  geom_point(data = target_onsets, aes(x = day, y = onsets), color = "black") +
  scale_y_continuous(trans = "log")
  ```

This should be a lot more informative. We see that for longer forecast horizons the model is not doing a great job of capturing the reduction in onsets. However, we can now  see that the model seems to be producing very reasonable forecasts for the first week or so of the forecast. This is a common pattern in forecasting where the model is good at capturing the short term dynamics but struggles with the longer term dynamics.
:::

As our forecasting model is based on the effective reproduction number we can also visualise the forecast of the reproduction number. This can be helpful for understanding why our forecasts of onsets look the way they do and for understanding the uncertainty in the forecasts. To get an even better picture here we will fit another model to a longer time series and then compare the forecast and estimated reproduction numbers.

```{r fit_model_long}
long_onset_df <- onset_df |>
  filter(day <= cutoff + horizon)

long_data <- list(
  n =nrow(long_onset_df),
  I0 = 1,
  obs = long_onset_df$onsets,
  gen_time_max = length(gen_time_pmf),
  gen_time_pmf = gen_time_pmf,
  ip_max = length(ip_pmf) - 1,
  ip_pmf = ip_pmf,
  h = 0
)

rw_long <- mod$sample(
  data = long_data, parallel_chains = 4, refresh = 0, show_exceptions = FALSE, show_messages = FALSE, adapt_delta = 0.95)
```

We first need to extract the forecast and estimated reproduction numbers.

```{r extract-Rt}
forecast_r <- rw_forecast |>
  gather_draws(R[day]) |>
  mutate(type = "forecast")

long_r <- rw_long |>
  gather_draws(R[day]) |>
  mutate(type = "estimate")
```

We can now plot the forecast and estimated reproduction numbers.

```{r plot-Rt}
forecast_r |>
  bind_rows(long_r) |>
  filter(.draw <= 50) |>
  ggplot(aes(x = day)) +
  geom_vline(xintercept = cutoff, linetype = "dashed") +
  geom_hline(yintercept = 1, linetype = "dashed") +
  geom_line(aes(y = .value, group = .draw, color = type), alpha = 0.1)
  ```

::: {.callout-tip}
The horizontal dashed line at 1 represents the threshold for epidemic growth. If the reproduction number is above 1 then the epidemic is growing, if it is below 1 then the epidemic is shrinking.
:::

::: {.callout-note}
The vertical dashed line represents the point at which we started forecasting.
:::

::: {.callout-tip}
## Take 5 minutes
Can you use this plot to explain why the forecast of onsets looks the way it does?
:::

::: {.callout-note collapse="true"}
## Solution
- When both models are being fit to data (i.e before the vertical dashed line) the forecast and estimated reproduction numbers are very similar.
- For short-term forecasts R estimates continue to be fairly similar.
- However, the estimates have a consistent downwards trend which is not captured by the forecast (which looks like it has a constant mean value with increasing uncertainty).
- This explains the divergence between the forecast and the data as the horizon increases.
- It looks like only a relatively small number of forecast R trajectories grow to be very large but these are enough to visually dominate the forecast of onsets on the natural scale.
- The performance we are seeing here makes sense given that random walks are defined to have a constant mean and increasing variance.
:::

We managed to learn quite a lot about our models forecasting limitations just looking at a single forecast using visualisations. However, what if we wanted to quantify how well the model is doing? This is where forecast evaluation comes in which we will cover in the next section.

# Introduction to forecast evaluation

See the lecure notes for more information on forecast evaluation.

# Evaluate your forecast

In order to properly evaluate forecasts from this model we really need to forecast over a period of time. Ideally, capturing different epidemic dynamics. To save time we have already fitted the model to day in turn from the data we used above and produced a forecast. We will load these forecasts in now.

```{r load_forecasts

```

::: {.callout-tip}
You can see the code that we used to make here forecasts here:
:::
- Here are some forecasts we made earlier
- How do we know if they are any good?

## Visualising your forecast

Our first step is to visualise the forecasts as this can give us a good idea of how well the model is doing without having to calculate any metrics.
A good first visualisation is to show all the forecasts across all horizons alongside the data.


As might be clear, this can be hard to interpret, so we might want also want to visualise each individual forecast horizon versus the data.


::: .callout-tip
What do you think of these forecasts?
Are they any good?
How well do they capture changes in trend?
Does the uncertainty seem reasonable?
Do they seem to under or over predict consistently?
:::

::: {.callout-note collapse="true"}
## Solution


:::

As we have a generative model based on the effective reproduction number we can also visualise the forecasts of the reproduction number. This can be helpful for understanding why our forecasts of onsets look the way they do and for understanding the uncertainty in the forecasts.

## Scoring your forecast

On top of visualising the forecasts we can also transform them using scoring metrics to summarise performance.
Whilst some of these metrics are more useful for comparing models, many can be also be useful for understanding the performance of a single model.
We will look at some of these metrics in the next section.

We will use the `{scoringutils}` package to calculate these metrics.
Our first step is to convert our forecasts into a format that the `{scoringutils}` package can use.
We will use `as_forecast` to do this:


As you can see this has created a `forecast` object which has a print method that summarises the forecasts.


::: .callout-tip
## Take 2 minutes
What important information is in the `forecast` object?
:::

::: {.callout-note collapse="true"}
## Solution

:::

Everything seems to be in order. We can now use the `scoringutils` package to calculate some metrics. We will use the default quantile metrics (as our forecasts are in quantile format) and score our forecasts. 

### Calibration

### Bias

### Weighted interval score

### Weighted interval score decomposition

## Scoring on the log scale

We can also score on the log scale.
This can be useful if we are interested in the relative performance of the model at different scales of the data.
For example, if we are interested in the model's performance at capturing the exponential growth phase of the epidemic.
We again use `scoringutils` but first transform both the forecasts and observations to the log scale. Note that we cannot log scale after scoring as this will give incorrect results.


For more on scoring on the log scale see [Scoring forecasts on transformed scales](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1011393).

### Calibration

### Bias

### Weighted interval score

### Weighted interval score decomposition

# Going further

- What other ways could we summarise the performance of the forecasts?
- What other metrics could we use?
- There is no one-size-fits-all approach to forecast evaluation, often you will need to use a combination of metrics to understand the performance of your model  and typically the metrics you use will depend on the context of the forecast. What attributes of the forecast are most important to you?

# Wrap up
